{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = open('trial/STS.input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for line in input_text.splitlines():\n",
    "    identifier, sentence1, sentence2 = line.split('\\t')\n",
    "    dataset.append({'id': identifier, 's1': sentence1, 's2': sentence2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jordiae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6923076923076923,\n",
       " 0.6666666666666666,\n",
       " 0.5333333333333333,\n",
       " 0.5454545454545454,\n",
       " 0.7692307692307693,\n",
       " 0.8620689655172413]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "# nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N','V'}:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0]\n",
    "# nltk.download('punkt')\n",
    "similarities = []\n",
    "for row in dataset:\n",
    "    identifier = row['id']\n",
    "    # We tokenize the sentences in order to avoid detecting\n",
    "    # the final word of a sentence and a punctuation mark\n",
    "    # as the same token, for instance.\n",
    "    s1 = nltk.word_tokenize(row['s1'])\n",
    "    s2 = nltk.word_tokenize(row['s2'])\n",
    "    # POS tags (ie. word categories) are required for the lemmatizer\n",
    "    # (for decreasing the ambiguity in order to get the right lemmas)\n",
    "    s1_pos = pos_tag(s1)\n",
    "    s2_pos = pos_tag(s2)\n",
    "    s1_lemmas = [lemmatize(pair) for pair in s1_pos]\n",
    "    s2_lemmas = [lemmatize(pair) for pair in s2_pos]\n",
    "    # Once lemmatized (so, we have the root form of words), we compute distances\n",
    "    similarity = jaccard_distance(set(s1_lemmas), set(s2_lemmas))\n",
    "    similarities.append(similarity)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49067081037569205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "text_output = open('trial/STS.gs.txt', 'r').readlines()\n",
    "refs = []\n",
    "for line in text_output:\n",
    "    refs.append(int(line.split()[1]))\n",
    "tsts = []\n",
    "for sim in similarities:\n",
    "    tsts.append(sim)\n",
    "pearsonr(refs, tsts)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer justification: The Pearson correlation is greater than the one obtained in the previous session (0.49 vs 0.39) because lemmatization gives us the canonical form of words. Therefore, all the words derived from a root word will be considered the same. Since we are keen on measuring semantical distances, this is useful, because in word-level settings, morphological information (at least in English) introduces \"noise\". However, the obtained result is still relatively low, because word-level comparisons miss deeper semantic information introduced by sequences of words (ie. the full sentence as a whole).\n",
    "\n",
    "Generally, we believe that using lemmas will perform better (although it will still miss synonyms, for instance). In some cases, it would perform worse. In particular, we can think of at least three possible cases:\n",
    "1 - Cases in which the lemmatizer (or the POS tagger) fails, because taggers are not infallible.\n",
    "2 - Cases in which the 2 texts are almost identical (as in word-by-word identical) except one word with the same lemma but a different morpheme (eg. singular vs plural) which introduces some semantically relevant. In this case, the desired similarity would be almost 100%, but not 100%. But still, it would work quite well.\n",
    "3 - Cases in which morphemes introduce semantically relevant information, probably more important in other languages than English."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
