{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT-MAI S6: Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.wsd import lesk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading the trial set sentences and its respective gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trial/STS.input.txt') as fp:\n",
    "    data = fp.readlines()\n",
    "    \n",
    "with open('trial/STS.gs.txt') as e:\n",
    "    gs = e.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define auxiliary function to map the NLTK pos tags to the ones used by WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "    mapping = {'NOUN': wn.NOUN, 'ADJ': wn.ADJ, 'VERB': wn.VERB, 'ADP': wn.ADV}\n",
    "    if nltk_pos in mapping:\n",
    "        return mapping[nltk_pos]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define auxiliary functions to get the disambiguated synsets from a given context (ie. sentence) and\n",
    "the part-of-speech, respectively. For getting the disambiguated synsets, we use the Lesk algorithm for\n",
    "word sense disambiguation, which requires part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(context):\n",
    "    words = [word for (word, pos) in context]\n",
    "    return {lesk(words, w, p) for w, p in context if lesk(words, w, p)}\n",
    "\n",
    "\n",
    "def get_pos(sent):\n",
    "    tok_sent = nltk.word_tokenize(sent)\n",
    "    pos_sent = pos_tag(tok_sent, tagset='universal')\n",
    "    w_pos_sent = [(word, nltk_pos_to_wordnet_pos(pos)) for (word, pos) in pos_sent]\n",
    "    filtered_pos_sent = [(word, pos) for (word, pos) in w_pos_sent if pos is not None]\n",
    "    return filtered_pos_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define two functions for the two different ways we have investigated to compute the Jaccard distance:\n",
    "- Synsets (eval_synsets): Compute the Jaccard distance between the set of the disambiguated synsets of the first\n",
    "    sentence and the set of disambiguated synsets of the second sentence.\n",
    "- Definitions (eval_definitions): Compute the Jaccard distance between the set of words in the **definitions** of the disambiguated synsets of the first sentence and the ones in the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between gold and jaccard distance using synsets: 0.4697360281253835\n",
      "Pearson correlation between gold and jaccard distance using words in the definition: 0.45389450800788006\n"
     ]
    }
   ],
   "source": [
    "def eval_synsets(sent1, sent2):\n",
    "    pos_sent1 = get_pos(sent1)\n",
    "    pos_sent2 = get_pos(sent2)\n",
    "    synsets1 = get_synsets(pos_sent1)\n",
    "    synsets2 = get_synsets(pos_sent2)\n",
    "    return jaccard_distance(synsets1, synsets2)\n",
    "\n",
    "def eval_definitions(sent1, sent2):\n",
    "    pos_sent1 = get_pos(sent1)\n",
    "    pos_sent2 = get_pos(sent2)\n",
    "    synsets1 = get_synsets(pos_sent1)\n",
    "    synsets2 = get_synsets(pos_sent2)\n",
    "    definitions1 = set([])\n",
    "    for synset in synsets1:\n",
    "        for word in nltk.word_tokenize(synset.definition()):\n",
    "            definitions1.add(word)\n",
    "    definitions2 = set([])\n",
    "    for synset in synsets2:\n",
    "        for word in nltk.word_tokenize(synset.definition()):\n",
    "            definitions2.add(word)\n",
    "    return jaccard_distance(definitions1, definitions2)\n",
    "    \n",
    "\n",
    "jaccard_synsets = []\n",
    "jaccard_definitions = []\n",
    "gold = []\n",
    "for index, line in enumerate(data):\n",
    "    (num, sent1, sent2) = line.split('\\t')\n",
    "    jaccard_synsets.append(eval_synsets(sent1, sent2))\n",
    "    jaccard_definitions.append(eval_definitions(sent1, sent2))\n",
    "    gold.append(int(gs[index].split('\\t')[1][0]))\n",
    "\n",
    "print('Pearson correlation between gold and jaccard distance using synsets:', pearsonr(gold, jaccard_synsets)[0])\n",
    "print('Pearson correlation between gold and jaccard distance using words in the definition:', pearsonr(gold, jaccard_definitions)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison and conclusions\n",
    "Recall that in session 2 (Document) we obtained a correlation of 0.3962389776119233 when doing the same exercise but considering words themselves and not applying any word sense disambiguation technique. This time, by applying word sense disambiguation techniques, we have slightly improved the result.\n",
    "\n",
    "However, the result obtained in this session is slightly worse than the one obtained in session 3 (Morphology), when we used lemmas instead of the original words. Lemmatization gives us the canonical form of words. Therefore, all the words derived from a root word will be considered the same. Since we are keen on measuring semantical distances, this is useful, because in word-level settings, morphological information (at least in English) introduces \"noise\".\n",
    "\n",
    "Still, using word sense disambiguation techniques should obtain better results than just using the lemmas, which is a simpler technique, at least intuitevely. The reason why we believe this is happening is that the Lesk algorithm is too simple and has a relatively low accuracy. As a future work, more complex algorithms for word sense disambiguation could be investigated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
