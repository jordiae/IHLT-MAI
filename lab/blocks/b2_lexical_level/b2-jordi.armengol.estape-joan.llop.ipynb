{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 2: Lexical Level. SMS Spam Filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We start by downloading the provided dataset.\n",
    "\n",
    "Source: [UCI repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "It consists of 5574 instances of SMS messages (aggregated from different sources) belonging to either the class 'ham' or the class 'spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-05 12:49:02--  https://gebakx.github.io/ihlt/b2/resources/smsspamcollection.zip\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/zip]\n",
      "Saving to: ‘smsspamcollection.zip’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198,65K  --.-KB/s    in 0,08s   \n",
      "\n",
      "2019-11-05 12:49:03 (2,43 MB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://gebakx.github.io/ihlt/b2/resources/smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  smsspamcollection.zip\r\n",
      "  inflating: SMSSpamCollection       \r\n",
      "  inflating: readme                  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SMSSpamCollection', 'r') as f:\n",
    "    raw_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset effectively has 5574 lines with the following class distributions\n",
      "spam: 746 of 5574 (13.38%)\n",
      "ham: 4826 of 5574 (86.58%)\n"
     ]
    }
   ],
   "source": [
    "print('The dataset effectively has', len(raw_data), 'lines', 'with the following class distributions')\n",
    "labeled_data = []\n",
    "freqs = {}\n",
    "for line in raw_data:\n",
    "    label, text = line.split('\\t')\n",
    "    labeled_data.append((label, text))\n",
    "    if label in freqs:\n",
    "        freqs[label] += 1\n",
    "    else:\n",
    "        freqs[label] = 0\n",
    "for key, value in freqs.items():\n",
    "    percentage = 100*value/len(raw_data)\n",
    "    print(key + ':', value, 'of', len(raw_data), '(' + \"{0:.2f}\".format(percentage) + '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, which poses a challenge when applying machine learning techniques. If we excesively optimize accuracy, perhaps we obtain low precision or recall values for the minority class. We will see this point with more detail later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "In order to ease the task to the machine learning algorithm and decrease noise introduced by some variance that might be irrelevant to this problem, we will:\n",
    "    - Remove punctuation.\n",
    "    - Convert characters to lowercase.\n",
    "Ideally, we would like to take both punctuation and case into account, because perhaps texts with many capital letters and exclamation marks have higher probability of being spam. However, since we do not have a large database, in this case it would make the learning process more difficult.\n",
    "\n",
    "In addition, we will have a second version of the preprocessed dataset with lemmas instead of the original words. Lemmatization gives the canonical form of words, decreasing the variance introduced by morphology, and since we will be using representations that do not take into account the order of the words, like bag of words, decreasing the vocabulary and considering all the words with the same lemma as equal can ease the process of learning. We suspect that we will obtain better results with lemmas, but we will have to see the results of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "# nltk.download('punkt')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def remove_punctuation(token):\n",
    "    res = ''\n",
    "    for c in token:\n",
    "        if c not in string.punctuation:\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N','V'}:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0]\n",
    "\n",
    "preprocessed_data = []\n",
    "for label, text in labeled_data:\n",
    "    tokenized = nltk.word_tokenize(text.lower())\n",
    "    tokenized = [remove_punctuation(tok) for tok in tokenized]\n",
    "    tokenized = list(filter(None, tokenized))\n",
    "    pos_tags = pos_tag(tokenized)\n",
    "    lemmas = [lemmatize(pair) for pair in pos_tags]\n",
    "    preprocessed_data.append((label, tokenized, lemmas))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design\n",
    "The assignment suggests a \"single validation (50% - 50%)\" and a random shuffle. In this context, by validation we understand test, so we will have a train-test split of 50-50. We will provide a random seed in order to make the experiments reproducible. On the other hand, for selecting the best model with regard to generalization, we suggest performing a 5-fold cross-validation with the 50% of the training data. The test set will only be used for the final evaluation of the selected model.\n",
    "\n",
    "In the cross-validation, we will optimize certain hyperparameters and experiment with different options:\n",
    "- The 'C' hyperparameter in SVMs, for penalizing more or less the errors (points outside the decision boundaries).\n",
    "- Whether to use lemmas or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2787, 2787)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "shuffled_preprocessed_data = preprocessed_data.copy()\n",
    "random.seed(1)\n",
    "random.shuffle(shuffled_preprocessed_data)\n",
    "n = len(shuffled_preprocessed_data)\n",
    "train, test = shuffled_preprocessed_data[:n//2], shuffled_preprocessed_data[n//2:]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation\n",
    "The way of representing text is crucial for the application of machine learning techniques to natural language. Typically, each machine learning algorithm has an associated input format. In our case, we are keen on defining a custom kernel for sets in SVMs for the reasons we will see later on. Therefore, we will be working with sets, and the required text representation for our algorithm will be a bag of words, that is to say, a boolean vector codifying the presence of the indexed words. Bags of words encode the set of present words as a vector such that the present words will have its corresponding element set to true.\n",
    "\n",
    "This representation has some caveats, since it does not account for the frequency of the words, for instance, unlike other representations. However, it is the required one for the algorithm of our choice, and perhaps in short texts frequency is not that important. Among the proposed representations, term-frequency times inverse document-frequency seems to be the most robust one, since it avoids the effect of common words.\n",
    "\n",
    "In order to obtain an honest evaluation of the method, only words in the train set will be indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'i': 1431, 'to': 1126, 'you': 1091, 'a': 714, 'the': 676, 'u': 559, 'and': 478, 'is': 470, 'in': 438, 'me': 405, ...}),\n",
       " FreqDist({'i': 1431, 'be': 1229, 'to': 1126, 'you': 1091, 'a': 714, 'the': 676, 'u': 559, 'and': 478, 'in': 438, 'do': 437, ...}))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "train_tokens = [tokens for label, tokens, lemmas in train]\n",
    "flat_train_tokens = list(reduce(lambda x, y: x + y, train_tokens))\n",
    "fdist_tokens = nltk.FreqDist(flat_train_tokens)\n",
    "train_lemmas = [lemmas for label, tokens, lemmas in train]\n",
    "flat_train_lemmas = list(reduce(lambda x, y: x + y, train_lemmas))\n",
    "fdist_lemmas = nltk.FreqDist(flat_train_lemmas)\n",
    "fdist_tokens, fdist_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_bow_from_freq(fdist, tokens):\n",
    "    bow = []\n",
    "    for key in fdist:\n",
    "        if key in tokens:\n",
    "            bow.append(True)\n",
    "        else:\n",
    "            bow.append(False)\n",
    "    return bow\n",
    "\n",
    "def build_mat(fdist, texts):\n",
    "    mat = []\n",
    "    for text in texts:\n",
    "        bow = get_bow_from_freq(fdist, text)\n",
    "        mat.append(bow)\n",
    "    return np.array(mat)\n",
    "X_train_tokens = build_mat(fdist_tokens, train_tokens)\n",
    "X_train_lemmas = build_mat(fdist_lemmas, train_lemmas)\n",
    "test_tokens = [tokens for label, tokens, lemmas in test]\n",
    "test_lemmas = [lemmas for label, tokens, lemmas in test]\n",
    "X_test_tokens = build_mat(fdist_tokens, test_tokens)\n",
    "X_test_lemmas = build_mat(fdist_lemmas, test_lemmas)\n",
    "y_train = np.array([label for label, tokens, lemmas in train])\n",
    "y_test = np.array([label for label, tokens, lemmas in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "As we have anticipated, from the suggested methods in the assignment, our machine learning algorithm of choice will be SVMs. In particular, we will define a custom kernel for determining set similarity between two given sets of tokens (ie. the respective bags of words of two given texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2787, 5639) (2787,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9006099748833871,\n",
       " (array([0.8969494, 1.       ]),\n",
       "  array([1.        , 0.26329787]),\n",
       "  array([0.94567562, 0.41684211]),\n",
       "  array([2411,  376])))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gram matrix.B = A @ A.T\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "def set_kernel(A, B):\n",
    "    \"\"\"\n",
    "    We create a custom kernel:\n",
    "\n",
    "                 (2  0)\n",
    "    k(X, Y) = X  (    ) Y.T\n",
    "                 (0  1)\n",
    "    \"\"\"\n",
    "    #M = np.array([[2, 0], [0, 1.0]])\n",
    "    #return np.dot(np.dot(X, M), Y.T)\n",
    "    #print(X.shape, Y.shape)\n",
    "    #print((np.dot(np.dot(X, M), Y.T)).shape)\n",
    "    #return sum(X == Y)\n",
    "    return np.exp2(np.dot(A, B.T))\n",
    "    #return np.exp2(A @ B.T)#(np.dot(A,B.T))\n",
    "\n",
    "#print(X.shape, Y.shape)\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data.\n",
    "clf = svm.SVC(kernel=set_kernel, C=1.0)\n",
    "print(X_train_lemmas.shape, y_train.shape)\n",
    "clf.fit(X_train_lemmas, y_train)\n",
    "y_pred = clf.predict(X_test_lemmas)\n",
    "#precision : float (if average is not None) or array of float, shape = [n_unique_labels]\n",
    "#recall : float (if average is not None) or array of float, , shape = [n_unique_labels]\n",
    "#fbeta_score : float (if average is not None) or array of float, shape = [n_unique_labels]\n",
    "#support : int (if average is not None) or array of int, shape = [n_unique_labels]\n",
    "#The number of occurrences of each label in y_true.\n",
    "metrics.accuracy_score(y_true=y_test, y_pred=y_pred), metrics.precision_recall_fscore_support(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
