{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 2: Lexical Level. SMS Spam Filtering.\n",
    "Jordi Armengol - Joan Llop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We start by downloading the provided dataset.\n",
    "\n",
    "Source: [UCI repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "It consists of 5574 instances of SMS messages (aggregated from different sources) belonging to either the class 'ham' or the class 'spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-05 12:49:02--  https://gebakx.github.io/ihlt/b2/resources/smsspamcollection.zip\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/zip]\n",
      "Saving to: ‘smsspamcollection.zip’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198,65K  --.-KB/s    in 0,08s   \n",
      "\n",
      "2019-11-05 12:49:03 (2,43 MB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://gebakx.github.io/ihlt/b2/resources/smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  smsspamcollection.zip\r\n",
      "  inflating: SMSSpamCollection       \r\n",
      "  inflating: readme                  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SMSSpamCollection', 'r') as f:\n",
    "    raw_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset effectively has 5574 lines with the following class distributions\n",
      "spam: 746 of 5574 (13.38%)\n",
      "ham: 4826 of 5574 (86.58%)\n"
     ]
    }
   ],
   "source": [
    "print('The dataset effectively has', len(raw_data), 'lines', 'with the following class distributions')\n",
    "labeled_data = []\n",
    "freqs = {}\n",
    "for line in raw_data:\n",
    "    label, text = line.split('\\t')\n",
    "    labeled_data.append((label, text))\n",
    "    if label in freqs:\n",
    "        freqs[label] += 1\n",
    "    else:\n",
    "        freqs[label] = 0\n",
    "for key, value in freqs.items():\n",
    "    percentage = 100*value/len(raw_data)\n",
    "    print(key + ':', value, 'of', len(raw_data), '(' + \"{0:.2f}\".format(percentage) + '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, which poses a challenge when applying machine learning techniques. If we excesively optimize accuracy, perhaps we obtain low precision or recall values for the minority class. We will see this point with more detail later on.\n",
    "\n",
    "In the particular case of spam, having a high precision for the spam class and low recall for the ham class should be the priority, because the cost of miss-classifying a legit message as spam is way higher than the opposite. However, we do not want to take this criterium to the extremes, because such classifier would be useless and actually behave like a constant predictor that always outputs 'ham'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "In order to ease the task to the machine learning algorithm and decrease noise introduced by some variance that might be irrelevant to this problem, we will, as suggested by the assignment:\n",
    "    - Remove punctuation.\n",
    "    - Convert characters to lowercase.\n",
    "\n",
    "Ideally, we would like to take both punctuation and case into account, because perhaps texts with many capital letters and exclamation marks have higher probability of being spam. However, since we do not have a large database, in this case it would make the learning process more difficult.\n",
    "\n",
    "Moreover, we will perform an additional pre-processing step: removing stop words, which are common words that most search engines are programmed to ignore. Examples of stop words are articles such as \"the\". Probably, these kind of words are introducing noise.\n",
    "\n",
    "In addition, we will use lemmas instead of the original words. Lemmatization gives the canonical form of words, decreasing the variance introduced by morphology, and since we will be using representations that do not take into account the order of the words, like bag of words, decreasing the vocabulary and considering all the words with the same lemma as equal can ease the process of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "# nltk.download('punkt')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "def remove_punctuation(token):\n",
    "    res = ''\n",
    "    for c in token:\n",
    "        if c not in string.punctuation:\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N','V'}:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0]\n",
    "\n",
    "preprocessed_data = []\n",
    "for label, text in labeled_data:\n",
    "    tokenized = nltk.word_tokenize(text.lower())\n",
    "    tokenized = [remove_punctuation(tok) for tok in tokenized if tok not in stopwords_set]\n",
    "    tokenized = list(filter(None, tokenized))\n",
    "    pos_tags = pos_tag(tokenized)\n",
    "    lemmas = [lemmatize(pair) for pair in pos_tags]\n",
    "    preprocessed_data.append((label, tokenized, lemmas))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design\n",
    "The assignment suggests a \"single validation (50% - 50%)\" and a random shuffle. In this context, by validation we understand test, so we will have a train-test split of 50-50. We will provide a random seed in order to make the experiments reproducible. The test set will only be used for the final evaluation of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2787, 2787)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "shuffled_preprocessed_data = preprocessed_data.copy()\n",
    "random.seed(1)\n",
    "random.shuffle(shuffled_preprocessed_data)\n",
    "n = len(shuffled_preprocessed_data)\n",
    "train, test = shuffled_preprocessed_data[:n//2], shuffled_preprocessed_data[n//2:]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation\n",
    "The way of representing text is crucial for the application of machine learning techniques to natural language. Typically, each machine learning algorithm has an associated input format. In our case, we are keen on defining a custom kernel for sets in SVMs for the reasons we will see later on. Therefore, we will be working with sets, and the required text representation for our algorithm will be a bag of words, that is to say, a boolean vector codifying the presence of the indexed words. Bags of words encode the set of present words as a vector such that the present words will have its corresponding element set to true.\n",
    "\n",
    "This representation has some caveats, since it does not account for the frequency of the words, for instance, unlike other representations. However, it is the required one for the algorithm of our choice, and perhaps in short texts frequency is not that important. Among the proposed representations, term-frequency times inverse document-frequency seems to be the most robust one, since it avoids the effect of common words.\n",
    "\n",
    "In order to obtain a honest evaluation of the method, only words in the train set will be indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'u': 559, 'call': 281, 's': 267, '2': 229, 'ur': 209, 'get': 191, 'm': 190, 'nt': 180, '4': 172, 'gt': 154, ...}),\n",
       " FreqDist({'u': 559, 'get': 345, 'call': 323, 'go': 274, 's': 267, '2': 229, 'ur': 209, 'm': 190, 'come': 181, 'nt': 180, ...}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "train_tokens = [tokens for label, tokens, lemmas in train]\n",
    "flat_train_tokens = list(reduce(lambda x, y: x + y, train_tokens))\n",
    "fdist_tokens = nltk.FreqDist(flat_train_tokens)\n",
    "train_lemmas = [lemmas for label, tokens, lemmas in train]\n",
    "flat_train_lemmas = list(reduce(lambda x, y: x + y, train_lemmas))\n",
    "fdist_lemmas = nltk.FreqDist(flat_train_lemmas)\n",
    "fdist_tokens, fdist_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_bow_from_freq(fdist, tokens):\n",
    "    bow = []\n",
    "    for key in fdist:\n",
    "        if key in tokens:\n",
    "            bow.append(True)\n",
    "        else:\n",
    "            bow.append(False)\n",
    "    return bow\n",
    "\n",
    "def build_mat(fdist, texts):\n",
    "    mat = []\n",
    "    for text in texts:\n",
    "        bow = get_bow_from_freq(fdist, text)\n",
    "        mat.append(bow)\n",
    "    return np.array(mat)\n",
    "\n",
    "X_train_lemmas = build_mat(fdist_lemmas, train_lemmas)\n",
    "test_lemmas = [lemmas for label, tokens, lemmas in test]\n",
    "X_test_lemmas = build_mat(fdist_lemmas, test_lemmas)\n",
    "y_train = np.array([label for label, tokens, lemmas in train])\n",
    "y_test = np.array([label for label, tokens, lemmas in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning: SVM with set kernel\n",
    "As we have anticipated, from the suggested methods in the assignment, our machine learning algorithm of choice will be SVMs. In particular, we will define a custom kernel for determining set similarity between two given sets of tokens (ie. the respective bags of words of two given texts).\n",
    "\n",
    "The reasons we have for using this method are the following:\n",
    "- For the sake of learning and personal interests: We have used SVMs before in other subjects, but we had never used a custom kernel. In particular, we defined the set kernel as a theoretical exercise found in Bishop's Pattern Recognition and Machine Learning in the Machine Learning subject from the Computer Science degree at UPC, and demonstrated that it fulfills the kernel properties, but we had never actually coded it.\n",
    "- Because SVMs are a powerful algorithm and we believe that can lead to good results in this case.\n",
    "\n",
    "In a nutshell, an SVM (Support Vector Machines) is a supervised machine learning algorithm, tyically used for classification, that learns to define a decision boundary (specifically, a hyperplane) for separating the different classes. The algorithm itself is linear, but if the kernel, which is the function used for defining the similarity between two given instances, is not, then the model is non-linear. In our case, the kernel will be linear, but it is a custom one, because we want to define similarities between sets, not vectors.\n",
    "\n",
    "If we encode a (finite) set as a boolean vector such that if the i-th element is defined as 'true' then the corresponding element of the set is present, we can compute the dot product between the boolean vectors of two instances. It will be equal to the number of elements that they have in common (so, the cardinality of the intersection). We can take 2 to the power of this cardinality instead, and the function will still be a valid kernel (see the properties of kernels in Bishop's Pattern Recognition and Machine Learning). This exponentiation can be useful for giving much more importance to the elements in common the more elements in common there are, instead of a linear growth.\n",
    "\n",
    "Specifically, we define and implement the kernel set as:\n",
    "\n",
    "$\\kappa(S_1, S_2) = 2^{\\vert S1 \\cap S2\\vert} = \\sum_{j = 0}^{|U|-1} \\mathcal{I}_x(u_j) \\mathcal{I}_{x'}(u_j)$\n",
    "\n",
    "with U being the universe of the possible elements that can appear in our set instances and\n",
    "\n",
    "$\\mathcal{I}_x(u) :=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ u \\notin x\\\\\n",
    "      1, & \\text{oteh}\n",
    "    \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9024040186580553\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.90      1.00      0.95      2411\n",
      "        spam       1.00      0.28      0.43       376\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2787\n",
      "   macro avg       0.95      0.64      0.69      2787\n",
      "weighted avg       0.91      0.90      0.88      2787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "def set_kernel(A, B):\n",
    "    \"\"\"During training, A and B are equal. In particular, they are the\n",
    "    training data matrix, X (design matrix). In inference, A is the\n",
    "    training X and B is the test X. We return a custom Gram matrix,\n",
    "    in the sense that we define our own 'inner product' for boolean\n",
    "    vectors representing sets. It can be interpreted as set similarity.\"\"\"\n",
    "    return np.exp2(np.dot(A, B.T))\n",
    "\n",
    "classifier = svm.SVC(kernel=set_kernel, C=1.0)\n",
    "classifier.fit(X_train_lemmas, y_train)\n",
    "y_pred = classifier.predict(X_test_lemmas)\n",
    "print('accuracy =', metrics.accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print()\n",
    "print(metrics.classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The obtained accuracy score *seems* high, but it is **not**. Recall that the dataset was imbalanced and it is a binary classification problem, so a constant classifier that always returned 'ham' would have an accuracy of 0.8658%, which is the percentage of the majority class. In this sense, our results are not impressive, just slightly better than a constant classifier.\n",
    "\n",
    "However, the precision and recall are way better than the ones we could expect from a constant classifier. In particular, they fulfill the requirements we said in the beginning. The precision of the spam class (and the recall of the ham class) is maximal, which means that no legit SMS is miss-classified as spam, which would have a higher cost than the opposite error. However, even with this condition, we still detect almost 30% of the SMSs messages, so our classifier is useful for filtering almost one third of the spam without removing by mistake any legit messages.\n",
    "\n",
    "Note that our kernel implementation is very efficienty, because we have written pure Numpy vectorized code, without loops. It operates over the whole data matrix instead of going instance by instance. \n",
    "\n",
    "Notice that we have tried other approaches, like not using lemas and not discarding stop words, ans more simple machine learning algorithms, like K-NN or Naïve Bayes, but the results with these other approaches were not promising in some early experiments we performed in a subset of our training set. Optimizing the 'C' hyperparameter of the SVM (for penalizing the points outside the error) did not improve our results either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
