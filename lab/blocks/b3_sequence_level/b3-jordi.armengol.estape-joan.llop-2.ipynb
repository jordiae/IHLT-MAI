{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 3: Sequence Level. Building a NER chunker with CRF.\n",
    "Jordi Armengol - Joan Llop\n",
    "\n",
    "In this block we want to discover the name entities of sentences using conditional random fields. \n",
    "#### CRFs \n",
    "Usually a neural network is model that takes a single input and returns the most likely label, but with conditional random fields, the previous inputs and the next inputs matter in the task of assigning a label to an instance. Therefore, we can think of them as a way of modeling the join distribution of a whole sequence of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "# import pycrfsuite\n",
    "from nltk.tag import CRFTagger\n",
    "# gensim can be downloaded using pip install -U gensim\n",
    "import gensim # m'ha semblat inutil, crec que ho hauriem de borrar de cara a l'entrega\n",
    "from gensim.models import Word2Vec # \" \"\n",
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'NNP', 'I-ORG'),\n",
       " ('rejects', 'VBZ', 'O'),\n",
       " ('German', 'JJ', 'I-MISC'),\n",
       " ('call', 'NN', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('boycott', 'VB', 'O'),\n",
       " ('British', 'JJ', 'I-MISC'),\n",
       " ('lamb', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = ConllCorpusReader('conll2003', 'eng.train', ['words', 'pos', 'ne', 'chunk']).iob_sents()[1:]\n",
    "testa = ConllCorpusReader('conll2003', 'eng.testa', ['words', 'pos', 'ne', 'chunk']).iob_sents()[1:]\n",
    "testb = ConllCorpusReader('conll2003', 'eng.testb', ['words', 'pos', 'ne', 'chunk']).iob_sents()[1:]\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### The features\n",
    "In order to train our CRF we need to create the features (from the data we have the words and the pos). We have decided to use the following features:\n",
    "- The word in lowercase\n",
    "- The POS\n",
    "- The lenght of the word\n",
    "- A bool that indicates if the word is the beginning of the sentence\n",
    "- A bool that indicates if the word is the end of the sentence\n",
    "- A bool that indicates if the word is all in uppercase\n",
    "- A bool that indicates if the word is a digit\n",
    "- A bool that indicates if the word is a title\n",
    "\n",
    "We repeat all these features for the two previous words and for the two next words of the sentence (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_and_pos_tags_from_tokens(tokens):\n",
    "    words, pos_tags = [[], []]\n",
    "    for token in tokens:\n",
    "        word, pos_tag = token.split(' ')\n",
    "        words.append(word)\n",
    "        pos_tags.append(pos_tag)\n",
    "    return words, pos_tags\n",
    "\n",
    "\n",
    "def get_word_features(tokens, i):\n",
    "    words, pos_tags = get_words_and_pos_tags_from_tokens(tokens)\n",
    "    word = words[i]\n",
    "    pos_tag = pos_tags[i]\n",
    "    features = []\n",
    "    features.append('lowercase word: ' + word.lower()) # word in lowercase\n",
    "    features.append('postag: ' + str(pos_tag)) # Postag                   \n",
    "    features.append('lenght of word: ' + str(len(words))) # lenght of word\n",
    "    features.append('BOS: ' + str(i==0)) # beggining of a sentence\n",
    "    features.append('EOF: ' + str(i==len(words)-1)) # end of sentence\n",
    "    features.append('word is upper: ' + str(word.isupper())) # is uppercase\n",
    "    features.append('word is digit: ' + str(word.isdigit())) # is a digit\n",
    "    features.append('word is title: ' + str(word.istitle())) # is a title\n",
    "    if (i > 0):\n",
    "        previous_word = words[i-1]\n",
    "        pos_tag = pos_tags[i-1]\n",
    "        features.append('lowercase previous word: ' + previous_word.lower())\n",
    "        features.append('postag previous word: ' + str(pos_tag))\n",
    "        features.append('lenght of previous word: ' + str(len(previous_word)))\n",
    "        features.append('previous word is BOS: ' + str(i-1==0))\n",
    "        features.append('previous word is EOF: ' + str(i-1==len(words)-1))\n",
    "        features.append('previous word is upper: ' + str(previous_word.isupper()))\n",
    "        features.append('previous word is digit: ' + str(previous_word.isdigit()))\n",
    "        features.append('previous word is title: ' + str(previous_word.istitle()))\n",
    "    if (i > 1):\n",
    "        previous_word = words[i-2]\n",
    "        pos_tag = pos_tags[i-2]\n",
    "        features.append('lowercase second previous word: ' + previous_word.lower())\n",
    "        features.append('postag second previous word: ' + str(pos_tag))\n",
    "        features.append('lenght of second previous word: ' + str(len(previous_word)))\n",
    "        features.append('second previous word is BOS: ' + str(i-2==0))\n",
    "        features.append('second previous word is EOF: ' + str(i-2==len(words)-1))\n",
    "        features.append('second previous word is upper: ' + str(previous_word.isupper()))\n",
    "        features.append('second previous word is digit: ' + str(previous_word.isdigit()))\n",
    "        features.append('second previous word is title: ' + str(previous_word.istitle()))\n",
    "    if (i < len(words)-1):\n",
    "        next_word = words[i+1]\n",
    "        pos_tag = pos_tags[i+1]\n",
    "        features.append('lowercase next word: ' + next_word.lower())\n",
    "        features.append('postag next word: ' + str(pos_tag))\n",
    "        features.append('lenght of next word: ' + str(len(next_word)))\n",
    "        features.append('next word is BOS: ' + str(i+1==0))\n",
    "        features.append('next word is EOF: ' + str(i+1==len(words)-1))\n",
    "        features.append('next word is upper: ' + str(next_word.isupper()))\n",
    "        features.append('next word is digit: ' + str(next_word.isdigit()))\n",
    "        features.append('next word is title: ' + str(next_word.istitle()))\n",
    "    if (i < len(words)-2):\n",
    "        next_word = words[i+2]\n",
    "        pos_tag = pos_tags[i+2]\n",
    "        features.append('lowercase second next word: ' + next_word.lower())\n",
    "        features.append('postag second next word: ' + str(pos_tag))\n",
    "        features.append('lenght of second next word: ' + str(len(next_word)))\n",
    "        features.append('second next word is BOS: ' + str(i+2==0))\n",
    "        features.append('second next word is EOF: ' + str(i+2==len(words)-1))\n",
    "        features.append('second next word is upper: ' + str(next_word.isupper()))\n",
    "        features.append('second next word is digit: ' + str(next_word.isdigit()))\n",
    "        features.append('second next word is title: ' + str(next_word.istitle()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_token_label(word_tag_ne):\n",
    "    word, tag, label = word_tag_ne\n",
    "    return (word + ' ' + tag, label)\n",
    "\n",
    "\n",
    "def get_list_of_word_tag_ne(sent):\n",
    "    return [get_token_label(word_tag_ne) for word_tag_ne in sent]\n",
    "\n",
    "\n",
    "def get_tokens_from_iob_sents(iob_corpus):\n",
    "    tokens = [get_list_of_word_tag_ne(sent) for sent in iob_corpus]\n",
    "    return tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945 / 14986\n"
     ]
    }
   ],
   "source": [
    "train_tokens = get_tokens_from_iob_sents(train)\n",
    "testa_tokens = get_tokens_from_iob_sents(testa)\n",
    "testb_tokens = get_tokens_from_iob_sents(testb)\n",
    "emp = 0\n",
    "for t in train_tokens:\n",
    "    if (len(t) == 0):\n",
    "        emp += 1\n",
    "print(emp, '/', len(train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'conll2003-eng.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b36249fd1980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRFTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conll2003-eng.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tag/crf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, model_file)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "CRF = CRFTagger()\n",
    "CRF.train(train_tokens, 'conll2003-eng.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
