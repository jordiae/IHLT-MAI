{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block 3: Sequence Level. Building a NER with CRF.\n",
    "Jordi Armengol - Joan Llop\n",
    "\n",
    "In this block, we want to perform Named Entity Recognition and Classification (NERC) in the given corpus, Conll2003, for the following entities:\n",
    "- Miscellaneous.\n",
    "- Organizations.\n",
    "- Locations.\n",
    "- Persons.\n",
    "\n",
    "NERC usually requires sequence-level models, since words alone are not enough for disambiguating some cases. For this reason, we will use a sequence-level model, CRFs.\n",
    "\n",
    "#### CRFs \n",
    "\n",
    "Conditional Random Fields are a kind of discriminative model (ie. not generative) based on undirected probabilistic graphs. We can think of them as a way of modeling the joint distribution of a whole sequence of inputs.\n",
    "\n",
    "We will use pycrfsuite, a Python library for CRFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "import pycrfsuite\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "We will use the english files of the Conll2003 corpus. A 'conll2003' folder with the files 'eng.train', 'eng.testa' and 'eng.testb' is required to be in the same folder as the notebook.\n",
    "\n",
    "The data will be a list of sentences of tripplets: (word, pos, ne). we will use the testa for validation purposes and the testb as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('conll2003', 'eng.train', ['words', 'pos', 'ne', 'chunk']).iob_sents()[1:]\n",
    "testa = ConllCorpusReader('conll2003', 'eng.testa', ['words', 'pos', 'ne', 'chunk']).iob_sents()[1:]\n",
    "testb = ConllCorpusReader('conll2003', 'eng.testb', ['words', 'pos', 'ne', 'chunk']).iob_sents()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### Data cleaning, tokenization and other preprocessing issues\n",
    "ConllCorpusReader already provides the tokenized sentences, Part-of-Speech and annotations. No further preprocessing is required as far as these matters are concerned.\n",
    "\n",
    "## Feature engineering\n",
    "In order to train our CRF, we need to create our own feature set to represent samples. We have done some research for knowing which are the usual features used in these cases (ie. NER) and examined examples of using pycrfsuite. Some of these features are pretty obvious, like whether the word starts with uppercase, in which case, the probability of being a named entity will be way higher.\n",
    "\n",
    "We have decided to experiment with the following features:\n",
    "- The word in lowercase\n",
    "- The POS\n",
    "- The length of the word\n",
    "- A bool that indicates if the word is the beginning of the sentence\n",
    "- A bool that indicates if the word is the end of the sentence\n",
    "- A bool that indicates if the word is all in uppercase\n",
    "- A bool that indicates if the word is a digit\n",
    "- A bool that indicates if the word starts with an uppercase character.\n",
    "\n",
    "We repeat all these features for the two previous words and for the two next words of the sentence (if they exist). We have used testa (the validation set) for assuring that these features are actually useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_sent(sent):\n",
    "    return [words for words, postag, label in sent]\n",
    "\n",
    "def get_individual_word_features(word, words, sent, index, id_):\n",
    "    features = {}\n",
    "    features[id_ + 'lowercase_word'] = word.lower() # word in lowercase\n",
    "    features[id_ + 'postag'] = str(sent[index][1]) # Part-of-Speech                   \n",
    "    features[id_ + 'length'] = str(len(words)) # length of word\n",
    "    features[id_ + 'BOS'] = str(index==0) # beggining of a sentence\n",
    "    features[id_ + 'EOF'] = str(index==len(words)-1) # end of sentence\n",
    "    features[id_ + 'is_upper'] = str(word.isupper()) # is uppercase\n",
    "    features[id_ + 'is_digit'] = str(word.isdigit()) # is a digit\n",
    "    features[id_ + 'starts_upper'] = str(word.istitle()) # starts with uppercase\n",
    "    return features\n",
    "    \n",
    "def get_word_features(sent, i):\n",
    "    words = get_words_from_sent(sent)\n",
    "    word = words[i]\n",
    "    features = get_individual_word_features(word, words, sent, i, '')\n",
    "    if i > 0:\n",
    "        previous_word1 = words[i-1]\n",
    "        previous1_word_features = get_individual_word_features(previous_word1, words, sent, i-1, 'previous1_')\n",
    "        features = dict(**features, **previous1_word_features)\n",
    "    if i > 1:\n",
    "        previous_word2 = words[i-2]\n",
    "        previous2_word_features = get_individual_word_features(previous_word2, words, sent, i-2, 'previous2_')\n",
    "        features = dict(**features, **previous2_word_features)\n",
    "    if i < len(words)-1:\n",
    "        next_word1 = words[i+1]\n",
    "        next_word1_features = get_individual_word_features(next_word1, words, sent, i+1, 'next1_')\n",
    "        features = dict(**features, **next_word1_features)\n",
    "    if i < len(words)-2:\n",
    "        next_word2 = words[i+2]\n",
    "        next_word2_features = get_individual_word_features(next_word2, words, sent, i+2, 'next2_')\n",
    "        features = dict(**features, **next_word2_features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_sentence_features(sent):\n",
    "    #return pycrfsuite.ItemSequence([get_word_features(sent, i) for i in range(len(sent))])\n",
    "    return [get_word_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def get_features(corpus):\n",
    "    return [get_sentence_features(sent) for sent in corpus]\n",
    "    \n",
    "                        \n",
    "def get_sentence_labels(sent):\n",
    "    return [label for words, postag, label in sent]\n",
    "                        \n",
    "                        \n",
    "def get_labels(corpus):\n",
    "    return [get_sentence_labels(sent) for sent in corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train phase\n",
    "We separate the data in features and the labels and use the previous functions for building the feature sets. We will use the validation set for assuring that the features that we have thought of are actually useful. However, for computational constraints, we will not check all the combinations. Instead, we will test feature by feature and we will assume that if they are useful individually, the aggregate will be useful as well. We will not test the length of the context (the 2 previous words and the 2 next words) for the same reason (computational time).\n",
    "\n",
    "We add all sentences (where each word has been converted to features) to the respective CRFs. The resulting models will be saved in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with postag...\n",
      "Training with length...\n",
      "Training with BOS...\n",
      "Training with EOF...\n",
      "Training with is_upper...\n",
      "Training with is_digit...\n",
      "Training with starts_upper...\n",
      "Training with words only...\n",
      "Training with words and all features...\n",
      "CPU times: user 34min 49s, sys: 1.04 s, total: 34min 50s\n",
      "Wall time: 34min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def select(instances, feature_names_to_select):\n",
    "    res = []\n",
    "    for instance in instances:\n",
    "        res.append(pycrfsuite.ItemSequence([{k: s[k] for k in feature_names_to_select} for s in instance]))\n",
    "    return res\n",
    "    \n",
    "train_features = get_features(train)\n",
    "train_labels = get_labels(train)\n",
    "feature_names = ['postag', 'length', 'BOS', 'EOF', 'is_upper', 'is_digit', 'starts_upper']\n",
    "for feature_name in feature_names:\n",
    "    print('Training with', feature_name + '...')\n",
    "    train_features_ = select(train_features, ['lowercase_word', feature_name])\n",
    "    CRF = pycrfsuite.Trainer(verbose=False)\n",
    "    for x, y in zip(train_features, train_labels):\n",
    "        CRF.append(x, y)\n",
    "    CRF.train('conll2003-eng-' + feature_name + '.model')\n",
    "print('Training with words only...')\n",
    "train_features_ = select(train_features, ['lowercase_word'])\n",
    "CRF = pycrfsuite.Trainer(verbose=False)\n",
    "for x, y in zip(train_features_, train_labels):\n",
    "    CRF.append(x, y)\n",
    "CRF.train('conll2003-eng-words.model')\n",
    "print('Training with words and all features...')\n",
    "CRF = pycrfsuite.Trainer(verbose=False)\n",
    "for x, y in zip(train_features, train_labels):\n",
    "    CRF.append(x, y)\n",
    "CRF.train('conll2003-eng-all.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and model selection phase\n",
    "We select the model with the highest accuracy in the validation (testa) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy using postag = 0.07526965460846541\n",
      "accuracy using length = 0.06545695261087964\n",
      "accuracy using BOS = 0.0630427164051244\n",
      "accuracy using EOF = 0.05737704918032787\n",
      "accuracy using is_upper = 0.13657957244655583\n",
      "accuracy using is_digit = 0.03775164518515634\n",
      "accuracy using starts_upper = 0.43722985865036407\n",
      "accuracy using words only = 0.9040535804680503\n",
      "accuracy using words and all features = 0.969705229547136\n"
     ]
    }
   ],
   "source": [
    "testa_features = get_features(testa)\n",
    "testa_labels = get_labels(testa)\n",
    "feature_names = ['postag', 'length', 'BOS', 'EOF', 'is_upper', 'is_digit', 'starts_upper']\n",
    "y_true = []\n",
    "for sentence_labels in testa_labels:\n",
    "    for label in sentence_labels:\n",
    "        y_true.append(label)\n",
    "best_accuracy = 0\n",
    "best_model = ''\n",
    "for feature_name in feature_names:\n",
    "    testa_features_ = select(testa_features, ['lowercase_word', feature_name])\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open('conll2003-eng-' + feature_name + '.model')\n",
    "    y_pred = []\n",
    "    for sentence_pred in [tagger.tag(x) for x in testa_features_]:\n",
    "        for pred in sentence_pred:\n",
    "            y_pred.append(pred)\n",
    "    accuracy = metrics.accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print('accuracy using ' + feature_name + ' = ' + str(accuracy))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = 'conll2003-eng-' + feature_name + '.model'\n",
    "\n",
    "        \n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2003-eng-words.model')\n",
    "testa_features_ = select(testa_features, ['lowercase_word'])\n",
    "y_pred = []\n",
    "for sentence_pred in [tagger.tag(x) for x in testa_features_]:\n",
    "    for pred in sentence_pred:\n",
    "        y_pred.append(pred)\n",
    "accuracy = metrics.accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "print('accuracy using words only = ' + str(accuracy))\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_model = 'conll2003-eng-words.model'\n",
    "\n",
    "        \n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2003-eng-all.model')\n",
    "y_pred = []\n",
    "for sentence_pred in [tagger.tag(x) for x in testa_features]:\n",
    "    for pred in sentence_pred:\n",
    "        y_pred.append(pred)\n",
    "accuracy = metrics.accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "print('accuracy using words and all features = ' + str(accuracy))\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_model = 'conll2003-eng-all.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test phase\n",
    "\n",
    "We have used testb for the final test, applying the selected model. This set has not been used for training or model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9534618283622268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordiae/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.82      0.83      0.82      1919\n",
      "      I-MISC       0.73      0.69      0.71       909\n",
      "       I-ORG       0.76      0.75      0.76      2491\n",
      "       I-PER       0.84      0.89      0.87      2773\n",
      "           O       0.99      0.98      0.99     38323\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     46435\n",
      "   macro avg       0.52      0.52      0.52     46435\n",
      "weighted avg       0.95      0.95      0.95     46435\n",
      "\n",
      "confusion matrix: \n",
      "[[    0     0     0     0     0     4     2     0]\n",
      " [    0     0     0     0     5     1     2     1]\n",
      " [    0     0     0     0     0     4     1     0]\n",
      " [    0     0     0  1585    33   122    82    97]\n",
      " [    0     0     0    56   625    57    40   131]\n",
      " [    0     0     0   145    66  1876   218   186]\n",
      " [    0     0     0    56    13   130  2471   103]\n",
      " [    0     0     0    89   112   284   121 37717]]\n"
     ]
    }
   ],
   "source": [
    "testb_features = get_features(testb)\n",
    "testb_labels = get_labels(testb)\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(best_model)\n",
    "y_pred = []\n",
    "for sentence_pred in [tagger.tag(x) for x in testb_features]:\n",
    "    for pred in sentence_pred:\n",
    "        y_pred.append(pred)\n",
    "        \n",
    "y_test = []\n",
    "for sentence_labels in testb_labels:\n",
    "    for label in sentence_labels:\n",
    "        y_test.append(label)\n",
    "        \n",
    "# Print results \n",
    "print('accuracy =', metrics.accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print(metrics.classification_report(y_true=y_test, y_pred=y_pred))\n",
    "print('confusion matrix: ')\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We can see that the more data the better. In the classes where there are almost no data (B-LOC, B-MISC and B-ORG, since apparently most entities are composed of a single word) we are not able to predict any instance, while with the classes with more data our model performs much better. In the class 'O' (there is no name entity) we get more than 98 percent of precision and recall (we have to remark that a classifier that assign all claases to 'O' will get 0.82 of precision). We can see that we predict better the class I-LOC than the class I-ORG, but the latter class has more instances than the former. One possible explanation to this anomaly is the length of each class: the names of organizations are usually longer than the locations (name of cities, for instance).\n",
    "\n",
    "With regard to model selection, we have seen that all the features we experimented with seem to be useful, as we can see in our partial ablation study in the model selection and validation phase. However, some features seem to be way more important than others, specially the feature denoting whether the given word starts with capital letters, which seems to be logical, in the sense that named entities will usually start with a capital letter. The importance of some other features seems to be marginal.\n",
    "\n",
    "Finally, we conclude that we have not overfitted to the validation set, since the results with the best model are consistent with the ones obtained in the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
