{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab block 1: Text level\n",
    "\n",
    "Jordi Armengol Estapé, Joan Llop Palao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from math import inf, log\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We define a function for preprocessing the corpus. It will read the sentences of each line, convert them to lowercase, remove digits and continuous white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(text_lines,l=None):\n",
    "    preprocessed = []\n",
    "    for index, line in enumerate(text_lines):\n",
    "        num, sentence = line.split('\\t')\n",
    "        new_line = (re.sub(\"\\d\", \"\", sentence)).lower()\n",
    "        new_line = ' '.join(new_line.split())\n",
    "        preprocessed.append(new_line)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the aforementioned function to the whole corpus, including both train and test sets for all languages.\n",
    "\n",
    "Notice that in the case of the train set, we will concatenate all the sentences (separated by a double space, denoting end of sentence), while for the test set the sentences will remain separate, because for evaluating we have to go sentence by sentence. However, we will add double spaces to the sentences in test, in order to be preprocessed in the same way as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['eng', 'deu', 'fra', 'ita', 'nld', 'spa']\n",
    "dataset = {}\n",
    "for l in langs:\n",
    "    dataset[l] = {}\n",
    "    with open('langId/' + l + '_trn.txt', 'r') as file:\n",
    "        dataset[l]['trn'] = file.readlines()\n",
    "    with open('langId/' + l + '_tst.txt', 'r') as file:\n",
    "        dataset[l]['tst'] = file.readlines()\n",
    "    dataset[l]['trn'] = '  ' + '  '.join(preprocessing(dataset[l]['trn'],l)) + '  '\n",
    "    dataset[l]['tst'] = ['  ' + sentence + '  ' for sentence in preprocessing(dataset[l]['tst'],l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models and training\n",
    "\n",
    "We will define a generic trigram LanguageModel class, which will be initialized with the cleaned corpus of its corresponding language.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We will extract all the trigrams (with their respective frequencies) and filter out the ones with less than 5 occurrences (as said in the assignment). For approximating the probabilities, as described in Jurafsky’s book, we will need bigrams as well.\n",
    "\n",
    "### Infer\n",
    "\n",
    "As described in Jurafsky’s book, instead of computing the exact probabilities by taking into account the whole sequences, we approximate the probability of a given trigram by dividing the frequency of the said trigram (ci) by the frequency of the corresponding starting bigram (N). We apply Laplace smoothing with the trigram vocabualary size (V) and lambda = 1.\n",
    "\n",
    "### Eval\n",
    "\n",
    "Given a sentence, the function eval(sentence) will compute the log probability of the sentence by summing the logarithms of the probabilities of each trigram in the sentence (using the infer(trigram) function). We used the log probability because with vanilla probabilities we got 0 many times due to floating point precision errors. We could have used perplexity as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, clean_corpus, lang):\n",
    "        self.trigrams = TrigramCollocationFinder.from_words(clean_corpus)\n",
    "        self.trigrams.apply_freq_filter(5)\n",
    "        self.bigrams = self.trigrams.bigram_finder()\n",
    "        self.lambd = 1\n",
    "        self.V = len(self.trigrams.ngram_fd)\n",
    "        self.lang = lang\n",
    "            \n",
    "    def infer(self, trigram):\n",
    "        # N: bigram_freq, V: train trigrams vocabulary size, ci: trigram frequency\n",
    "        ci = self.trigrams.ngram_fd[trigram]\n",
    "        (t1, t2, t3) = trigram\n",
    "        N = self.bigrams.ngram_fd[(t1, t2)]\n",
    "        return (ci + self.lambd) / (N + self.lambd * self.V)\n",
    "        \n",
    "        \n",
    "    def eval(self, sentence):\n",
    "        # log probablity, not perplexity\n",
    "        p = 1\n",
    "        trigrams = TrigramCollocationFinder.from_words(sentence)\n",
    "        for trigram in trigrams.ngram_fd:\n",
    "            p += log(self.infer(trigram))\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each language, we instantiate its corresponding LanguageModel object. Notice that instead of storing a table with the computed probabilities we just look up it dynamically (because we believe that it's more efficient), so the training time it's really fast, since it only extracts the corresponding trigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training English ...\n",
      "Training Deutsch ...\n",
      "Training Français ...\n",
      "Training Italiano ...\n",
      "Training Nederlands ...\n",
      "Training Español ...\n",
      "Training time: 75.50163626670837\n"
     ]
    }
   ],
   "source": [
    "lt0 = time.time()\n",
    "print('Training English ...')\n",
    "engModel = LanguageModel(dataset['eng']['trn'], 'eng')\n",
    "print('Training Deutsch ...')\n",
    "deuModel = LanguageModel(dataset['deu']['trn'], 'deu')\n",
    "print('Training Français ...')\n",
    "fraModel = LanguageModel(dataset['fra']['trn'], 'fra')\n",
    "print('Training Italiano ...')\n",
    "itaModel = LanguageModel(dataset['ita']['trn'], 'ita')\n",
    "print('Training Nederlands ...')\n",
    "nldModel = LanguageModel(dataset['nld']['trn'], 'nld')\n",
    "print('Training Español ...')\n",
    "spaModel = LanguageModel(dataset['spa']['trn'], 'spa')\n",
    "lt1 = time.time()\n",
    "print('Training time:', lt1 - lt0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metamodel\n",
    "\n",
    "The general idea of our classifier is the following: once we have trained a language model for each language in the corpus, we can define a meta-model that evaluates each of the language models for the given sentence, and then returns the one that has the higher probability (ie. the one that would have the lowest perplexity). Recall that we computed the log probabilities.\n",
    "\n",
    "Once the language models are built, it would be easy to define alternative meta-models, as in ensemble learning. For instance, we could train a classifier that would output the identified language given the perplexities of each language model (that could be useful if there were many \"draws\", and the meta-model could learn that in case of draw between, say, English and Spanish, Spanish should have priority), instead of just taking the onbe with the lowest perplexity. However, we believe that this is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metamodel(models, sentence):\n",
    "    max_p = inf\n",
    "    res = None\n",
    "    for model in models:\n",
    "        p = model.eval(sentence)\n",
    "        if abs(p) < max_p:\n",
    "            # (log probablity)\n",
    "            max_p = abs(p)\n",
    "            res = model.lang\n",
    "    return res\n",
    "\n",
    "models = [engModel, deuModel, fraModel, itaModel, nldModel, spaModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "We run the metamodel for all the sentences in the train set and build the confusion matrix. Inference is relatively fast, but it's slower than the train set because we are dynamically computing the probabilities instead of storing a large table, which would not be very memory-efficient. Recall that the test sentences, which have not been used for training the model (in order to have a fair evaluation), are also labeled, so we can evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ... eng\n",
      "Evaluating on ... deu\n",
      "Evaluating on ... fra\n",
      "Evaluating on ... ita\n",
      "Evaluating on ... nld\n",
      "Evaluating on ... spa\n",
      "Inference time: 192.2320203781128\n"
     ]
    }
   ],
   "source": [
    "tt0 = time.time()\n",
    "confusion_matrix = {}\n",
    "for l in langs:\n",
    "    print('Evaluating on ...', l)\n",
    "    confusion_matrix[l] = {}\n",
    "    for sentence in dataset[l]['tst']:\n",
    "        res = metamodel(models, sentence)\n",
    "        if res in confusion_matrix[l]:\n",
    "            confusion_matrix[l][res] += 1\n",
    "        else:\n",
    "            confusion_matrix[l][res] = 1\n",
    "tt1 = time.time()\n",
    "\n",
    "print('Inference time:', tt1-tt0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can extract the accuracy and print a prettified version of our confusion matrix, which tells us what the model should have output (ie. the ground truth) vs what it actually output. Ideally, we would like a diagonal matrix, which would imply an accuracy of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***|*eng|*deu|*fra|*ita|*nld|*spa|\n",
      "eng|9985|   .|   1|   .|   1|   .|\n",
      "deu|  11|9971|   .|   1|   6|   1|\n",
      "fra|  11|   .|9980|   5|   3|   1|\n",
      "ita|   8|   .|   1|9987|   .|   4|\n",
      "nld|  28|   7|   2|   4|9957|   2|\n",
      "spa|   4|   .|   1|   3|   .|9992|\n",
      "----------------------------------\n",
      "Accuracy 0.9982493289094153\n",
      "Training time: 70.53643941879272\n",
      "Inference time: 209.0808641910553\n"
     ]
    }
   ],
   "source": [
    "def prettify(conf_mat):\n",
    "    print('***', end = '')\n",
    "    for l in langs:\n",
    "        print('|*' + l, end = '')\n",
    "    print('|')\n",
    "    for l in langs:\n",
    "        print(l, end = '|')\n",
    "        for l2 in langs:\n",
    "            if l2 in conf_mat[l]:\n",
    "                print(str(conf_mat[l][l2]).rjust(4), end = '|')\n",
    "            else:\n",
    "                print(str('   .'), end = '|')\n",
    "        print()\n",
    "    print('-'*34)\n",
    "\n",
    "def get_accuracy(conf_mat):\n",
    "    total = 0\n",
    "    right = 0\n",
    "    for l in langs:\n",
    "        right += conf_mat[l][l]\n",
    "        total += sum(conf_mat[l].values())\n",
    "    return right/total\n",
    "    \n",
    "prettify(confusion_matrix)\n",
    "print('Accuracy', get_accuracy(confusion_matrix))\n",
    "print('Training time:', lt1 - lt0)\n",
    "print('Inference time:', tt1-tt0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The confusion matrix is almost diagonal and the accuracy is almost perfect. Accuracy is the right metric in this case because the dataset is balanced.\n",
    "\n",
    "Inspecting the test and errors, we can see that there are some special cases, like English sentences with Japanese characters, for instance, which can fool the classifier, but in general it is pretty solid for all languages.\n",
    "\n",
    "We can see that the most errors are committed when a non-English sentence is tagged as English. We believe that this happens because English words are more usually used in other languages than the other way around.\n",
    "\n",
    "With regard the execution times, the model is very fast. Perhaps it could be faster on inference, but there is a trade-off between training and inference time (eg. we could have stored a big table with pre-computed probabilities).\n",
    "\n",
    "As future work, we suggest using dynamic programming for avoiding to recompute certain probabilities (we didn't implement it because we don't know if the inference time would have been fair in that case) and building an ensemble classifier. Also, it should be investigated whether the special sentences we mentioned could be detected and treated in some way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
