{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Final Project: Semantinc Textual Similarity\n",
    "Jordi Armengol - Joan LLop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "We start by downloading the SemEval 2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-08 17:06:23--  https://gebakx.github.io/ihlt/sts/resources/train.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.109.153, 185.199.111.153, 185.199.110.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125822 (123K) [application/octet-stream]\n",
      "Saving to: ‘data/train.tgz.9’\n",
      "\n",
      "train.tgz.9         100%[===================>] 122,87K  --.-KB/s    in 0,06s   \n",
      "\n",
      "2019-12-08 17:06:24 (1,92 MB/s) - ‘data/train.tgz.9’ saved [125822/125822]\n",
      "\n",
      "--2019-12-08 17:06:24--  https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.111.153, 185.199.110.153, 185.199.108.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118094 (115K) [application/octet-stream]\n",
      "Saving to: ‘data/test-gold.tgz.9’\n",
      "\n",
      "test-gold.tgz.9     100%[===================>] 115,33K  --.-KB/s    in 0,06s   \n",
      "\n",
      "2019-12-08 17:06:24 (1,82 MB/s) - ‘data/test-gold.tgz.9’ saved [118094/118094]\n",
      "\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project/data\n",
      "train/\n",
      "train/00-readme.txt\n",
      "train/STS.output.MSRpar.txt\n",
      "train/STS.input.SMTeuroparl.txt\n",
      "train/STS.input.MSRpar.txt\n",
      "train/STS.gs.MSRpar.txt\n",
      "train/STS.input.MSRvid.txt\n",
      "train/STS.gs.MSRvid.txt\n",
      "train/correlation.pl\n",
      "train/STS.gs.SMTeuroparl.txt\n",
      "test-gold/\n",
      "test-gold/STS.input.MSRpar.txt\n",
      "test-gold/STS.gs.MSRpar.txt\n",
      "test-gold/STS.input.MSRvid.txt\n",
      "test-gold/STS.gs.MSRvid.txt\n",
      "test-gold/STS.input.SMTeuroparl.txt\n",
      "test-gold/STS.gs.SMTeuroparl.txt\n",
      "test-gold/STS.input.surprise.SMTnews.txt\n",
      "test-gold/STS.gs.surprise.SMTnews.txt\n",
      "test-gold/STS.input.surprise.OnWN.txt\n",
      "test-gold/STS.gs.surprise.OnWN.txt\n",
      "test-gold/STS.gs.ALL.txt\n",
      "test-gold/00-readme.txt\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus assembly\n",
    "Train and test sets. The test set will not be used for learning or model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General class/interface\n",
    "There are so many things to try that we will start by defining a general class/interface for all the models that we will use. This class has, among other features, a cross-validation method (obviously, using only the train set, not the test set). The models will inherit the methods of this class and, essentially, they will only have to implement the a method from extracting the features from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,  x, y, regr=LinearRegression(),):\n",
    "        self.regr = regr\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.name = None\n",
    "        self.description = None\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        pickle.dump(self, open(self.name + '.model', 'wb').write())\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, name):\n",
    "        return pickle.load(open(self.name + '.model', 'rb').read())\n",
    "        \n",
    "    \n",
    "    def _extract_features(self, x):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.regr.fit(self.x_features, self.y)\n",
    "    \n",
    "    \n",
    "    def predict(self, new_x):\n",
    "        new_x_features = self._extract_features(new_x)\n",
    "        return self.regr.predict(new_x_features)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, true_labels, predicted_labels):\n",
    "        pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "        return pearson, p_value\n",
    "    \n",
    "    \n",
    "    def cross_validate(self, n_folds=5, seed=1):\n",
    "        assert self.x_features is not None\n",
    "        kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "        average_pearson = 0\n",
    "        for train_index, val_index in kf.split(self.x_features):\n",
    "            X_train, X_val = self.x_features[train_index], self.x_features[val_index]\n",
    "            y_train, y_val = self.y[train_index], self.y[val_index]\n",
    "            self.regr.fit(X_train, y_train)\n",
    "            predicted_labels = self.regr.predict(X_val)\n",
    "            pearson, _ = self.evaluate(y_val, predicted_labels)\n",
    "            average_pearson += abs(pearson)\n",
    "        return average_pearson/n_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 1: Linguistic feature engineering and classical machine learning\n",
    "Firstly, we will try the classical NLP strategies that we have seen in class. Since there are so many possibilities to try, we will start by fixing the regression algorithm to linear regression, and just change the particular features we input to the model (preprocessing, text representation strategies, features and ways of aggregating such features such as distances). Once we have a general idea of the features that seem to be useful, we will aggregate them into a single model. Finally, we will try different regresion algorithms, specifically non-linear ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance of some basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class JaccardModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'JaccardModel'\n",
    "        self.description = 'Jaccard distance, some basic features'\n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "        super().__init__(*kwargs)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(sent):\n",
    "            preprocessed = \"\"\n",
    "            for char in sent:\n",
    "                if char.isdigit():\n",
    "                    preprocessed += char\n",
    "                elif char.isalpha():\n",
    "                    preprocessed += char.lower()\n",
    "                elif char == ' ':\n",
    "                    preprocessed += char\n",
    "\n",
    "            return str(preprocessed)\n",
    "\n",
    "        x = [[preprocess(sent1), preprocess(sent2)] for sent1, sent2 in x]\n",
    "        \n",
    "        def lemmatize(token, pos):\n",
    "            if pos in {'N','V'}:\n",
    "                return wnl.lemmatize(token.lower(), pos.lower())\n",
    "            return token.lower()\n",
    "\n",
    "\n",
    "        def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "            mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "            if nltk_pos in mapping:\n",
    "                return mapping[nltk_pos]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def get_synsets(sent):\n",
    "            saved_synsets = []\n",
    "            tokens = word_tokenize(sent)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            for token, pos, lemma in zip(tokens, pos_tags, lemmas):\n",
    "                wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "                if wordnet_pos is not None:\n",
    "                    word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "                    if len(word_synsets) > 0:\n",
    "                        most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                        saved_synsets.append(most_freq_synset)\n",
    "            return saved_synsets\n",
    "\n",
    "\n",
    "        def get_features_from_word(sent, index, pos):\n",
    "            word = sent[index]\n",
    "            features = []\n",
    "            features.append(str(pos)) # Part-of-Speech                   \n",
    "            features.append(str(len(word))) # length of word\n",
    "            features.append(str(index==0)) # beggining of a sentence\n",
    "            features.append(str(index==len(sent)-1)) # end of sentence\n",
    "            features.append(str(word.isdigit())) # is a digit\n",
    "            return features\n",
    "\n",
    "        def sent2features(sent):\n",
    "            features = []\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.stop_words]\n",
    "            features.append(tokens)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            features.append(pos_tags)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            features.append(lemmas)\n",
    "            synsets = get_synsets(sent)\n",
    "            if len(synsets) > 0:\n",
    "                features.append(synsets)\n",
    "            else:\n",
    "                features.append([0])\n",
    "            temp_f = []\n",
    "            for i in range(len(tokens)):\n",
    "                temp_f += get_features_from_word(tokens, i, pos_tags[i])\n",
    "            features.append(temp_f)\n",
    "\n",
    "            return features\n",
    "        \n",
    "        def distance(features1, features2, sent1, sent2, index):\n",
    "            distances = []\n",
    "            init = True\n",
    "            for f1, f2 in zip(features1, features2):\n",
    "                distances.append(jaccard_distance(set(f1), set(f2)))\n",
    "            return distances\n",
    "\n",
    "        \n",
    "        pairs_of_features = [(sent2features(sent1), sent2features(sent2)) for sent1, sent2 in x]\n",
    "        distances = np.array([distance(features1, features2, sent1, sent2, index) for index, ((features1, features2), (sent1, sent2)) in enumerate(zip(pairs_of_features, x))])\n",
    "        return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744670790260964"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccardModel = JaccardModel(train_data, train_labels)\n",
    "jaccardModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "Cosine similarity of the union of the sets of bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5968105783849429"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "from string import punctuation\n",
    "\n",
    "class BagWordsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagWords'\n",
    "        self.description = 'We get the bag of words of both setences, \\\n",
    "        calculate the union, get the count of each word of the union for each sentence \\\n",
    "        and get the distance as the sum of the element-wise distance'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char\n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_bag_of_words(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            union = np.union1d(tokens1, tokens2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for token in tokens1:\n",
    "                count1[np.where(union == token)] += 1\n",
    "            for token in tokens2:\n",
    "                count2[np.where(union == token)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_words(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "bagWordsModel = BagWordsModel(train_data, train_labels)\n",
    "bagWordsModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of lemmas\n",
    "Same as before, but with lemmas. Recall that we are still using a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076451713539923"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "\n",
    "class BagLemmasModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagLemmasModel'\n",
    "        self.description = 'We get the bag of lemmas of both setences, calculate the union,\\\n",
    "        get the count of each word of the union for each sentence and get the cosine distance \\\n",
    "        between them'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char \n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def lemmatize(token):\n",
    "            return self.wnl.lemmatize(token)\n",
    "        \n",
    "        \n",
    "        def get_bag_of_lemmas(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            lemmas1 = [lemmatize(word) for word in tokens1]\n",
    "            lemmas2 = [lemmatize(word) for word in tokens2]\n",
    "            return get_cosine_of_frequencies(lemmas1, lemmas2)\n",
    "                \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_lemmas(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "bagLemmasModel = BagLemmasModel(train_data, train_labels)\n",
    "bagLemmasModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6488292812545408"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "\n",
    "class BagStemsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagWords'\n",
    "        self.description = 'We get the bag of stems of both setences, calculate the union, \\\n",
    "        get the count of each word of the union for each sentence and get the cosine distance between them'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char\n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def stemmatize(token):\n",
    "            return self.stemmer.stem(token)\n",
    "        \n",
    "        \n",
    "        def get_bag_of_stems(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            stems1 = [stemmatize(word) for word in tokens1]\n",
    "            stems2 = [stemmatize(word) for word in tokens2]\n",
    "            return get_cosine_of_frequencies(stems1, stems2)                \n",
    "            \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_stems(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "bagStemsModel = BagStemsModel(train_data, train_labels)\n",
    "bagStemsModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams vector representation\n",
    "We compute the cosine similarity of a frequency bigram representation of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.663661970721763"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "class BiGramsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BiGramsModel'\n",
    "        self.description = 'BiGramsModel'\n",
    "        self.allowed = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                                 '(', ')', '.', ' ', '!', '?', 'a', 'b', 'c', 'd', 'e', \\\n",
    "                                 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "                                 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
    "        self.not_allowed_words = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def get_allowed_characters(sent):\n",
    "            sent = sent.lower()\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.not_allowed_words]\n",
    "            stems = [self.stemmer.stem(word) for word in tokens]\n",
    "            new_sent = \" \".join(stems)\n",
    "            return \"\".join([char for char in new_sent if char in self.allowed])\n",
    "        \n",
    "        \n",
    "        def preprocess(data):\n",
    "            word_freq = {}\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                allowed_chars1 = get_allowed_characters(sent1)\n",
    "                allowed_chars2 = get_allowed_characters(sent2)\n",
    "                processed_data.append([allowed_chars1, allowed_chars2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_vector_bigrams(sent):\n",
    "            count = np.zeros(len(self.allowed)**2) # number of possible bigrams\n",
    "            for i in range(len(sent)-1):\n",
    "                idx_f = int(np.where(self.allowed == sent[i])[0])\n",
    "                idx_s = int(np.where(self.allowed == sent[i+1])[0])\n",
    "                count[idx_f*len(self.allowed) + idx_s] += 1\n",
    "            return count\n",
    "            \n",
    "                \n",
    "        def get_cos_of_bigrams(sent1, sent2):\n",
    "            return [cosine_similarity(get_vector_bigrams(sent1), get_vector_bigrams(sent2))]                \n",
    "            \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BiGms = np.array([get_cos_of_bigrams(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BiGms\n",
    "\n",
    "biGramsModel = BiGramsModel(train_data, train_labels)\n",
    "biGramsModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets\n",
    "We apply the same algorithm as in the case of words and lemmas, but with synsets and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46043551305575275"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class SynsetsLemmasModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'SynsetsLemmasModel'\n",
    "        self.description = 'SynsetsLemmasModel'\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def get_union_SynLemmas(word):\n",
    "            synsets = wordnet.synsets(word)\n",
    "            if len(synsets) > 0:\n",
    "                return reduce(np.union1d, ([str(lemma.name()) for lemma in synsets[0].lemmas()]))\n",
    "            return np.empty(0)\n",
    "        \n",
    "        \n",
    "        def get_sentence_union_synsets(tokens):\n",
    "            return reduce(np.union1d, ([get_union_SynLemmas(word) for word in tokens]))\n",
    "        \n",
    "        \n",
    "        def get_union_synsets(sent1, sent2):\n",
    "            synsets1 = get_sentence_union_synsets(word_tokenize(sent1))\n",
    "            synsets2 = get_sentence_union_synsets(word_tokenize(sent2))\n",
    "            return get_cosine_of_frequencies(synsets1, synsets2)\n",
    "            \n",
    "        \n",
    "        BoW = np.array([get_union_synsets(sent1, sent2) for sent1, sent2 in x])\n",
    "        return BoW\n",
    "\n",
    "synsetsLemmasModel = SynsetsLemmasModel(train_data, train_labels)\n",
    "synsetsLemmasModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams specific counts\n",
    "Like in Bag of stems. \n",
    "This way we can try trigrams without that much sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6049352032417628"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "class BiGramsSpecificModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BiGramsSpecificModel'\n",
    "        self.description = 'BiGramsSpecificModel'\n",
    "        self.allowed = np.array(['0', '1', '2', '3', '4', '5', '6', '7',\n",
    "                                 '8', '9', '(', ')', '.', ' ', '!', '?',\n",
    "                                 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
    "                                 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
    "                                 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
    "        self.not_allowed_words = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def get_allowed_characters(sent):\n",
    "            sent = sent.lower()\n",
    "            tokens = np.array([word for word in word_tokenize(sent) if not word in self.not_allowed_words])\n",
    "            stems = [self.stemmer.stem(word) for word in tokens]\n",
    "            new_sent = \" \".join(stems)\n",
    "            final_sent = \"\".join([char for char in new_sent if char in self.allowed])\n",
    "            return final_sent\n",
    "        \n",
    "        \n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                allowed_chars1 = get_allowed_characters(sent1)\n",
    "                allowed_chars2 = get_allowed_characters(sent2)\n",
    "                processed_data.append([allowed_chars1, allowed_chars2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_vector_bigrams(sent):\n",
    "            bigrams = []\n",
    "            for i in range(len(sent)):\n",
    "                bigrams.append(sent[i])\n",
    "            return np.array(bigrams)\n",
    "        \n",
    "        \n",
    "        def get_cosine_distance_of_count_of_bigrams(sent1, sent2):\n",
    "            bigrams1 = get_vector_bigrams(sent1)\n",
    "            bigrams2 = get_vector_bigrams(sent2)\n",
    "            union = np.union1d(bigrams1, bigrams2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in bigrams1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in bigrams2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "            \n",
    "                \n",
    "        def get_cosinus_of_bigrams(sent1, sent2):\n",
    "            return get_cosine_distance_of_count_of_bigrams(sent1, sent2)              \n",
    "            \n",
    "            \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        nBiGms = np.array([get_cosinus_of_bigrams(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return nBiGms\n",
    "\n",
    "biGramsSpecificModel = BiGramsSpecificModel(train_data, train_labels)\n",
    "biGramsSpecificModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun phrases and verbs comparison\n",
    "Spacy is a NLP toolkit similar to Stanford CoreNLP, but without requiring a Java server. We will use it to retrieve noun phrases and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/jordiae/.local/lib/python3.5/site-packages (2.0.18)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (0.2.9)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (1.16.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: regex==2018.01.10 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jordiae/.local/lib/python3.5/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /home/jordiae/.local/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /home/jordiae/.local/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /home/jordiae/.local/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.12.0)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /home/jordiae/.local/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/jordiae/.local/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.29.1)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /home/jordiae/.local/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jordiae/.local/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/jordiae/.local/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jordiae/.local/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/jordiae/.local/lib/python3.5/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
      "Collecting en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K     |████████████████████████████████| 37.4MB 19.2MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.0.0-cp35-none-any.whl size=37406825 sha256=7e310d00ddd6164c7c0602d415746d2033bc9c6903d7ad7089bf826c23ae7513\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_q8i_uv4/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/jordiae/.local/lib/python3.5/site-packages/en_core_web_sm -->\n",
      "    /home/jordiae/.local/lib/python3.5/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install spacy --user\n",
    "!python3 -m spacy download en_core_web_sm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4110560196683541"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "\n",
    "\n",
    "class NounPhraseModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'NounPhraseModel'\n",
    "        self.description = 'NounPhraseModel'\n",
    "        # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.not_allowed_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        \n",
    "        \n",
    "        def get_distance(v1, v2):\n",
    "            if not v1 or not v2: return 0\n",
    "            count_pairs = 0\n",
    "            for sent1 in v1:\n",
    "                for sent2 in v2:\n",
    "                    if sent1 in sent2 or sent2 in sent1:\n",
    "                        count_pairs += 1\n",
    "            return count_pairs/max(len(v1), len(v2))\n",
    "        \n",
    "        \n",
    "        def get_noun_phrases(sent):\n",
    "            doc = self.nlp(sent)\n",
    "            noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "            return noun_phrases\n",
    "        \n",
    "        \n",
    "        def get_verbs(sent):\n",
    "            doc = self.nlp(sent)\n",
    "            verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "            return verbs\n",
    "        \n",
    "        \n",
    "        def get_name_entities(sent):\n",
    "            doc = self.nlp(sent)\n",
    "            ne = [entity.text for entity in doc.ents]\n",
    "            return ne\n",
    "        \n",
    "        def get_similarity(sent1, sent2):\n",
    "            noun_phrases_1 = get_noun_phrases(str(sent1))\n",
    "            noun_phrases_2 = get_noun_phrases(str(sent2))\n",
    "            verbs1 = get_verbs(str(sent1))\n",
    "            verbs2 = get_verbs(str(sent2))\n",
    "            ne1 = get_name_entities(str(sent1))\n",
    "            ne2 = get_name_entities(str(sent2))\n",
    "            dist_nouns = get_distance(noun_phrases_1, noun_phrases_2)\n",
    "            dist_verbs = get_distance(verbs1, verbs2)\n",
    "            dist_ne = get_distance(ne1, ne2)\n",
    "            return [dist_verbs, dist_nouns, dist_ne]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sm = np.array([get_similarity(sent1, sent2) for sent1, sent2 in x])\n",
    "        return sm\n",
    "\n",
    "nounPhraseModel = NounPhraseModel(train_data, train_labels)\n",
    "nounPhraseModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a summary of what have been experimented with. So far, we have tested many features, still with linear regression. There are many tests that we do not include here, for brevity (eg. the same models but without removing stop words, which gave a worse result). Since linear regression is a very simple algorithm, many times we employed distances and cosine similarities as features, instead of the bags of words (for instance) themselves.\n",
    "\n",
    "Now, are going to aggregate some features into a single model. The assumption here is that their contribution will be additive or at least will not damage the result. As preliminary research, we conducted some experiments on some of the combinations, but for brevity and computational constraints is not possible to test all the combinations. Instead, we will include the features that seemed the most promising. In the case of the NounPhraseModel, for instance, we will not include it, because it is way slower than the other ones and the obtained results were  mediocre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating features gives better results, but we are still using a linear regression. Now, we are going to try non-linear models. Since they are more powerful, it may not be a good idea to reduce the dimensionality of the feature set by applying distances or cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatedFeaturesModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'AggregatedFeaturesModel'\n",
    "        self.description = 'AggregatedFeaturesModel'\n",
    "        self.allowed = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                                 '(', ')', '.', ' ', '!', '?', 'a', 'b', 'c', 'd', 'e', \\\n",
    "                                 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "                                 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
    "        self.not_allowed_words = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def preprocess(data):\n",
    "            word_freq = {}\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                allowed_chars1 = get_allowed_characters(sent1)\n",
    "                allowed_chars2 = get_allowed_characters(sent2)\n",
    "                processed_data.append([allowed_chars1, allowed_chars2])\n",
    "            return processed_data\n",
    "\n",
    "        def get_allowed_characters(sent):\n",
    "            sent = sent.lower()\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.not_allowed_words]\n",
    "            stems = [self.stemmer.stem(word) for word in tokens]\n",
    "            new_sent = \" \".join(stems)\n",
    "            return \"\".join([char for char in new_sent if char in self.allowed])\n",
    "\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "\n",
    "\n",
    "        def get_vector_bigrams(sent):\n",
    "            count = np.zeros(len(self.allowed)**2) # number of possible bigrams\n",
    "            for i in range(len(sent)-1):\n",
    "                idx_f = int(np.where(self.allowed == sent[i])[0])\n",
    "                idx_s = int(np.where(self.allowed == sent[i+1])[0])\n",
    "                count[idx_f*len(self.allowed) + idx_s] += 1\n",
    "            return count\n",
    "\n",
    "\n",
    "        def get_cos_of_bigrams(sent1, sent2):\n",
    "            return [cosine_similarity(get_vector_bigrams(sent1), get_vector_bigrams(sent2))]   \n",
    "\n",
    "        def stemmatize(token):\n",
    "            return self.stemmer.stem(token)\n",
    "\n",
    "\n",
    "        def get_bag_of_stems(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            stems1 = [stemmatize(word) for word in tokens1]\n",
    "            stems2 = [stemmatize(word) for word in tokens2]\n",
    "            return get_cosine_of_frequencies(stems1, stems2)\n",
    "        \n",
    "        def lemmatize(token, pos):\n",
    "            if pos in {'N','V'}:\n",
    "                return wnl.lemmatize(token.lower(), pos.lower())\n",
    "            return token.lower()\n",
    "\n",
    "\n",
    "        def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "            mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "            if nltk_pos in mapping:\n",
    "                return mapping[nltk_pos]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def get_synsets(sent):\n",
    "            saved_synsets = []\n",
    "            tokens = word_tokenize(sent)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            for token, pos, lemma in zip(tokens, pos_tags, lemmas):\n",
    "                wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "                if wordnet_pos is not None:\n",
    "                    word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "                    if len(word_synsets) > 0:\n",
    "                        most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                        saved_synsets.append(most_freq_synset)\n",
    "            return saved_synsets\n",
    "\n",
    "\n",
    "        def get_features_from_word(sent, index, pos):\n",
    "            word = sent[index]\n",
    "            features = []\n",
    "            features.append(str(pos)) # Part-of-Speech                   \n",
    "            features.append(str(len(word))) # length of word\n",
    "            features.append(str(index==0)) # beggining of a sentence\n",
    "            features.append(str(index==len(sent)-1)) # end of sentence\n",
    "            features.append(str(word.isdigit())) # is a digit\n",
    "            return features\n",
    "\n",
    "        def sent2features(sent):\n",
    "            features = []\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.stop_words]\n",
    "            features.append(tokens)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            features.append(pos_tags)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            features.append(lemmas)\n",
    "            synsets = get_synsets(sent)\n",
    "            if len(synsets) > 0:\n",
    "                features.append(synsets)\n",
    "            else:\n",
    "                features.append([0])\n",
    "            temp_f = []\n",
    "            for i in range(len(tokens)):\n",
    "                temp_f += get_features_from_word(tokens, i, pos_tags[i])\n",
    "            features.append(temp_f)\n",
    "\n",
    "            return features\n",
    "        \n",
    "        def distance(features1, features2, sent1, sent2, index):\n",
    "            distances = []\n",
    "            init = True\n",
    "            for f1, f2 in zip(features1, features2):\n",
    "                distances.append(jaccard_distance(set(f1), set(f2)))\n",
    "            return distances\n",
    "        def sent2features(sent):\n",
    "            features = []\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.stop_words]\n",
    "            features.append(tokens)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            features.append(pos_tags)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            features.append(lemmas)\n",
    "            synsets = get_synsets(sent)\n",
    "            if len(synsets) > 0:\n",
    "                features.append(synsets)\n",
    "            else:\n",
    "                features.append([0])\n",
    "            temp_f = []\n",
    "            for i in range(len(tokens)):\n",
    "                temp_f += get_features_from_word(tokens, i, pos_tags[i])\n",
    "            features.append(temp_f)\n",
    "\n",
    "            return features\n",
    "        \n",
    "        def distance(features1, features2, sent1, sent2, index):\n",
    "            distances = []\n",
    "            init = True\n",
    "            for f1, f2 in zip(features1, features2):\n",
    "                distances.append(jaccard_distance(set(f1), set(f2)))\n",
    "            return distances\n",
    "\n",
    "\n",
    "        preprocessed_x = preprocess(x)\n",
    "        pairs_of_features = [(sent2features(sent1), sent2features(sent2)) for sent1, sent2 in preprocessed_x]\n",
    "        distances = np.array([distance(features1, features2, sent1, sent2, index) for index, ((features1, features2), (sent1, sent2)) in enumerate(zip(pairs_of_features, x))])\n",
    "        \n",
    "        feat = np.array([get_cos_of_bigrams(sent1, sent2) + get_bag_of_stems(sent1, sent2)\n",
    "                           for sent1, sent2 in preprocessed_x])\n",
    "        \n",
    "        feat = np.concatenate((feat, distances), axis=1)\n",
    "        return feat\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6442796735068107"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AggregatedFeaturesLRModel(AggregatedFeaturesModel):    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'AggregatedFeaturesLRModel'\n",
    "        self.description = 'AggregatedFeaturesLRModel'\n",
    "        super().__init__(*kwargs)\n",
    "\n",
    "aggregatedFeaturesLRModel = AggregatedFeaturesLRModel(train_data, train_labels)\n",
    "aggregatedFeaturesLRModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6491118599806167"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class AggregatedFeaturesRFModel(AggregatedFeaturesModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'AggregatedFeaturesRFModel'\n",
    "        self.description = 'AggregatedFeaturesRFModel'\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = RandomForestRegressor(n_estimators=10, max_depth=3, random_state=1)\n",
    "        \n",
    "aggregatedFeaturesRFModel = AggregatedFeaturesRFModel(train_data, train_labels)\n",
    "aggregatedFeaturesRFModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6438921599375783"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "class AggregatedFeaturesAdaBoostModel(AggregatedFeaturesModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'AggregatedFeaturesAdaBoostModel'\n",
    "        self.description = 'AggregatedFeaturesAdaBoostModel'\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = AdaBoostRegressor(random_state=1)\n",
    "    \n",
    "aggregatedFeaturesAdaBoostModel = AggregatedFeaturesAdaBoostModel(train_data, train_labels)\n",
    "aggregatedFeaturesAdaBoostModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6400979480439546"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "class AggregatedFeaturesMLPModel(AggregatedFeaturesModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'AggregatedFeaturesAdaBoostModel'\n",
    "        self.description = 'AggregatedFeaturesAdaBoostModel'\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr =  MLPRegressor(\n",
    "            early_stopping=True, random_state=1, max_iter=1000, hidden_layer_sizes=(10))\n",
    "        \n",
    "aggregatedFeaturesMLPModel = AggregatedFeaturesMLPModel(train_data, train_labels)\n",
    "aggregatedFeaturesMLPModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.642291372735609"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "class AggregatedFeaturesSVRModel(AggregatedFeaturesModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'AggregatedFeaturesSVRModel'\n",
    "        self.description = 'AggregatedFeaturesSVRModel'\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = SVR(gamma='scale')\n",
    "        \n",
    "aggregatedFeaturesSVRModel = AggregatedFeaturesSVRModel(train_data, train_labels)\n",
    "aggregatedFeaturesSVRModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-06 15:12:23--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘data/wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650,22M  10,4MB/s    in 64s     \n",
      "\n",
      "2019-12-06 15:13:28 (10,1 MB/s) - ‘data/wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n",
      "/home/jordiae/MAI/IHLT-MAI-clone/lab/project/data\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n",
      "/home/jordiae/MAI/IHLT-MAI-clone/lab/project\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip --directory-prefix=data\n",
    "%cd data\n",
    "!unzip wiki-news-300d-1M.vec.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "train_tokens = [word_tokenize(sent1) + word_tokenize(sent2) for sent1, sent2 in train_data]\n",
    "vocabulary = set([])\n",
    "for tokenized in train_tokens:\n",
    "    for token in tokenized:\n",
    "        vocabulary.add(token)\n",
    "pretrained_embeddings_path = os.path.join('data', 'wiki-news-300d-1M.vec')\n",
    "needed_tokens = set()\n",
    "embedding_table = {}\n",
    "dim = 0\n",
    "for line in open(pretrained_embeddings_path, 'r').readlines():\n",
    "    if dim == 0:\n",
    "        dim = int(line.split()[1])\n",
    "        continue\n",
    "    row = line.split()\n",
    "    token = row[0]\n",
    "    if token not in vocabulary:\n",
    "        continue\n",
    "    vector = np.array(list(map(float, row[1:])))\n",
    "    embedding_table[token] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class WordEmbeddingsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embedding_table, dim, method, *kwargs):\n",
    "        assert method in ['avg', 'sum', 'max']\n",
    "        self.embedding_table = embedding_table\n",
    "        self.dim = dim\n",
    "        self.method = method\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "    def _get_sentence_embedding(self, sent):\n",
    "        tokenized = word_tokenize(sent)\n",
    "        def contains_punct(token):\n",
    "            for c in string.punctuation:\n",
    "                if c in token:\n",
    "                    return True\n",
    "            return False\n",
    "        tokenized = [token for token in tokenized if not contains_punct(token)] # empitjora\n",
    "        tokenized = [token.lower() for token in tokenized if token.lower() not in stop_words] # empitjora\n",
    "        embeddings = np.zeros((len(tokenized), self.dim))\n",
    "        for idx, token in enumerate(tokenized):\n",
    "            if token in embedding_table:\n",
    "                embeddings[idx] = self.embedding_table[token]\n",
    "            else:\n",
    "                embeddings[idx] = np.zeros(dim)       \n",
    "        if self.method == 'avg':\n",
    "            aggregated_embeddings = np.mean(embeddings, axis=0)\n",
    "        elif self.method == 'sum':\n",
    "            aggregated_embeddings = np.sum(embeddings, axis=0)\n",
    "        elif self.method == 'max':\n",
    "            aggregated_embeddings = np.max(embeddings, axis=0)\n",
    "        return aggregated_embeddings\n",
    "    \n",
    "    def _get_embeddings_and_cosine_similarity(self, sent1, sent2):\n",
    "        \n",
    "        emb1 = self._get_sentence_embedding(sent1)\n",
    "        emb2 = self._get_sentence_embedding(sent2)\n",
    "        cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "        emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "        return cos_sim, emb1_emb2\n",
    "\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6213398555295904"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NegatedModel():\n",
    "            def fit(self, X, y):\n",
    "                return self\n",
    "            def predict(self, X):\n",
    "                pred = []\n",
    "                for row in X:\n",
    "                    pred.append(-row[0])\n",
    "                return pred\n",
    "\n",
    "class WordEmbeddingsCosineSimilarityModel(WordEmbeddingsModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = NegatedModel()\n",
    "        self.name = 'WordEmbeddingsCosineSimilarityModel'\n",
    "        self.description = 'Pre-trained word Embeddings + stop words and punctuation filtering + cosine sim'\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return np.array([[self._get_embeddings_and_cosine_similarity(sent1, sent2)[0]] for sent1, sent2 in x])\n",
    "\n",
    "wordEmbeddingsCosineSimilarityModel = WordEmbeddingsCosineSimilarityModel(\n",
    "    embedding_table, dim, 'avg', train_data, train_labels)\n",
    "wordEmbeddingsCosineSimilarityModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4173597246401437"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "class WordEmbeddingsMLPModel(WordEmbeddingsModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr =  MLPRegressor(\n",
    "            early_stopping=True, random_state=1, max_iter=1000, hidden_layer_sizes=(300, 300))\n",
    "        self.name = 'WordEmbeddingsMLPModel'\n",
    "        self.description = 'Pre-trained word Embeddings + stop words and punctuation filtering + 2 layer MLP'\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return np.array([self._get_embeddings_and_cosine_similarity(sent1, sent2)[1] for sent1, sent2 in x])\n",
    "wordEmbeddingsMLPModel = WordEmbeddingsMLPModel(\n",
    "    embedding_table, dim, 'avg', train_data, train_labels)\n",
    "wordEmbeddingsMLPModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jordiae/.local/lib/python3.5/site-packages (2.2.1)\n",
      "Requirement already satisfied: tqdm in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (4.29.1)\n",
      "Requirement already satisfied: regex in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (2018.1.10)\n",
      "Requirement already satisfied: sentencepiece in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied: boto3 in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (1.10.34)\n",
      "Requirement already satisfied: requests in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (1.16.2)\n",
      "Requirement already satisfied: sacremoses in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.34 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers) (1.13.34)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests->transformers) (2.7)\n",
      "Requirement already satisfied: six in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: joblib in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.5/dist-packages (from botocore<1.14.0,>=1.13.34->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/jordiae/.local/lib/python3.5/site-packages (from botocore<1.14.0,>=1.13.34->boto3->transformers) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1206 18:17:41.475112 140300861650688 file_utils.py:319] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmppil2k8dg\n",
      "100%|██████████| 213450/213450 [00:00<00:00, 398927.28B/s]\n",
      "I1206 18:17:42.593273 140300861650688 file_utils.py:334] copying /tmp/tmppil2k8dg to cache at /home/jordiae/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1206 18:17:42.603806 140300861650688 file_utils.py:338] creating metadata file for /home/jordiae/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1206 18:17:42.610208 140300861650688 file_utils.py:347] removing temp file /tmp/tmppil2k8dg\n",
      "I1206 18:17:42.612698 140300861650688 tokenization_utils.py:379] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/jordiae/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1206 18:17:43.486097 140300861650688 file_utils.py:319] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpjkzx4nu_\n",
      "100%|██████████| 313/313 [00:00<00:00, 43635.48B/s]\n",
      "I1206 18:17:44.272827 140300861650688 file_utils.py:334] copying /tmp/tmpjkzx4nu_ to cache at /home/jordiae/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "I1206 18:17:44.279542 140300861650688 file_utils.py:338] creating metadata file for /home/jordiae/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "I1206 18:17:44.284778 140300861650688 file_utils.py:347] removing temp file /tmp/tmpjkzx4nu_\n",
      "I1206 18:17:44.289153 140300861650688 configuration_utils.py:157] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/jordiae/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "I1206 18:17:44.295077 140300861650688 configuration_utils.py:174] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I1206 18:17:44.876996 140300861650688 file_utils.py:319] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpn83w76n8\n",
      "100%|██████████| 435779157/435779157 [00:19<00:00, 22609598.21B/s]\n",
      "I1206 18:18:04.982883 140300861650688 file_utils.py:334] copying /tmp/tmpn83w76n8 to cache at /home/jordiae/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I1206 18:18:06.260002 140300861650688 file_utils.py:338] creating metadata file for /home/jordiae/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I1206 18:18:06.261048 140300861650688 file_utils.py:347] removing temp file /tmp/tmpn83w76n8\n",
      "I1206 18:18:06.318659 140300861650688 modeling_utils.py:393] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/jordiae/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5132,  0.5097,  0.1991,  ..., -0.3900,  0.4053, -0.2315],\n",
       "         [ 0.5395, -0.3658,  0.6667,  ..., -0.3920,  0.2505,  0.0202],\n",
       "         [ 0.7767,  0.6823,  0.7110,  ..., -0.0420, -0.3718,  0.3748],\n",
       "         ...,\n",
       "         [ 0.3555,  0.4486,  0.6175,  ..., -0.0388, -0.2631,  0.3514],\n",
       "         [ 0.7927, -0.1282,  0.2737,  ..., -0.5220,  0.4836,  0.0937],\n",
       "         [ 1.2903,  1.0356,  0.5054,  ..., -0.4344,  1.1973, -0.4236]]],\n",
       "       grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers, torch\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "#padding = [0] * ( 128 - len(input_ids))\n",
    "#input_ids += padding\n",
    "\n",
    "#attn_mask = input_ids.ne(0) # I added this to create a mask for padded indices\n",
    "outputs = model(input_ids)#, attention_mask=attn_mask)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 768)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.detach().numpy().shape\n",
    "#np.mean(last_hidden_states.detach().numpy()[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 2234\n",
      "10 of 2234\n",
      "20 of 2234\n",
      "30 of 2234\n",
      "40 of 2234\n",
      "50 of 2234\n",
      "60 of 2234\n",
      "70 of 2234\n",
      "80 of 2234\n",
      "90 of 2234\n",
      "100 of 2234\n",
      "110 of 2234\n",
      "120 of 2234\n",
      "130 of 2234\n",
      "140 of 2234\n",
      "150 of 2234\n",
      "160 of 2234\n",
      "170 of 2234\n",
      "180 of 2234\n",
      "190 of 2234\n",
      "200 of 2234\n",
      "210 of 2234\n",
      "220 of 2234\n",
      "230 of 2234\n",
      "240 of 2234\n",
      "250 of 2234\n",
      "260 of 2234\n",
      "270 of 2234\n",
      "280 of 2234\n",
      "290 of 2234\n",
      "300 of 2234\n",
      "310 of 2234\n",
      "320 of 2234\n",
      "330 of 2234\n",
      "340 of 2234\n",
      "350 of 2234\n",
      "360 of 2234\n",
      "370 of 2234\n",
      "380 of 2234\n",
      "390 of 2234\n",
      "400 of 2234\n",
      "410 of 2234\n",
      "420 of 2234\n",
      "430 of 2234\n",
      "440 of 2234\n",
      "450 of 2234\n",
      "460 of 2234\n",
      "470 of 2234\n",
      "480 of 2234\n",
      "490 of 2234\n",
      "500 of 2234\n",
      "510 of 2234\n",
      "520 of 2234\n",
      "530 of 2234\n",
      "540 of 2234\n",
      "550 of 2234\n",
      "560 of 2234\n",
      "570 of 2234\n",
      "580 of 2234\n",
      "590 of 2234\n",
      "600 of 2234\n",
      "610 of 2234\n",
      "620 of 2234\n",
      "630 of 2234\n",
      "640 of 2234\n",
      "650 of 2234\n",
      "660 of 2234\n",
      "670 of 2234\n",
      "680 of 2234\n",
      "690 of 2234\n",
      "700 of 2234\n",
      "710 of 2234\n",
      "720 of 2234\n",
      "730 of 2234\n",
      "740 of 2234\n",
      "750 of 2234\n",
      "760 of 2234\n",
      "770 of 2234\n",
      "780 of 2234\n",
      "790 of 2234\n",
      "800 of 2234\n",
      "810 of 2234\n",
      "820 of 2234\n",
      "830 of 2234\n",
      "840 of 2234\n",
      "850 of 2234\n",
      "860 of 2234\n",
      "870 of 2234\n",
      "880 of 2234\n",
      "890 of 2234\n",
      "900 of 2234\n",
      "910 of 2234\n",
      "920 of 2234\n",
      "930 of 2234\n",
      "940 of 2234\n",
      "950 of 2234\n",
      "960 of 2234\n",
      "970 of 2234\n",
      "980 of 2234\n",
      "990 of 2234\n",
      "1000 of 2234\n",
      "1010 of 2234\n",
      "1020 of 2234\n",
      "1030 of 2234\n",
      "1040 of 2234\n",
      "1050 of 2234\n",
      "1060 of 2234\n",
      "1070 of 2234\n",
      "1080 of 2234\n",
      "1090 of 2234\n",
      "1100 of 2234\n",
      "1110 of 2234\n",
      "1120 of 2234\n",
      "1130 of 2234\n",
      "1140 of 2234\n",
      "1150 of 2234\n",
      "1160 of 2234\n",
      "1170 of 2234\n",
      "1180 of 2234\n",
      "1190 of 2234\n",
      "1200 of 2234\n",
      "1210 of 2234\n",
      "1220 of 2234\n",
      "1230 of 2234\n",
      "1240 of 2234\n",
      "1250 of 2234\n",
      "1260 of 2234\n",
      "1270 of 2234\n",
      "1280 of 2234\n",
      "1290 of 2234\n",
      "1300 of 2234\n",
      "1310 of 2234\n",
      "1320 of 2234\n",
      "1330 of 2234\n",
      "1340 of 2234\n",
      "1350 of 2234\n",
      "1360 of 2234\n",
      "1370 of 2234\n",
      "1380 of 2234\n",
      "1390 of 2234\n",
      "1400 of 2234\n",
      "1410 of 2234\n",
      "1420 of 2234\n",
      "1430 of 2234\n",
      "1440 of 2234\n",
      "1450 of 2234\n",
      "1460 of 2234\n",
      "1470 of 2234\n",
      "1480 of 2234\n",
      "1490 of 2234\n",
      "1500 of 2234\n",
      "1510 of 2234\n",
      "1520 of 2234\n",
      "1530 of 2234\n",
      "1540 of 2234\n",
      "1550 of 2234\n",
      "1560 of 2234\n",
      "1570 of 2234\n",
      "1580 of 2234\n",
      "1590 of 2234\n",
      "1600 of 2234\n",
      "1610 of 2234\n",
      "1620 of 2234\n",
      "1630 of 2234\n",
      "1640 of 2234\n",
      "1650 of 2234\n",
      "1660 of 2234\n",
      "1670 of 2234\n",
      "1680 of 2234\n",
      "1690 of 2234\n",
      "1700 of 2234\n",
      "1710 of 2234\n",
      "1720 of 2234\n",
      "1730 of 2234\n",
      "1740 of 2234\n",
      "1750 of 2234\n",
      "1760 of 2234\n",
      "1770 of 2234\n",
      "1780 of 2234\n",
      "1790 of 2234\n",
      "1800 of 2234\n",
      "1810 of 2234\n",
      "1820 of 2234\n",
      "1830 of 2234\n",
      "1840 of 2234\n",
      "1850 of 2234\n",
      "1860 of 2234\n",
      "1870 of 2234\n",
      "1880 of 2234\n",
      "1890 of 2234\n",
      "1900 of 2234\n",
      "1910 of 2234\n",
      "1920 of 2234\n",
      "1930 of 2234\n",
      "1940 of 2234\n",
      "1950 of 2234\n",
      "1960 of 2234\n",
      "1970 of 2234\n",
      "1980 of 2234\n",
      "1990 of 2234\n",
      "2000 of 2234\n",
      "2010 of 2234\n",
      "2020 of 2234\n",
      "2030 of 2234\n",
      "2040 of 2234\n",
      "2050 of 2234\n",
      "2060 of 2234\n",
      "2070 of 2234\n",
      "2080 of 2234\n",
      "2090 of 2234\n",
      "2100 of 2234\n",
      "2110 of 2234\n",
      "2120 of 2234\n",
      "2130 of 2234\n",
      "2140 of 2234\n",
      "2150 of 2234\n",
      "2160 of 2234\n",
      "2170 of 2234\n",
      "2180 of 2234\n",
      "2190 of 2234\n",
      "2200 of 2234\n",
      "2210 of 2234\n",
      "2220 of 2234\n",
      "2230 of 2234\n",
      "0.5105574211248864\n",
      "0.0944570744222997\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def get_sentence_embedding(sent, model, dim, method='avg'):\n",
    "    assert method in ['avg', 'sum']\n",
    "    input_ids = torch.tensor(tokenizer.encode(sent)).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]\n",
    "    if method == 'avg':\n",
    "        aggregated_embeddings = np.mean(last_hidden_states.detach().numpy()[0], axis=0)\n",
    "    elif method == 'sum':\n",
    "        aggregated_embeddings = np.sum(last_hidden_states.detach().numpy()[0], axis=0)\n",
    "    return aggregated_embeddings\n",
    "\n",
    "def get_embeddings_and_cosine_similarity(sent1, sent2, model, dim):\n",
    "    emb1 = get_sentence_embedding(sent1, model, dim)\n",
    "    emb2 = get_sentence_embedding(sent2, model, dim)\n",
    "    cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "    emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "    return cos_sim, emb1_emb2\n",
    "\n",
    "cosine_similarities = np.zeros((len(train_data), 1))\n",
    "embeddings = np.zeros((len(train_data), 768*2))\n",
    "for idx, (sent1, sent2) in enumerate(train_data):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx, 'of', len(train_data))\n",
    "    cos_sim, emb1_emb2 = get_embeddings_and_cosine_similarity(sent1, sent2, model, 768)\n",
    "    cosine_similarities[idx] = np.array([cos_sim])\n",
    "    embeddings[idx] = np.array(emb1_emb2)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#cross_validate(cosine_similarities, train_labels, RandomForestRegressor(max_depth=4, random_state=0))\n",
    "\n",
    "class NegatedModel():\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for row in X:\n",
    "            pred.append(-row[0])\n",
    "        return pred\n",
    "print(cross_validate(cosine_similarities, train_labels, NegatedModel()))\n",
    "print(cross_validate(embeddings, train_labels, LinearRegression()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/6e/5c98f5f26698276bacd09077b039fa1a00797ed080a628ee844bd9f281d4/sentence-transformers-0.2.4.1.tar.gz (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 686kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: transformers==2.2.1 in /home/jordiae/.local/lib/python3.5/site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: tqdm in /home/jordiae/.local/lib/python3.5/site-packages (from sentence-transformers) (4.29.1)\n",
      "Requirement already satisfied: torch>=1.0.1 in /home/jordiae/.local/lib/python3.5/site-packages (from sentence-transformers) (1.0.1.post2)\n",
      "Requirement already satisfied: numpy in /home/jordiae/.local/lib/python3.5/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn in /home/jordiae/.local/lib/python3.5/site-packages (from sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: nltk in /home/jordiae/.local/lib/python3.5/site-packages (from sentence-transformers) (3.4)\n",
      "Requirement already satisfied: boto3 in /home/jordiae/.local/lib/python3.5/site-packages (from transformers==2.2.1->sentence-transformers) (1.10.34)\n",
      "Requirement already satisfied: sentencepiece in /home/jordiae/.local/lib/python3.5/site-packages (from transformers==2.2.1->sentence-transformers) (0.1.83)\n",
      "Requirement already satisfied: sacremoses in /home/jordiae/.local/lib/python3.5/site-packages (from transformers==2.2.1->sentence-transformers) (0.0.35)\n",
      "Requirement already satisfied: requests in /home/jordiae/.local/lib/python3.5/site-packages (from transformers==2.2.1->sentence-transformers) (2.21.0)\n",
      "Requirement already satisfied: regex in /home/jordiae/.local/lib/python3.5/site-packages (from transformers==2.2.1->sentence-transformers) (2018.1.10)\n",
      "Requirement already satisfied: six in /home/jordiae/.local/lib/python3.5/site-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /home/jordiae/.local/lib/python3.5/site-packages (from nltk->sentence-transformers) (3.4.0.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers==2.2.1->sentence-transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.34 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers==2.2.1->sentence-transformers) (1.13.34)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers==2.2.1->sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied: click in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers==2.2.1->sentence-transformers) (6.7)\n",
      "Requirement already satisfied: joblib in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers==2.2.1->sentence-transformers) (0.14.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests->transformers==2.2.1->sentence-transformers) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers==2.2.1->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers==2.2.1->sentence-transformers) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers==2.2.1->sentence-transformers) (2018.11.29)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.5/dist-packages (from botocore<1.14.0,>=1.13.34->boto3->transformers==2.2.1->sentence-transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/jordiae/.local/lib/python3.5/site-packages (from botocore<1.14.0,>=1.13.34->boto3->transformers==2.2.1->sentence-transformers) (0.15.2)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.4.1-cp35-none-any.whl size=69118 sha256=7026083d88d1bb4496f67b2f2ad06d0e0a5d64f6f57afe708c328f59dd4456fc\n",
      "  Stored in directory: /home/jordiae/.cache/pip/wheels/12/a5/1c/03b7d87e027121fe1e23048007594e73f39a23e833658529c7\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-0.2.4.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install sentence-transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (datasets.py, line 24)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3267\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-63-c4e037d073ba>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from sentence_transformers import SentenceTransformer\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jordiae/.local/lib/python3.5/site-packages/sentence_transformers/__init__.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .datasets import SentencesDataset, SentenceLabelDataset\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jordiae/.local/lib/python3.5/site-packages/sentence_transformers/datasets.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    tokens: List[List[List[str]]]\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (3.7.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/jordiae/.local/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.10.0)\n",
      "Installing collected packages: tensorflow\n",
      "\u001b[33m  The scripts freeze_graph, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jordiae/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tensorflow-1.14.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow_hub\n",
      "  Using cached https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow_hub) (1.16.2)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow_hub) (3.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/jordiae/.local/lib/python3.6/site-packages (from tensorflow_hub) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow_hub) (40.2.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "\u001b[33m  The script make_image_classifier is installed in '/home/jordiae/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tensorflow-hub-0.7.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tensorflow --user\n",
    "!python3 -m pip install tensorflow_hub --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "tf.Tensor(\n",
      "[[-0.03133016 -0.06338634 -0.01607501 ... -0.0324278  -0.04575741\n",
      "   0.05370457]\n",
      " [ 0.05080863 -0.0165243   0.01573782 ...  0.00976661  0.03170121\n",
      "   0.01788118]], shape=(2, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embeddings = embed([\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\"])\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5772823478552047"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NegatedModel():\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for row in X:\n",
    "            pred.append(-row[0])\n",
    "        return pred\n",
    "class UniversalSentenceEncoderEmbeddingsCosineSimModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dim, *kwargs):\n",
    "        self.embed =hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        self.dim = dim\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = NegatedModel()\n",
    "    def _get_embeddings_and_cosine_similarity(self, sent1, sent2):\n",
    "        embeddings = embed([sent1, sent2])\n",
    "        emb1 = embeddings[0]\n",
    "        emb2 = embeddings[1]\n",
    "        cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "        emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "        return cos_sim, emb1_emb2\n",
    "\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return np.array(\n",
    "            [[self._get_embeddings_and_cosine_similarity(sent1, sent2)[0]] for sent1, sent2 in x])\n",
    "    \n",
    "\n",
    "universalSentenceEncoderEmbeddingsCosineSimModel = UniversalSentenceEncoderEmbeddingsCosineSimModel(\n",
    "    512, train_data, train_labels)\n",
    "universalSentenceEncoderEmbeddingsCosineSimModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "class UniversalSentenceEncoderEmbeddingsMLPModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dim, *kwargs):\n",
    "        self.embed =hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        self.dim = dim\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = MLPRegressor(\n",
    "            early_stopping=True, random_state=1, max_iter=1000, hidden_layer_sizes=(512))\n",
    "    def _get_embeddings_and_cosine_similarity(self, sent1, sent2):\n",
    "        embeddings = embed([sent1, sent2])\n",
    "        emb1 = embeddings[0]\n",
    "        emb2 = embeddings[1]\n",
    "        cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "        emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "        return cos_sim, emb1_emb2\n",
    "\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return np.array(\n",
    "            [[self._get_embeddings_and_cosine_similarity(sent1, sent2)[0]] for sent1, sent2 in x])\n",
    "\n",
    "universalSentenceEncoderEmbeddingsMLPModel = UniversalSentenceEncoderEmbeddingsMLPModel(\n",
    "    512, train_data, train_labels)\n",
    "universalSentenceEncoderEmbeddingsMLPModel.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihlt",
   "language": "python",
   "name": "ihlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
