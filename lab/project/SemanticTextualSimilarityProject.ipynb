{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Final Project: Semantinc Textual Similarity\n",
    "Jordi Armengol - Joan LLop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "We start by downloading the SemEval 2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-06 11:32:53--  https://gebakx.github.io/ihlt/sts/resources/train.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.109.153, 185.199.108.153, 185.199.110.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125822 (123K) [application/octet-stream]\n",
      "Saving to: ‘data/train.tgz’\n",
      "\n",
      "train.tgz           100%[===================>] 122,87K  --.-KB/s    in 0,04s   \n",
      "\n",
      "2019-12-06 11:32:54 (3,16 MB/s) - ‘data/train.tgz’ saved [125822/125822]\n",
      "\n",
      "--2019-12-06 11:32:54--  https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118094 (115K) [application/octet-stream]\n",
      "Saving to: ‘data/test-gold.tgz’\n",
      "\n",
      "test-gold.tgz       100%[===================>] 115,33K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2019-12-06 11:32:54 (2,19 MB/s) - ‘data/test-gold.tgz’ saved [118094/118094]\n",
      "\n",
      "/home/jordiae/MAI/IHLT-MAI-clone/lab/project/data\n",
      "train/\n",
      "train/00-readme.txt\n",
      "train/STS.output.MSRpar.txt\n",
      "train/STS.input.SMTeuroparl.txt\n",
      "train/STS.input.MSRpar.txt\n",
      "train/STS.gs.MSRpar.txt\n",
      "train/STS.input.MSRvid.txt\n",
      "train/STS.gs.MSRvid.txt\n",
      "train/correlation.pl\n",
      "train/STS.gs.SMTeuroparl.txt\n",
      "test-gold/\n",
      "test-gold/STS.input.MSRpar.txt\n",
      "test-gold/STS.gs.MSRpar.txt\n",
      "test-gold/STS.input.MSRvid.txt\n",
      "test-gold/STS.gs.MSRvid.txt\n",
      "test-gold/STS.input.SMTeuroparl.txt\n",
      "test-gold/STS.gs.SMTeuroparl.txt\n",
      "test-gold/STS.input.surprise.SMTnews.txt\n",
      "test-gold/STS.gs.surprise.SMTnews.txt\n",
      "test-gold/STS.input.surprise.OnWN.txt\n",
      "test-gold/STS.gs.surprise.OnWN.txt\n",
      "test-gold/STS.gs.ALL.txt\n",
      "test-gold/00-readme.txt\n",
      "/home/jordiae/MAI/IHLT-MAI-clone/lab/project\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "train = list(zip(train_data, train_labels))\n",
    "test = list(zip(test_data, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import sklearn\n",
    "def evaluate(true_labels, predicted_labels):\n",
    "    pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "    return pearson, p_value\n",
    "def cross_validate(data, labels, model, n_folds=5, seed=1):\n",
    "    kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "    average_pearson = 0\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        m = model.fit(data[X_train], labels[X_train])\n",
    "        predicted_labels = model.predict(data[y_val])\n",
    "        pearson, _ = evaluate(labels[y_val], predicted_labels)\n",
    "        average_pearson += pearson\n",
    "    return average_pearson/kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def evaluate(true_labels, predicted_labels):\n",
    "    print('NOTE: The official Perl scripts requires installing the Statistics::Basic module')\n",
    "    print('curl -L http://cpanmin.us | perl - --sudo Statistics::Basic')\n",
    "    print()\n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "    true_labels = list(map(str, true_labels))\n",
    "    true_labels = list(map(lambda x: x + '\\n', true_labels))\n",
    "    predicted_labels = list(map(str, predicted_labels))\n",
    "    predicted_labels = list(map(lambda x: x + '\\t100\\n', predicted_labels))\n",
    "    print(predicted_labels)\n",
    "    tmp = os.path.join('data', 'tmp')\n",
    "    if not os.path.exists(tmp):\n",
    "        os.makedirs(tmp)\n",
    "    timestamp = 'output_{date:%Y-%m-%d_%H:%M:%S}.txt'.format(date=datetime.now())\n",
    "    os.makedirs(os.path.join(tmp, timestamp))\n",
    "    predicted_path = os.path.join(tmp, timestamp, 'predicted.txt')\n",
    "    with open(predicted_path, 'w') as f:\n",
    "        f.writelines(predicted_labels)\n",
    "    true_path = os.path.join(tmp, timestamp, 'true.txt')\n",
    "    with open(true_path, 'w') as f:\n",
    "        f.writelines(true_labels)\n",
    "    res_path = os.path.join(tmp, timestamp, 'res.txt')\n",
    "    cmd = os.path.join('data', 'train', 'correlation.pl') + ' ' + true_path + ' ' + predicted_path\n",
    "    print(cmd)\n",
    "    res = os.popen(cmd).read()\n",
    "    with open(res_path, 'w') as f:\n",
    "        f.write(res)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
