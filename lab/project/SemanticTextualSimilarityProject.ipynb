{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Final Project: Semantinc Textual Similarity\n",
    "Jordi Armengol - Joan LLop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "We start by downloading the SemEval 2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-08 01:54:10--  https://gebakx.github.io/ihlt/sts/resources/train.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125822 (123K) [application/octet-stream]\n",
      "Saving to: ‘data/train.tgz.8’\n",
      "\n",
      "train.tgz.8         100%[===================>] 122,87K  --.-KB/s    in 0,06s   \n",
      "\n",
      "2019-12-08 01:54:10 (1,85 MB/s) - ‘data/train.tgz.8’ saved [125822/125822]\n",
      "\n",
      "--2019-12-08 01:54:10--  https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118094 (115K) [application/octet-stream]\n",
      "Saving to: ‘data/test-gold.tgz.8’\n",
      "\n",
      "test-gold.tgz.8     100%[===================>] 115,33K  --.-KB/s    in 0,06s   \n",
      "\n",
      "2019-12-08 01:54:10 (1,94 MB/s) - ‘data/test-gold.tgz.8’ saved [118094/118094]\n",
      "\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project/data\n",
      "train/\n",
      "train/00-readme.txt\n",
      "train/STS.output.MSRpar.txt\n",
      "train/STS.input.SMTeuroparl.txt\n",
      "train/STS.input.MSRpar.txt\n",
      "train/STS.gs.MSRpar.txt\n",
      "train/STS.input.MSRvid.txt\n",
      "train/STS.gs.MSRvid.txt\n",
      "train/correlation.pl\n",
      "train/STS.gs.SMTeuroparl.txt\n",
      "test-gold/\n",
      "test-gold/STS.input.MSRpar.txt\n",
      "test-gold/STS.gs.MSRpar.txt\n",
      "test-gold/STS.input.MSRvid.txt\n",
      "test-gold/STS.gs.MSRvid.txt\n",
      "test-gold/STS.input.SMTeuroparl.txt\n",
      "test-gold/STS.gs.SMTeuroparl.txt\n",
      "test-gold/STS.input.surprise.SMTnews.txt\n",
      "test-gold/STS.gs.surprise.SMTnews.txt\n",
      "test-gold/STS.input.surprise.OnWN.txt\n",
      "test-gold/STS.gs.surprise.OnWN.txt\n",
      "test-gold/STS.gs.ALL.txt\n",
      "test-gold/00-readme.txt\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General class/interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,  x, y, regr=LinearRegression(),):\n",
    "        self.regr = regr\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.name = None\n",
    "        self.description = None\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        pickle.dump(self, open(self.name + '.model', 'wb').write())\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, name):\n",
    "        return pickle.load(open(self.name + '.model', 'rb').read())\n",
    "        \n",
    "    \n",
    "    def _extract_features(self, x):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.regr.fit(self.x_features, self.y)\n",
    "    \n",
    "    \n",
    "    def predict(self, new_x):\n",
    "        new_x_features = self._extract_features(new_x)\n",
    "        return self.regr.predict(new_x_features)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, true_labels, predicted_labels):\n",
    "        pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "        return pearson, p_value\n",
    "    \n",
    "    \n",
    "    def cross_validate(self, n_folds=5, seed=1):\n",
    "        assert self.x_features is not None\n",
    "        kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "        average_pearson = 0\n",
    "        for train_index, val_index in kf.split(self.x_features):\n",
    "            X_train, X_val = self.x_features[train_index], self.x_features[val_index]\n",
    "            y_train, y_val = self.y[train_index], self.y[val_index]\n",
    "            self.regr.fit(X_train, y_train)\n",
    "            predicted_labels = self.regr.predict(X_val)\n",
    "            pearson, _ = self.evaluate(y_val, predicted_labels)\n",
    "            average_pearson += abs(pearson)\n",
    "        return average_pearson/n_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 1: Linguistic feature engineering and classical machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class JaccardModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'prova0'\n",
    "        self.description = 'primera prova'\n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "        super().__init__(*kwargs)\n",
    "\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(sent):\n",
    "            preprocessed = \"\"\n",
    "            for char in sent:\n",
    "                if char.isdigit():\n",
    "                    preprocessed += char\n",
    "                elif char.isalpha():\n",
    "                    preprocessed += char.lower()\n",
    "                elif char == ' ':\n",
    "                    preprocessed += char\n",
    "\n",
    "            return str(preprocessed)\n",
    "\n",
    "        x = [[preprocess(sent1), preprocess(sent2)] for sent1, sent2 in x]\n",
    "        \n",
    "        def lemmatize(token, pos):\n",
    "            if pos in {'N','V'}:\n",
    "                return wnl.lemmatize(token.lower(), pos.lower())\n",
    "            return token.lower()\n",
    "\n",
    "\n",
    "        def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "            mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "            if nltk_pos in mapping:\n",
    "                return mapping[nltk_pos]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def get_synsets(sent):\n",
    "            saved_synsets = []\n",
    "            tokens = word_tokenize(sent)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            for token, pos, lemma in zip(tokens, pos_tags, lemmas):\n",
    "                wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "                if wordnet_pos is not None:\n",
    "                    word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "                    if len(word_synsets) > 0:\n",
    "                        most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                        saved_synsets.append(most_freq_synset)\n",
    "            return saved_synsets\n",
    "\n",
    "\n",
    "        def get_features_from_word(sent, index, pos):\n",
    "            word = sent[index]\n",
    "            features = []\n",
    "            features.append(str(pos)) # Part-of-Speech                   \n",
    "            features.append(str(len(word))) # length of word\n",
    "            features.append(str(index==0)) # beggining of a sentence\n",
    "            features.append(str(index==len(sent)-1)) # end of sentence\n",
    "            features.append(str(word.isdigit())) # is a digit\n",
    "            return features\n",
    "\n",
    "        def sent2features(sent):\n",
    "            features = []\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.stop_words]\n",
    "            features.append(tokens)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            features.append(pos_tags)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            features.append(lemmas)\n",
    "            synsets = get_synsets(sent)\n",
    "            if len(synsets) > 0:\n",
    "                features.append(synsets)\n",
    "            else:\n",
    "                features.append([0])\n",
    "            temp_f = []\n",
    "            for i in range(len(tokens)):\n",
    "                temp_f += get_features_from_word(tokens, i, pos_tags[i])\n",
    "            features.append(temp_f)\n",
    "\n",
    "            return features\n",
    "        \n",
    "        def distance(features1, features2, sent1, sent2, index):\n",
    "            distances = []\n",
    "            init = True\n",
    "            for f1, f2 in zip(features1, features2):\n",
    "                distances.append(jaccard_distance(set(f1), set(f2)))\n",
    "\n",
    "            # ...\n",
    "            return distances\n",
    "\n",
    "        \n",
    "        pairs_of_features = [(sent2features(sent1), sent2features(sent2)) for sent1, sent2 in x]\n",
    "        distances = np.array([distance(features1, features2, sent1, sent2, index) for index, ((features1, features2), (sent1, sent2)) in enumerate(zip(pairs_of_features, x))])\n",
    "        return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744670790260961"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova0 = JaccardModel(train_data, train_labels)\n",
    "prova0.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import sklearn\n",
    "def evaluate(true_labels, predicted_labels):\n",
    "    pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "    return pearson, p_value\n",
    "def cross_validate(data, labels, model, n_folds=5, seed=1):\n",
    "    kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "    average_pearson = 0\n",
    "    for train_index, val_index in kf.split(data):\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = labels[train_index], labels[val_index]\n",
    "        m = model.fit(X_train, y_train)\n",
    "        predicted_labels = model.predict(X_val)\n",
    "        pearson, _ = evaluate(y_val, predicted_labels)\n",
    "        average_pearson += pearson\n",
    "    return average_pearson/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4109743529779384"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(distances, train_labels, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-06 15:12:23--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘data/wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650,22M  10,4MB/s    in 64s     \n",
      "\n",
      "2019-12-06 15:13:28 (10,1 MB/s) - ‘data/wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n",
      "/home/jordiae/MAI/IHLT-MAI-clone/lab/project/data\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n",
      "/home/jordiae/MAI/IHLT-MAI-clone/lab/project\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip --directory-prefix=data\n",
    "%cd data\n",
    "!unzip wiki-news-300d-1M.vec.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "train_tokens = [word_tokenize(sent1) + word_tokenize(sent2) for sent1, sent2 in train_data]\n",
    "vocabulary = set([])\n",
    "for tokenized in train_tokens:\n",
    "    for token in tokenized:\n",
    "        vocabulary.add(token)\n",
    "pretrained_embeddings_path = os.path.join('data', 'wiki-news-300d-1M.vec')\n",
    "needed_tokens = set()\n",
    "embedding_table = {}\n",
    "dim = 0\n",
    "for line in open(pretrained_embeddings_path, 'r').readlines():\n",
    "    if dim == 0:\n",
    "        dim = int(line.split()[1])\n",
    "        continue\n",
    "    row = line.split()\n",
    "    token = row[0]\n",
    "    if token not in vocabulary:\n",
    "        continue\n",
    "    vector = np.array(list(map(float, row[1:])))\n",
    "    embedding_table[token] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6282995295159088\n",
      "0.3378078759991627\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_sentence_embedding(sent, embedding_table, dim, method='max'):\n",
    "    assert method in ['avg', 'sum', 'max']\n",
    "    tokenized = word_tokenize(sent)\n",
    "    def contains_punct(token):\n",
    "        for c in string.punctuation:\n",
    "            if c in token:\n",
    "                return True\n",
    "        return False\n",
    "    tokenized = [token for token in tokenized if not contains_punct(token)] # empitjora\n",
    "    tokenized = [token.lower() for token in tokenized if token.lower() not in stop_words] # empitjora\n",
    "    embeddings = np.zeros((len(tokenized), dim))\n",
    "    for idx, token in enumerate(tokenized):\n",
    "        if token in embedding_table:\n",
    "            embeddings[idx] = embedding_table[token]\n",
    "        else:\n",
    "            embeddings[idx] = np.zeros(dim)       \n",
    "    if method == 'avg':\n",
    "        aggregated_embeddings = np.mean(embeddings, axis=0)\n",
    "    elif method == 'sum':\n",
    "        aggregated_embeddings = np.sum(embeddings, axis=0)\n",
    "    elif method == 'max':\n",
    "        aggregated_embeddings = np.max(embeddings, axis=0)\n",
    "    return aggregated_embeddings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "def get_embeddings_and_cosine_similarity(sent1, sent2, embedding_table, dim):\n",
    "    emb1 = get_sentence_embedding(sent1, embedding_table, dim)\n",
    "    emb2 = get_sentence_embedding(sent2, embedding_table, dim)\n",
    "    cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "    emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "    return cos_sim, emb1_emb2\n",
    "\n",
    "cosine_similarities = np.zeros((len(train_data), 1))\n",
    "embeddings = np.zeros((len(train_data), dim*2))\n",
    "for idx, (sent1, sent2) in enumerate(train_data):\n",
    "    cos_sim, emb1_emb2 = get_embeddings_and_cosine_similarity(sent1, sent2, embedding_table, dim)\n",
    "    cosine_similarities[idx] = np.array([cos_sim])\n",
    "    embeddings[idx] = np.array(emb1_emb2)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#cross_validate(cosine_similarities, train_labels, RandomForestRegressor(max_depth=4, random_state=0))\n",
    "\n",
    "class NegatedModel():\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for row in X:\n",
    "            pred.append(-row[0])\n",
    "        return pred\n",
    "print(cross_validate(cosine_similarities, train_labels, NegatedModel()))\n",
    "print(cross_validate(embeddings, train_labels, LinearRegression()))\n",
    "#print(cross_validate(embeddings, train_labels, MLPRegressor(early_stopping=True, random_state=1, max_iter=1000, hidden_layer_sizes=(300,300))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('outfile', cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05247012])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = np.load('outfile.npy')\n",
    "cos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jordiae/.local/lib/python3.5/site-packages (2.2.1)\n",
      "Requirement already satisfied: tqdm in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (4.29.1)\n",
      "Requirement already satisfied: regex in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (2018.1.10)\n",
      "Requirement already satisfied: sentencepiece in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied: boto3 in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (1.10.34)\n",
      "Requirement already satisfied: requests in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (1.16.2)\n",
      "Requirement already satisfied: sacremoses in /home/jordiae/.local/lib/python3.5/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.34 in /home/jordiae/.local/lib/python3.5/site-packages (from boto3->transformers) (1.13.34)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/jordiae/.local/lib/python3.5/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests->transformers) (2.7)\n",
      "Requirement already satisfied: six in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: joblib in /home/jordiae/.local/lib/python3.5/site-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.5/dist-packages (from botocore<1.14.0,>=1.13.34->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/jordiae/.local/lib/python3.5/site-packages (from botocore<1.14.0,>=1.13.34->boto3->transformers) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1206 18:17:41.475112 140300861650688 file_utils.py:319] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmppil2k8dg\n",
      "100%|██████████| 213450/213450 [00:00<00:00, 398927.28B/s]\n",
      "I1206 18:17:42.593273 140300861650688 file_utils.py:334] copying /tmp/tmppil2k8dg to cache at /home/jordiae/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1206 18:17:42.603806 140300861650688 file_utils.py:338] creating metadata file for /home/jordiae/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1206 18:17:42.610208 140300861650688 file_utils.py:347] removing temp file /tmp/tmppil2k8dg\n",
      "I1206 18:17:42.612698 140300861650688 tokenization_utils.py:379] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/jordiae/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1206 18:17:43.486097 140300861650688 file_utils.py:319] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpjkzx4nu_\n",
      "100%|██████████| 313/313 [00:00<00:00, 43635.48B/s]\n",
      "I1206 18:17:44.272827 140300861650688 file_utils.py:334] copying /tmp/tmpjkzx4nu_ to cache at /home/jordiae/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "I1206 18:17:44.279542 140300861650688 file_utils.py:338] creating metadata file for /home/jordiae/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "I1206 18:17:44.284778 140300861650688 file_utils.py:347] removing temp file /tmp/tmpjkzx4nu_\n",
      "I1206 18:17:44.289153 140300861650688 configuration_utils.py:157] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/jordiae/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "I1206 18:17:44.295077 140300861650688 configuration_utils.py:174] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I1206 18:17:44.876996 140300861650688 file_utils.py:319] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpn83w76n8\n",
      "100%|██████████| 435779157/435779157 [00:19<00:00, 22609598.21B/s]\n",
      "I1206 18:18:04.982883 140300861650688 file_utils.py:334] copying /tmp/tmpn83w76n8 to cache at /home/jordiae/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I1206 18:18:06.260002 140300861650688 file_utils.py:338] creating metadata file for /home/jordiae/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I1206 18:18:06.261048 140300861650688 file_utils.py:347] removing temp file /tmp/tmpn83w76n8\n",
      "I1206 18:18:06.318659 140300861650688 modeling_utils.py:393] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/jordiae/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5132,  0.5097,  0.1991,  ..., -0.3900,  0.4053, -0.2315],\n",
       "         [ 0.5395, -0.3658,  0.6667,  ..., -0.3920,  0.2505,  0.0202],\n",
       "         [ 0.7767,  0.6823,  0.7110,  ..., -0.0420, -0.3718,  0.3748],\n",
       "         ...,\n",
       "         [ 0.3555,  0.4486,  0.6175,  ..., -0.0388, -0.2631,  0.3514],\n",
       "         [ 0.7927, -0.1282,  0.2737,  ..., -0.5220,  0.4836,  0.0937],\n",
       "         [ 1.2903,  1.0356,  0.5054,  ..., -0.4344,  1.1973, -0.4236]]],\n",
       "       grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers, torch\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "#padding = [0] * ( 128 - len(input_ids))\n",
    "#input_ids += padding\n",
    "\n",
    "#attn_mask = input_ids.ne(0) # I added this to create a mask for padded indices\n",
    "outputs = model(input_ids)#, attention_mask=attn_mask)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 768)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.detach().numpy().shape\n",
    "#np.mean(last_hidden_states.detach().numpy()[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 2234\n",
      "10 of 2234\n",
      "20 of 2234\n",
      "30 of 2234\n",
      "40 of 2234\n",
      "50 of 2234\n",
      "60 of 2234\n",
      "70 of 2234\n",
      "80 of 2234\n",
      "90 of 2234\n",
      "100 of 2234\n",
      "110 of 2234\n",
      "120 of 2234\n",
      "130 of 2234\n",
      "140 of 2234\n",
      "150 of 2234\n",
      "160 of 2234\n",
      "170 of 2234\n",
      "180 of 2234\n",
      "190 of 2234\n",
      "200 of 2234\n",
      "210 of 2234\n",
      "220 of 2234\n",
      "230 of 2234\n",
      "240 of 2234\n",
      "250 of 2234\n",
      "260 of 2234\n",
      "270 of 2234\n",
      "280 of 2234\n",
      "290 of 2234\n",
      "300 of 2234\n",
      "310 of 2234\n",
      "320 of 2234\n",
      "330 of 2234\n",
      "340 of 2234\n",
      "350 of 2234\n",
      "360 of 2234\n",
      "370 of 2234\n",
      "380 of 2234\n",
      "390 of 2234\n",
      "400 of 2234\n",
      "410 of 2234\n",
      "420 of 2234\n",
      "430 of 2234\n",
      "440 of 2234\n",
      "450 of 2234\n",
      "460 of 2234\n",
      "470 of 2234\n",
      "480 of 2234\n",
      "490 of 2234\n",
      "500 of 2234\n",
      "510 of 2234\n",
      "520 of 2234\n",
      "530 of 2234\n",
      "540 of 2234\n",
      "550 of 2234\n",
      "560 of 2234\n",
      "570 of 2234\n",
      "580 of 2234\n",
      "590 of 2234\n",
      "600 of 2234\n",
      "610 of 2234\n",
      "620 of 2234\n",
      "630 of 2234\n",
      "640 of 2234\n",
      "650 of 2234\n",
      "660 of 2234\n",
      "670 of 2234\n",
      "680 of 2234\n",
      "690 of 2234\n",
      "700 of 2234\n",
      "710 of 2234\n",
      "720 of 2234\n",
      "730 of 2234\n",
      "740 of 2234\n",
      "750 of 2234\n",
      "760 of 2234\n",
      "770 of 2234\n",
      "780 of 2234\n",
      "790 of 2234\n",
      "800 of 2234\n",
      "810 of 2234\n",
      "820 of 2234\n",
      "830 of 2234\n",
      "840 of 2234\n",
      "850 of 2234\n",
      "860 of 2234\n",
      "870 of 2234\n",
      "880 of 2234\n",
      "890 of 2234\n",
      "900 of 2234\n",
      "910 of 2234\n",
      "920 of 2234\n",
      "930 of 2234\n",
      "940 of 2234\n",
      "950 of 2234\n",
      "960 of 2234\n",
      "970 of 2234\n",
      "980 of 2234\n",
      "990 of 2234\n",
      "1000 of 2234\n",
      "1010 of 2234\n",
      "1020 of 2234\n",
      "1030 of 2234\n",
      "1040 of 2234\n",
      "1050 of 2234\n",
      "1060 of 2234\n",
      "1070 of 2234\n",
      "1080 of 2234\n",
      "1090 of 2234\n",
      "1100 of 2234\n",
      "1110 of 2234\n",
      "1120 of 2234\n",
      "1130 of 2234\n",
      "1140 of 2234\n",
      "1150 of 2234\n",
      "1160 of 2234\n",
      "1170 of 2234\n",
      "1180 of 2234\n",
      "1190 of 2234\n",
      "1200 of 2234\n",
      "1210 of 2234\n",
      "1220 of 2234\n",
      "1230 of 2234\n",
      "1240 of 2234\n",
      "1250 of 2234\n",
      "1260 of 2234\n",
      "1270 of 2234\n",
      "1280 of 2234\n",
      "1290 of 2234\n",
      "1300 of 2234\n",
      "1310 of 2234\n",
      "1320 of 2234\n",
      "1330 of 2234\n",
      "1340 of 2234\n",
      "1350 of 2234\n",
      "1360 of 2234\n",
      "1370 of 2234\n",
      "1380 of 2234\n",
      "1390 of 2234\n",
      "1400 of 2234\n",
      "1410 of 2234\n",
      "1420 of 2234\n",
      "1430 of 2234\n",
      "1440 of 2234\n",
      "1450 of 2234\n",
      "1460 of 2234\n",
      "1470 of 2234\n",
      "1480 of 2234\n",
      "1490 of 2234\n",
      "1500 of 2234\n",
      "1510 of 2234\n",
      "1520 of 2234\n",
      "1530 of 2234\n",
      "1540 of 2234\n",
      "1550 of 2234\n",
      "1560 of 2234\n",
      "1570 of 2234\n",
      "1580 of 2234\n",
      "1590 of 2234\n",
      "1600 of 2234\n",
      "1610 of 2234\n",
      "1620 of 2234\n",
      "1630 of 2234\n",
      "1640 of 2234\n",
      "1650 of 2234\n",
      "1660 of 2234\n",
      "1670 of 2234\n",
      "1680 of 2234\n",
      "1690 of 2234\n",
      "1700 of 2234\n",
      "1710 of 2234\n",
      "1720 of 2234\n",
      "1730 of 2234\n",
      "1740 of 2234\n",
      "1750 of 2234\n",
      "1760 of 2234\n",
      "1770 of 2234\n",
      "1780 of 2234\n",
      "1790 of 2234\n",
      "1800 of 2234\n",
      "1810 of 2234\n",
      "1820 of 2234\n",
      "1830 of 2234\n",
      "1840 of 2234\n",
      "1850 of 2234\n",
      "1860 of 2234\n",
      "1870 of 2234\n",
      "1880 of 2234\n",
      "1890 of 2234\n",
      "1900 of 2234\n",
      "1910 of 2234\n",
      "1920 of 2234\n",
      "1930 of 2234\n",
      "1940 of 2234\n",
      "1950 of 2234\n",
      "1960 of 2234\n",
      "1970 of 2234\n",
      "1980 of 2234\n",
      "1990 of 2234\n",
      "2000 of 2234\n",
      "2010 of 2234\n",
      "2020 of 2234\n",
      "2030 of 2234\n",
      "2040 of 2234\n",
      "2050 of 2234\n",
      "2060 of 2234\n",
      "2070 of 2234\n",
      "2080 of 2234\n",
      "2090 of 2234\n",
      "2100 of 2234\n",
      "2110 of 2234\n",
      "2120 of 2234\n",
      "2130 of 2234\n",
      "2140 of 2234\n",
      "2150 of 2234\n",
      "2160 of 2234\n",
      "2170 of 2234\n",
      "2180 of 2234\n",
      "2190 of 2234\n",
      "2200 of 2234\n",
      "2210 of 2234\n",
      "2220 of 2234\n",
      "2230 of 2234\n",
      "0.5105574211248864\n",
      "0.0944570744222997\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def get_sentence_embedding(sent, model, dim, method='avg'):\n",
    "    assert method in ['avg', 'sum']\n",
    "    input_ids = torch.tensor(tokenizer.encode(sent)).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]\n",
    "    if method == 'avg':\n",
    "        aggregated_embeddings = np.mean(last_hidden_states.detach().numpy()[0], axis=0)\n",
    "    elif method == 'sum':\n",
    "        aggregated_embeddings = np.sum(last_hidden_states.detach().numpy()[0], axis=0)\n",
    "    return aggregated_embeddings\n",
    "\n",
    "def get_embeddings_and_cosine_similarity(sent1, sent2, model, dim):\n",
    "    emb1 = get_sentence_embedding(sent1, model, dim)\n",
    "    emb2 = get_sentence_embedding(sent2, model, dim)\n",
    "    cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "    emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "    return cos_sim, emb1_emb2\n",
    "\n",
    "cosine_similarities = np.zeros((len(train_data), 1))\n",
    "embeddings = np.zeros((len(train_data), 768*2))\n",
    "for idx, (sent1, sent2) in enumerate(train_data):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx, 'of', len(train_data))\n",
    "    cos_sim, emb1_emb2 = get_embeddings_and_cosine_similarity(sent1, sent2, model, 768)\n",
    "    cosine_similarities[idx] = np.array([cos_sim])\n",
    "    embeddings[idx] = np.array(emb1_emb2)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#cross_validate(cosine_similarities, train_labels, RandomForestRegressor(max_depth=4, random_state=0))\n",
    "\n",
    "class NegatedModel():\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for row in X:\n",
    "            pred.append(-row[0])\n",
    "        return pred\n",
    "print(cross_validate(cosine_similarities, train_labels, NegatedModel()))\n",
    "print(cross_validate(embeddings, train_labels, LinearRegression()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Template\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575376193600634"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "class BagWordsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagWords'\n",
    "        self.description = 'We get the bag of words of both setences, calculate the union, get the count of each word of the union for each sentence and get the distance as the sum of the element-wise distance'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char \n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def get_simple_bag_of_words(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            inter = np.intersect1d(tokens1, tokens2)\n",
    "            return [len(inter)]\n",
    "        \n",
    "        \n",
    "        def get_bag_of_words(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            union = np.union1d(tokens1, tokens2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for token in tokens1:\n",
    "                count1[np.where(union == token)] += 1\n",
    "            for token in tokens2:\n",
    "                count2[np.where(union == token)] += 1\n",
    "            return [np.average(np.abs(count1 - count2))]\n",
    "                \n",
    "            \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_words(sent1, sent2) + get_simple_bag_of_words(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "model0 = BagWordsModel(train_data, train_labels)\n",
    "model0.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best: 0.5753761936006341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going back to jaccard distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49781067395018946"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class JaccardModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'JaccardModel'\n",
    "        self.description = 'JaccardModel'\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data): \n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char \n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def get_jaccard_of_pos(sent1, sent2, stop_words):\n",
    "            tokens1 = word_tokenize(sent1)\n",
    "            tokens2 = word_tokenize(sent2)\n",
    "            \n",
    "            pos1 = set([pos for word, pos in pos_tag(tokens1) if not word in stop_words])\n",
    "            pos2 = set([pos for word, pos in pos_tag(tokens2) if not word in stop_words])\n",
    "            return [jaccard_distance(pos1, pos2), jaccard_distance(set(tokens1), set(tokens2))]\n",
    "                \n",
    "            \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_jaccard_of_pos(sent1, sent2, stop_words) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "model0 = JaccardModel(train_data, train_labels)\n",
    "model0.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams vector representation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
