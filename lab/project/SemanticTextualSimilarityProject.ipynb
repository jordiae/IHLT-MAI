{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Final Project: Semantinc Textual Similarity\n",
    "Jordi Armengol - Joan LLop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "We start by downloading the SemEval 2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-06 13:39:06--  https://gebakx.github.io/ihlt/sts/resources/train.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.111.153, 185.199.110.153, 185.199.109.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125822 (123K) [application/octet-stream]\n",
      "Saving to: ‘data/train.tgz.4’\n",
      "\n",
      "train.tgz.4         100%[===================>] 122,87K  --.-KB/s    in 0,04s   \n",
      "\n",
      "2019-12-06 13:39:06 (2,91 MB/s) - ‘data/train.tgz.4’ saved [125822/125822]\n",
      "\n",
      "--2019-12-06 13:39:06--  https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.111.153, 185.199.110.153, 185.199.109.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118094 (115K) [application/octet-stream]\n",
      "Saving to: ‘data/test-gold.tgz.4’\n",
      "\n",
      "test-gold.tgz.4     100%[===================>] 115,33K  --.-KB/s    in 0,04s   \n",
      "\n",
      "2019-12-06 13:39:07 (2,66 MB/s) - ‘data/test-gold.tgz.4’ saved [118094/118094]\n",
      "\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project/data\n",
      "train/\n",
      "train/00-readme.txt\n",
      "train/STS.output.MSRpar.txt\n",
      "train/STS.input.SMTeuroparl.txt\n",
      "train/STS.input.MSRpar.txt\n",
      "train/STS.gs.MSRpar.txt\n",
      "train/STS.input.MSRvid.txt\n",
      "train/STS.gs.MSRvid.txt\n",
      "train/correlation.pl\n",
      "train/STS.gs.SMTeuroparl.txt\n",
      "test-gold/\n",
      "test-gold/STS.input.MSRpar.txt\n",
      "test-gold/STS.gs.MSRpar.txt\n",
      "test-gold/STS.input.MSRvid.txt\n",
      "test-gold/STS.gs.MSRvid.txt\n",
      "test-gold/STS.input.SMTeuroparl.txt\n",
      "test-gold/STS.gs.SMTeuroparl.txt\n",
      "test-gold/STS.input.surprise.SMTnews.txt\n",
      "test-gold/STS.gs.surprise.SMTnews.txt\n",
      "test-gold/STS.input.surprise.OnWN.txt\n",
      "test-gold/STS.gs.surprise.OnWN.txt\n",
      "test-gold/STS.gs.ALL.txt\n",
      "test-gold/00-readme.txt\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 1: Linguistic feature engineering and classical machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['But',\n",
       "   'other',\n",
       "   'sources',\n",
       "   'close',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sale',\n",
       "   'said',\n",
       "   'Vivendi',\n",
       "   'was',\n",
       "   'keeping',\n",
       "   'the',\n",
       "   'door',\n",
       "   'open',\n",
       "   'to',\n",
       "   'further',\n",
       "   'bids',\n",
       "   'and',\n",
       "   'hoped',\n",
       "   'to',\n",
       "   'see',\n",
       "   'bidders',\n",
       "   'interested',\n",
       "   'in',\n",
       "   'individual',\n",
       "   'assets',\n",
       "   'team',\n",
       "   'up',\n",
       "   '.'],\n",
       "  [('But', 'CC'),\n",
       "   ('other', 'JJ'),\n",
       "   ('sources', 'NNS'),\n",
       "   ('close', 'RB'),\n",
       "   ('to', 'TO'),\n",
       "   ('the', 'DT'),\n",
       "   ('sale', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('Vivendi', 'NNP'),\n",
       "   ('was', 'VBD'),\n",
       "   ('keeping', 'VBG'),\n",
       "   ('the', 'DT'),\n",
       "   ('door', 'NN'),\n",
       "   ('open', 'JJ'),\n",
       "   ('to', 'TO'),\n",
       "   ('further', 'JJ'),\n",
       "   ('bids', 'NNS'),\n",
       "   ('and', 'CC'),\n",
       "   ('hoped', 'VBD'),\n",
       "   ('to', 'TO'),\n",
       "   ('see', 'VB'),\n",
       "   ('bidders', 'NNS'),\n",
       "   ('interested', 'JJ'),\n",
       "   ('in', 'IN'),\n",
       "   ('individual', 'JJ'),\n",
       "   ('assets', 'NNS'),\n",
       "   ('team', 'VBP'),\n",
       "   ('up', 'RB'),\n",
       "   ('.', '.')],\n",
       "  ['but',\n",
       "   'other',\n",
       "   'sources',\n",
       "   'close',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sale',\n",
       "   'said',\n",
       "   'vivendi',\n",
       "   'was',\n",
       "   'keeping',\n",
       "   'the',\n",
       "   'door',\n",
       "   'open',\n",
       "   'to',\n",
       "   'further',\n",
       "   'bids',\n",
       "   'and',\n",
       "   'hoped',\n",
       "   'to',\n",
       "   'see',\n",
       "   'bidders',\n",
       "   'interested',\n",
       "   'in',\n",
       "   'individual',\n",
       "   'assets',\n",
       "   'team',\n",
       "   'up',\n",
       "   '.']],\n",
       " [['But',\n",
       "   'other',\n",
       "   'sources',\n",
       "   'close',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sale',\n",
       "   'said',\n",
       "   'Vivendi',\n",
       "   'was',\n",
       "   'keeping',\n",
       "   'the',\n",
       "   'door',\n",
       "   'open',\n",
       "   'for',\n",
       "   'further',\n",
       "   'bids',\n",
       "   'in',\n",
       "   'the',\n",
       "   'next',\n",
       "   'day',\n",
       "   'or',\n",
       "   'two',\n",
       "   '.'],\n",
       "  [('But', 'CC'),\n",
       "   ('other', 'JJ'),\n",
       "   ('sources', 'NNS'),\n",
       "   ('close', 'RB'),\n",
       "   ('to', 'TO'),\n",
       "   ('the', 'DT'),\n",
       "   ('sale', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('Vivendi', 'NNP'),\n",
       "   ('was', 'VBD'),\n",
       "   ('keeping', 'VBG'),\n",
       "   ('the', 'DT'),\n",
       "   ('door', 'NN'),\n",
       "   ('open', 'JJ'),\n",
       "   ('for', 'IN'),\n",
       "   ('further', 'JJ'),\n",
       "   ('bids', 'NNS'),\n",
       "   ('in', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('next', 'JJ'),\n",
       "   ('day', 'NN'),\n",
       "   ('or', 'CC'),\n",
       "   ('two', 'CD'),\n",
       "   ('.', '.')],\n",
       "  ['but',\n",
       "   'other',\n",
       "   'sources',\n",
       "   'close',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sale',\n",
       "   'said',\n",
       "   'vivendi',\n",
       "   'was',\n",
       "   'keeping',\n",
       "   'the',\n",
       "   'door',\n",
       "   'open',\n",
       "   'for',\n",
       "   'further',\n",
       "   'bids',\n",
       "   'in',\n",
       "   'the',\n",
       "   'next',\n",
       "   'day',\n",
       "   'or',\n",
       "   'two',\n",
       "   '.']])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(token, pos):\n",
    "    if pos in {'N','V'}:\n",
    "        return wnl.lemmatize(token.lower(), pos.lower())\n",
    "    return token.lower()\n",
    "\n",
    "def sent2features(sent):\n",
    "    features = []\n",
    "    tokens = word_tokenize(sent)\n",
    "    features.append(tokens)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    features.append(pos_tags)\n",
    "    features.append([lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)])\n",
    "    return features\n",
    "\n",
    "\n",
    "pairs_of_features = [(sent2features(sent1), sent2features(sent2)) for sent1, sent2 in train_data]\n",
    "pairs_of_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distances between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "def distance(features1, features2):\n",
    "    distances = []\n",
    "    for f1, f2 in zip(features1, features2):\n",
    "        distances.append(jaccard_distance(set(f1), set(f2)))\n",
    "    # ...\n",
    "    return np.array(distances)\n",
    "\n",
    "\n",
    "distances = np.array([distance(features1, features2) for features1, features2 in pairs_of_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import sklearn\n",
    "def evaluate(true_labels, predicted_labels):\n",
    "    pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "    return pearson, p_value\n",
    "def cross_validate(data, labels, model, n_folds=5, seed=1):\n",
    "    kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "    average_pearson = 0\n",
    "    for train_index, val_index in kf.split(data):\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = labels[train_index], labels[val_index]\n",
    "        m = model.fit(X_train, y_train)\n",
    "        predicted_labels = model.predict(X_val)\n",
    "        pearson, _ = evaluate(y_val, predicted_labels)\n",
    "        average_pearson += pearson\n",
    "    return average_pearson/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45266685153720737"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(distances, train_labels, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 2: Transfer learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
