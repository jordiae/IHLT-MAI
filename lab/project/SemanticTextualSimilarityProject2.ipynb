{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Final Project: Semantic Textual Similarity Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "We start by downloading the SemEval 2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus assembly\n",
    "Train and test sets. The test set will not be used for learning or model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 1: Classical NLP and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "stopwords_set = set(stopwords.words('english')) \n",
    "\n",
    "def preprocess(X):\n",
    "    def is_number(s):\n",
    "        try:\n",
    "            x = float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def characters_not_punct(token):\n",
    "        for c in token:\n",
    "            if c in punctuation:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def lemmatize(token, pos):\n",
    "        if pos in {'N','V'}:\n",
    "            return wnl.lemmatize(token.lower(), pos.lower())\n",
    "        return token.lower()\n",
    "\n",
    "    def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "        mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "        if nltk_pos in mapping:\n",
    "            return mapping[nltk_pos]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_synset(lemma, pos):\n",
    "        wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "        if wordnet_pos is not None:\n",
    "            word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "            if len(word_synsets) > 0:\n",
    "                most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                return most_freq_synset\n",
    "        return None\n",
    "    \n",
    "    def get_nes(pos_tags):\n",
    "        nes = ne_chunk(pos_tags, binary=False)\n",
    "        nes_map = []\n",
    "        for tree_element in nes:\n",
    "            if type(tree_element) == nltk.tree.Tree:\n",
    "                for element in tree_element:\n",
    "                        nes_map.append(tree_element.label())\n",
    "            else:\n",
    "                nes_map.append(None)\n",
    "        return nes_map\n",
    "                \n",
    "    def preprocess_sentence(sent):\n",
    "        tokens = word_tokenize(sent)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        clean_tokens = []\n",
    "        synsets = set([])\n",
    "        chars = ''.join([c for c in sent if c not in punctuation + ' '])\n",
    "        nes_map = get_nes(pos_tags)\n",
    "        for token, pos, ne in zip(tokens, pos_tags, nes_map):\n",
    "            if ne is not None:\n",
    "                token = ne\n",
    "                clean_tokens.append(token)\n",
    "            if token not in stopwords_set and characters_not_punct(token):\n",
    "                if is_number(token):\n",
    "                    token = 'IS_NUMBER'\n",
    "                else:\n",
    "                    lemma = lemmatize(token, pos)\n",
    "                    token = lemma\n",
    "                    synset = get_synset(lemma, pos)\n",
    "                    if synset is not None:\n",
    "                        synsets.add(synset)\n",
    "                clean_tokens.append(token)\n",
    "        return clean_tokens, synsets, chars\n",
    "\n",
    "    clean_tokens = []\n",
    "    synsets = []\n",
    "    chars = []\n",
    "    nes = []\n",
    "    for sent1, sent2 in X:\n",
    "        tok1, syn1, ch1 = preprocess_sentence(sent1)\n",
    "        tok2, syn2, ch2 = preprocess_sentence(sent2)\n",
    "        clean_tokens.append((tok1, tok2))\n",
    "        synsets.append((syn1, syn2))\n",
    "        chars.append((ch1, ch2))\n",
    "        \n",
    "    return clean_tokens, synsets, chars\n",
    "\n",
    "preprocessed_tokens_train, synsets_train, chars_train = preprocess(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: Text representation and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.metrics import jaccard_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "        cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "        return cos_sim\n",
    "\n",
    "def get_features(preprocessed_tokens, synsets, chars):\n",
    "    \n",
    "    def build_bow(sequences, method, chars=False):\n",
    "        assert method in ['bow', 'tf', 'tf_idf']\n",
    "        if not chars:\n",
    "            corpus = [' '.join(tokens1) + ' '.join(tokens2) for tokens1, tokens2 in sequences]\n",
    "            analyzer = 'word'\n",
    "        else:\n",
    "            corpus = [' '.join(list(chars1)) + ' '.join(list(chars2)) for chars1, chars2 in sequences]\n",
    "            analyzer = 'char'\n",
    "        if method == 'bow':\n",
    "            cv = CountVectorizer(binary=True, analyzer=analyzer)\n",
    "        elif method == 'tf':\n",
    "            cv = CountVectorizer(binary=False, analyzer=analyzer)\n",
    "        else:\n",
    "            cv = TfidfVectorizer()\n",
    "        cv.fit(corpus)\n",
    "        return cv\n",
    "    bow_tokens = build_bow(preprocessed_tokens, 'bow')\n",
    "    tf_tokens = build_bow(preprocessed_tokens, 'tf')\n",
    "    tf_idf_tokens = build_bow(preprocessed_tokens, 'tf_idf')\n",
    "    bow_chars = build_bow(chars, 'bow', chars=True)\n",
    "    tf_chars = build_bow(chars, 'tf', chars=True)\n",
    "    tf_idf_chars = build_bow(chars, 'tf_idf', chars=True)\n",
    "    def feature_encoder(tokens, synsets, chars,\n",
    "                        bow_tokens=bow_tokens, tf_tokens=tf_tokens, tf_idf_tokens=tf_idf_tokens,\n",
    "                        bow_chars=bow_chars, tf_chars=tf_chars, tf_idf_chars=tf_idf_chars):\n",
    "                toks1, toks2 = tokens\n",
    "                syns1, syns2 = synsets\n",
    "                chars1, chars2 = chars\n",
    "                # encoding\n",
    "                encoded_bow_tokens = np.concatenate((bow_tokens.transform(toks1).toarray(),\n",
    "                                                     bow_tokens.transform(toks2).toarray()))\n",
    "                encoded_tf_tokens = np.concatenate((tf_tokens.transform(toks1).toarray(),\n",
    "                                                     tf_tokens.transform(toks2).toarray()))\n",
    "                encoded_tf_idf_tokens = np.concatenate((tf_idf_tokens.transform(toks1).toarray(),\n",
    "                                                     tf_idf_tokens.transform(toks2).toarray()))\n",
    "                encoded_bow_chars = np.concatenate((bow_chars.transform(chars1).toarray(),\n",
    "                                                     bow_chars.transform(chars2).toarray()))\n",
    "                encoded_tf_chars = np.concatenate((tf_chars.transform(chars1).toarray(),\n",
    "                                                     tf_chars.transform(chars2).toarray()))\n",
    "                encoded_tf_idf_chars = np.concatenate((tf_idf_chars.transform(chars1).toarray(),\n",
    "                                                     tf_idf_chars.transform(chars2).toarray()))\n",
    "                                                     \n",
    "                # distances\n",
    "                edit_dist = edit_distance(chars1, chars2)\n",
    "                cos_encoded_bow_tokens = cosine_similarity(\n",
    "                    encoded_bow_tokens[:encoded_bow_tokens.shape[0]//2],\n",
    "                    encoded_bow_tokens[encoded_bow_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_tokens = cosine_similarity(\n",
    "                    encoded_tf_tokens[:encoded_tf_tokens.shape[0]//2],\n",
    "                    encoded_tf_tokens[encoded_tf_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_idf_tokens = cosine_similarity(\n",
    "                    encoded_tf_idf_tokens[:encoded_tf_idf_tokens.shape[0]//2],\n",
    "                    encoded_tf_idf_tokens[encoded_tf_idf_tokens.shape[0]//2:])\n",
    "                cos_encoded_bow_chars = cosine_similarity(\n",
    "                    encoded_bow_chars[:encoded_bow_chars.shape[0]//2],\n",
    "                    encoded_bow_chars[encoded_bow_chars.shape[0]//2:])\n",
    "                cos_encoded_tf_chars = cosine_similarity(\n",
    "                    encoded_tf_chars[:encoded_tf_chars.shape[0]//2],\n",
    "                    encoded_tf_chars[encoded_tf_chars.shape[0]//2:])\n",
    "                cos_encoded_tf_idf_chars = cosine_similarity(\n",
    "                    encoded_tf_idf_chars[:encoded_tf_idf_chars.shape[0]//2],\n",
    "                    encoded_tf_idf_chars[encoded_tf_idf_chars.shape[0]//2:])\n",
    "                \n",
    "                                                      \n",
    "                toks1_set, toks2_set = set(toks1), set(toks2)\n",
    "                if len(toks1_set) > 0 and len(toks2_set) > 0:\n",
    "                    tok_jaccard = jaccard_distance(set(toks1), set(toks2))\n",
    "                else:\n",
    "                    tok_jaccard = 1\n",
    "                if len(syns1) > 0 and len(syns2) > 0:\n",
    "                    syn_jaccard = jaccard_distance(syns1, syns2)\n",
    "                else:\n",
    "                    syn_jaccard = 1\n",
    "                if len(chars1) > 0 and len(chars2) > 0:\n",
    "                    chars_jaccard = jaccard_distance(set(list(chars1)), set(list(chars2)))\n",
    "                else:\n",
    "                    chars_jaccard = 1\n",
    "                return dict(encoded_bow_tokens=encoded_bow_tokens, encoded_tf_tokens=encoded_tf_tokens,\n",
    "                           encoded_tf_idf_tokens=encoded_tf_idf_tokens, encoded_bow_chars=encoded_bow_chars,\n",
    "                           encoded_tf_chars=encoded_tf_chars, encoded_tf_idf_chars=encoded_tf_idf_chars,\n",
    "                           edit_dist=edit_dist, cos_encoded_bow_tokens=cos_encoded_bow_tokens,\n",
    "                           cos_encoded_tf_idf_tokens=cos_encoded_tf_idf_tokens,\n",
    "                           cos_encoded_bow_chars=cos_encoded_bow_chars, cos_encoded_tf_chars=cos_encoded_tf_chars,\n",
    "                           cos_encoded_tf_idf_chars=cos_encoded_tf_idf_chars, tok_jaccard=tok_jaccard,\n",
    "                           syn_jaccard=syn_jaccard, chars_jaccard=chars_jaccard)                             \n",
    "                \n",
    "    features = []\n",
    "    for tokens, synsets, char in zip(preprocessed_tokens, synsets, chars):\n",
    "        feat = feature_encoder(tokens, synsets, chars)\n",
    "        features.append(feat)\n",
    "        \n",
    "    return features, feature_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
