{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT Final Project: Semantic Textual Similarity Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jordi Armengol, Joan Llop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This projects consists of a Semantic Textual Similarity exercise from the SemEval (Semantic Evaluation Exercises). Specifically, we will use the dataset from SemEval 2012, and compare ourselves with the participants of that workshop. Basically, we will use the classical NLP and machine learning techniques that we have used, mostly with lexical features, but also experimenting with syntactic information. Additionally, we will provide some transfer learning (deep learning) alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "We start by downloading the SemEval 2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-12 04:43:10--  https://gebakx.github.io/ihlt/sts/resources/train.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.111.153, 185.199.110.153, 185.199.109.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125822 (123K) [application/octet-stream]\n",
      "Saving to: ‘data/train.tgz.16’\n",
      "\n",
      "train.tgz.16        100%[===================>] 122,87K  --.-KB/s    in 0,09s   \n",
      "\n",
      "2019-12-12 04:43:10 (1,27 MB/s) - ‘data/train.tgz.16’ saved [125822/125822]\n",
      "\n",
      "--2019-12-12 04:43:10--  https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz\n",
      "Resolving gebakx.github.io (gebakx.github.io)... 185.199.111.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to gebakx.github.io (gebakx.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118094 (115K) [application/octet-stream]\n",
      "Saving to: ‘data/test-gold.tgz.16’\n",
      "\n",
      "test-gold.tgz.16    100%[===================>] 115,33K  --.-KB/s    in 0,04s   \n",
      "\n",
      "2019-12-12 04:43:10 (2,75 MB/s) - ‘data/test-gold.tgz.16’ saved [118094/118094]\n",
      "\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project/data\n",
      "train/\n",
      "train/00-readme.txt\n",
      "train/STS.output.MSRpar.txt\n",
      "train/STS.input.SMTeuroparl.txt\n",
      "train/STS.input.MSRpar.txt\n",
      "train/STS.gs.MSRpar.txt\n",
      "train/STS.input.MSRvid.txt\n",
      "train/STS.gs.MSRvid.txt\n",
      "train/correlation.pl\n",
      "train/STS.gs.SMTeuroparl.txt\n",
      "test-gold/\n",
      "test-gold/STS.input.MSRpar.txt\n",
      "test-gold/STS.gs.MSRpar.txt\n",
      "test-gold/STS.input.MSRvid.txt\n",
      "test-gold/STS.gs.MSRvid.txt\n",
      "test-gold/STS.input.SMTeuroparl.txt\n",
      "test-gold/STS.gs.SMTeuroparl.txt\n",
      "test-gold/STS.input.surprise.SMTnews.txt\n",
      "test-gold/STS.gs.surprise.SMTnews.txt\n",
      "test-gold/STS.input.surprise.OnWN.txt\n",
      "test-gold/STS.gs.surprise.OnWN.txt\n",
      "test-gold/STS.gs.ALL.txt\n",
      "test-gold/00-readme.txt\n",
      "/home/nhikia/Documents/AI/IHLT/IHLT-MAI/lab/project\n",
      "CPU times: user 92.1 ms, sys: 116 ms, total: 208 ms\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus assembly\n",
    "Train and test sets. The test set will not be used for learning or model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 ms, sys: 0 ns, total: 26.7 ms\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative 1: Classical NLP and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary experiments for preprocessing and feature selection issues\n",
    "Before applying the final preprocessing and feature extraction, we performed a series of previous experiments for gaining some insights on the importance of certain preprocessing and feature extraction decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General class/interface\n",
    "There are so many things to try that we will start by defining a general class/interface for all the models that we will use. This class has, among other features, a cross-validation method (obviously, using only the train set, not the test set). The models will inherit the methods of this class and, essentially, they will only have to implement the a method from extracting the features from data. Notice that the Model class is very generic, but complete. Specifically, it has a method for **cross-validation**, which takes the train set and divided it in different folds (K-Fold Cross-Validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,  x, y, regr=LinearRegression(),):\n",
    "        self.regr = regr\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.name = None\n",
    "        self.description = None\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        pickle.dump(self, open(self.name + '.model', 'wb').write())\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, name):\n",
    "        return pickle.load(open(self.name + '.model', 'rb').read())\n",
    "        \n",
    "    \n",
    "    def _extract_features(self, x):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.regr.fit(self.x_features, self.y)\n",
    "    \n",
    "    \n",
    "    def predict(self, new_x):\n",
    "        new_x_features = self._extract_features(new_x)\n",
    "        return self.regr.predict(new_x_features)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, true_labels, predicted_labels):\n",
    "        pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "        return pearson, p_value\n",
    "    \n",
    "    \n",
    "    def cross_validate(self, n_folds=5, seed=1):\n",
    "        assert self.x_features is not None\n",
    "        kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "        average_pearson = 0\n",
    "        for train_index, val_index in kf.split(self.x_features):\n",
    "            X_train, X_val = self.x_features[train_index], self.x_features[val_index]\n",
    "            y_train, y_val = self.y[train_index], self.y[val_index]\n",
    "            self.regr.fit(X_train, y_train)\n",
    "            predicted_labels = self.regr.predict(X_val)\n",
    "            pearson, _ = self.evaluate(y_val, predicted_labels)\n",
    "            average_pearson += abs(pearson)\n",
    "        return average_pearson/n_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance of some basic features\n",
    "In this initial approach, we use the jaccard distance to compute the similarity between different classical features studied in class: lemmas, most common synsets,  and part-of-Speech, length of word and whether the word is a digit. We experimented with different preprocessing strategies as well, like filtering stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744670790260961"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class JaccardModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'JaccardModel'\n",
    "        self.description = 'Jaccard distance, some basic features'\n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "        super().__init__(*kwargs)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(sent):\n",
    "            preprocessed = \"\"\n",
    "            for char in sent:\n",
    "                if char.isdigit():\n",
    "                    preprocessed += char\n",
    "                elif char.isalpha():\n",
    "                    preprocessed += char.lower()\n",
    "                elif char == ' ':\n",
    "                    preprocessed += char\n",
    "\n",
    "            return str(preprocessed)\n",
    "\n",
    "        x = [[preprocess(sent1), preprocess(sent2)] for sent1, sent2 in x]\n",
    "        \n",
    "        def lemmatize(token, pos):\n",
    "            if pos in {'N','V'}:\n",
    "                return wnl.lemmatize(token.lower(), pos.lower())\n",
    "            return token.lower()\n",
    "\n",
    "\n",
    "        def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "            mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "            if nltk_pos in mapping:\n",
    "                return mapping[nltk_pos]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def get_synsets(sent):\n",
    "            saved_synsets = []\n",
    "            tokens = word_tokenize(sent)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            for token, pos, lemma in zip(tokens, pos_tags, lemmas):\n",
    "                wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "                if wordnet_pos is not None:\n",
    "                    word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "                    if len(word_synsets) > 0:\n",
    "                        most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                        saved_synsets.append(most_freq_synset)\n",
    "            return saved_synsets\n",
    "\n",
    "\n",
    "        def get_features_from_word(sent, index, pos):\n",
    "            word = sent[index]\n",
    "            features = []\n",
    "            features.append(str(pos)) # Part-of-Speech                   \n",
    "            features.append(str(len(word))) # length of word\n",
    "            features.append(str(index==0)) # beggining of a sentence\n",
    "            features.append(str(index==len(sent)-1)) # end of sentence\n",
    "            features.append(str(word.isdigit())) # is a digit\n",
    "            return features\n",
    "\n",
    "        def sent2features(sent):\n",
    "            features = []\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.stop_words]\n",
    "            features.append(tokens)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            features.append(pos_tags)\n",
    "            lemmas = [lemmatize(t, pos) for t, pos in zip(tokens, pos_tags)]\n",
    "            features.append(lemmas)\n",
    "            synsets = get_synsets(sent)\n",
    "            if len(synsets) > 0:\n",
    "                features.append(synsets)\n",
    "            else:\n",
    "                features.append([0])\n",
    "            temp_f = []\n",
    "            for i in range(len(tokens)):\n",
    "                temp_f += get_features_from_word(tokens, i, pos_tags[i])\n",
    "            features.append(temp_f)\n",
    "\n",
    "            return features\n",
    "        \n",
    "        def distance(features1, features2, sent1, sent2, index):\n",
    "            distances = []\n",
    "            init = True\n",
    "            for f1, f2 in zip(features1, features2):\n",
    "                distances.append(jaccard_distance(set(f1), set(f2)))\n",
    "            return distances\n",
    "\n",
    "        \n",
    "        pairs_of_features = [(sent2features(sent1), sent2features(sent2)) for sent1, sent2 in x]\n",
    "        distances = np.array([distance(features1, features2, sent1, sent2, index) for index, ((features1, features2), (sent1, sent2)) in enumerate(zip(pairs_of_features, x))])\n",
    "        return distances\n",
    "    \n",
    "    \n",
    "jaccardModel = JaccardModel(train_data, train_labels)\n",
    "jaccardModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using words\n",
    "Cosine similarity of the union of the sets of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5968105783849428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "from string import punctuation\n",
    "\n",
    "class BagWordsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagWords'\n",
    "        self.description = 'We get a set of words of both setences, \\\n",
    "        calculate the union, get the count of each word of the union for each sentence \\\n",
    "        and get the cosine similarity'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char\n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_bag_of_words(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            union = np.union1d(tokens1, tokens2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for token in tokens1:\n",
    "                count1[np.where(union == token)] += 1\n",
    "            for token in tokens2:\n",
    "                count2[np.where(union == token)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_words(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "bagWordsModel = BagWordsModel(train_data, train_labels)\n",
    "bagWordsModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using of lemmas\n",
    "Same as before, but with lemmas. Recall that we are still using a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076451713539923"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "\n",
    "class BagLemmasModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagLemmasModel'\n",
    "        self.description = 'We get the bag of lemmas of both setences, calculate the union,\\\n",
    "        get the count of each word of the union for each sentence and get the cosine distance \\\n",
    "        between them'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char \n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def lemmatize(token):\n",
    "            return self.wnl.lemmatize(token)\n",
    "        \n",
    "        \n",
    "        def get_bag_of_lemmas(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            lemmas1 = [lemmatize(word) for word in tokens1]\n",
    "            lemmas2 = [lemmatize(word) for word in tokens2]\n",
    "            return get_cosine_of_frequencies(lemmas1, lemmas2)\n",
    "                \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_lemmas(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "bagLemmasModel = BagLemmasModel(train_data, train_labels)\n",
    "bagLemmasModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stems\n",
    "Similar to our previous approach, but in this case we decided to experiment with stems instead of lemmas. Remarkably, stems performed better than lemmas, which is counter-intuitive if we take into account the theoretical foundations of lemmas (instead of heuristic-driven, lemmas are the canonical form of a word that we find in a dictionary, so we thought that they should have worked better; it was not the case). Recall that we are still using a linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6488292812545408"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "\n",
    "class BagStemsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BagStemsModel'\n",
    "        self.description = 'We get the bag of stems of both setences, calculate the union, \\\n",
    "        get the count of each word of the union for each sentence and get the cosine distance between them'\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def preprocess(data):\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                sent1 = sent1.lower()\n",
    "                sent2 = sent2.lower()\n",
    "                processed_sent1 = \"\"\n",
    "                processed_sent2 = \"\"\n",
    "                for char in sent1: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent1 += char\n",
    "                for char in sent2: \n",
    "                    if char.isalnum() or char == ' ': \n",
    "                        processed_sent2 += char\n",
    "                processed_data.append([processed_sent1, processed_sent2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def stemmatize(token):\n",
    "            return self.stemmer.stem(token)\n",
    "        \n",
    "        \n",
    "        def get_bag_of_stems(sent1, sent2):\n",
    "            tokens1 = [word for word in word_tokenize(sent1) if not word in self.stop_words]\n",
    "            tokens2 = [word for word in word_tokenize(sent2) if not word in self.stop_words]\n",
    "            stems1 = [stemmatize(word) for word in tokens1]\n",
    "            stems2 = [stemmatize(word) for word in tokens2]\n",
    "            return get_cosine_of_frequencies(stems1, stems2)                \n",
    "            \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BoW = np.array([get_bag_of_stems(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BoW\n",
    "\n",
    "bagStemsModel = BagStemsModel(train_data, train_labels)\n",
    "bagStemsModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams vector representation\n",
    "We compute the cosine similarity of a frequency bigram representation of the sentences. Cosine similarity is the canonical measure for comparing vectors. We decided to use a bigram representations because bigrams provide slightly more context (local context of size = 1, apart from the token itself) than just using unigrams. In the literature, some approaches have studied the use of n-grams for textual similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6637117020982714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "class BiGramsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'BiGramsModel'\n",
    "        self.description = 'BiGramsModel'\n",
    "        self.allowed = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                                 '(', ')', '.', ' ', '!', '?', 'a', 'b', 'c', 'd', 'e', \\\n",
    "                                 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "                                 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
    "        self.not_allowed_words = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        def get_allowed_characters(sent):\n",
    "            sent = sent.lower()\n",
    "            tokens = [word for word in word_tokenize(sent) if not word in self.not_allowed_words]\n",
    "            stems = [self.stemmer.stem(word) for word in tokens]\n",
    "            new_sent = \" \".join(stems)\n",
    "            return \"\".join([char for char in new_sent if char in self.allowed])\n",
    "        \n",
    "        \n",
    "        def preprocess(data):\n",
    "            word_freq = {}\n",
    "            processed_data = []\n",
    "            for sent1, sent2 in data:\n",
    "                allowed_chars1 = get_allowed_characters(sent1)\n",
    "                allowed_chars2 = get_allowed_characters(sent2)\n",
    "                processed_data.append([allowed_chars1, allowed_chars2])\n",
    "            return processed_data\n",
    "        \n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_vector_bigrams(sent):\n",
    "            count = np.zeros(len(self.allowed)**2) # number of possible bigrams\n",
    "            for i in range(len(sent)-1):\n",
    "                idx_f = int(np.where(self.allowed == sent[i])[0])\n",
    "                idx_s = int(np.where(self.allowed == sent[i+1])[0])\n",
    "                count[idx_f*len(self.allowed) + idx_s] += 1\n",
    "            return count\n",
    "            \n",
    "                \n",
    "        def get_cos_of_bigrams(sent1, sent2):\n",
    "            return [cosine_similarity(get_vector_bigrams(sent1), get_vector_bigrams(sent2))]                \n",
    "            \n",
    "            \n",
    "        preprocessed_x = preprocess(x)\n",
    "        BiGms = np.array([get_cos_of_bigrams(sent1, sent2) for sent1, sent2 in preprocessed_x])\n",
    "        return BiGms\n",
    "\n",
    "biGramsModel = BiGramsModel(train_data, train_labels)\n",
    "biGramsModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets\n",
    "We apply the same algorithm as in the case of words and lemmas, but with synsets and lemmas. Synsets have the theoretical adva. In practice, they do not work very well because most of the tokens do not have an assigned synset, and we do not perform word sense disambiguation (we retrieve the most sfrequent synset, not the relevant synset depending on the context, which would require a specific NLP application).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4602791132967097"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class SynsetsLemmasModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'SynsetsLemmasModel'\n",
    "        self.description = 'SynsetsLemmasModel'\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "            return cos_sim\n",
    "        \n",
    "        \n",
    "        def get_cosine_of_frequencies(vec1, vec2):\n",
    "            union = np.union1d(vec1, vec2)\n",
    "            count1 = np.zeros(len(union))\n",
    "            count2 = np.zeros(len(union))\n",
    "            for elem in vec1:\n",
    "                count1[np.where(union == elem)] += 1\n",
    "            for elem in vec2:\n",
    "                count2[np.where(union == elem)] += 1\n",
    "            return [cosine_similarity(count1, count2)]\n",
    "        \n",
    "        \n",
    "        def get_union_SynLemmas(word):\n",
    "            synsets = wordnet.synsets(word)\n",
    "            if len(synsets) > 0:\n",
    "                return reduce(np.union1d, ([str(lemma.name()) for lemma in synsets[0].lemmas()]))\n",
    "            return np.empty(0)\n",
    "        \n",
    "        \n",
    "        def get_sentence_union_synsets(tokens):\n",
    "            return reduce(np.union1d, ([get_union_SynLemmas(word) for word in tokens]))\n",
    "        \n",
    "        \n",
    "        def get_union_synsets(sent1, sent2):\n",
    "            synsets1 = get_sentence_union_synsets(word_tokenize(sent1))\n",
    "            synsets2 = get_sentence_union_synsets(word_tokenize(sent2))\n",
    "            return get_cosine_of_frequencies(synsets1, synsets2)\n",
    "            \n",
    "        \n",
    "        BoW = np.array([get_union_synsets(sent1, sent2) for sent1, sent2 in x])\n",
    "        return BoW\n",
    "\n",
    "synsetsLemmasModel = SynsetsLemmasModel(train_data, train_labels)\n",
    "synsetsLemmasModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun phrases and verbs comparison\n",
    "Spacy is a NLP toolkit similar to Stanford CoreNLP, but without requiring a Java server. We will use it to retrieve noun phrases and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python3 -m pip install spacy --user\n",
    "# !python3 -m spacy download en_core_web_sm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4953711707706738"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "\n",
    "\n",
    "class NounPhraseModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'NounPhraseModel'\n",
    "        self.description = 'NounPhraseModel'\n",
    "        # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.not_allowed_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        \n",
    "        \n",
    "        def get_distance(v1, v2):\n",
    "            if not v1 or not v2: return 0\n",
    "            count_pairs = 0\n",
    "            for sent1 in v1:\n",
    "                for sent2 in v2:\n",
    "                    if sent1 in sent2 or sent2 in sent1:\n",
    "                        count_pairs += 1\n",
    "            return count_pairs/max(len(v1), len(v2))\n",
    "        \n",
    "        \n",
    "        def get_noun_phrases(sent):\n",
    "            doc = self.nlp(sent)\n",
    "            noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "            return noun_phrases\n",
    "        \n",
    "        \n",
    "        def get_verbs(sent):\n",
    "            doc = self.nlp(sent)\n",
    "            verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "            return verbs\n",
    "        \n",
    "        \n",
    "        def get_name_entities(sent):\n",
    "            doc = self.nlp(sent)\n",
    "            ne = [entity.text for entity in doc.ents]\n",
    "            return ne\n",
    "        \n",
    "        def get_similarity(sent1, sent2):\n",
    "            noun_phrases_1 = get_noun_phrases(str(sent1))\n",
    "            noun_phrases_2 = get_noun_phrases(str(sent2))\n",
    "            verbs1 = get_verbs(str(sent1))\n",
    "            verbs2 = get_verbs(str(sent2))\n",
    "            ne1 = get_name_entities(str(sent1))\n",
    "            ne2 = get_name_entities(str(sent2))\n",
    "            dist_nouns = get_distance(noun_phrases_1, noun_phrases_2)\n",
    "            dist_verbs = get_distance(verbs1, verbs2)\n",
    "            dist_ne = get_distance(ne1, ne2)\n",
    "            return [dist_verbs, dist_nouns, dist_ne]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sm = np.array([get_similarity(sent1, sent2) for sent1, sent2 in x])\n",
    "        return sm\n",
    "\n",
    "nounPhraseModel = NounPhraseModel(train_data, train_labels)\n",
    "nounPhraseModel.cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary conclusions**\n",
    "Let us do a summary of what have been experimented with. So far, we have tested many features, still with linear regression. There are many tests that we do not include here, for brevity (eg. the same models but without removing stop words, which gave a worse result). \n",
    "\n",
    "Now, are going to aggregate some features into a single model. The assumption here is that their contribution will be additive or at least will not damage the result. As preliminary research, we conducted some experiments on some of the combinations, but for brevity and computational constraints is not possible to test all the combinations. Instead, we will include the features that seemed the most promising. In the case of the NounPhraseModel, for instance, we will not include it, because it is way slower than the other ones and the obtained results were mediocre. We will ignore synsets as well, for not giving very good results.\n",
    "\n",
    "On the other hand, we will add some other features that we experimented with but did not include for brevity: edit distance (it won't correlate with semantic similarity, but non-linear models can realize that a very short edit distance means that the sentence is almost literally equal). \n",
    "\n",
    "Also, we are going to consider trigrams, in addition to bigrams, and different ways of representing the frequencies (a simple boolean bag of words, a frequency distribution or Inverse Document Frequency).\n",
    "\n",
    "Notice that we use stems instead of lemmas because they empirically behaved better in the preliminary experiments. We replace words by stems for reducing the vocabulary. We experimented with replacing named entities by their tags as well, but it did not obtain promising results when cross-validating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 168 ms, total: 50.6 s\n",
      "Wall time: 50.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stopwords_set = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(X):\n",
    "    def is_number(s):\n",
    "        try:\n",
    "            x = float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def characters_not_punct(token):\n",
    "        for c in token:\n",
    "            if c in punctuation:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def lemmatize(token, pos):\n",
    "        if pos in {'N','V'}:\n",
    "            return wnl.lemmatize(token.lower(), pos.lower())\n",
    "        return token.lower()\n",
    "    \n",
    "    def stemmatize(token):\n",
    "            return stemmer.stem(token)\n",
    "\n",
    "    def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "        mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "        if nltk_pos in mapping:\n",
    "            return mapping[nltk_pos]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_synset(lemma, pos):\n",
    "        wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "        if wordnet_pos is not None:\n",
    "            word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "            if len(word_synsets) > 0:\n",
    "                most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                return most_freq_synset\n",
    "        return None\n",
    "    \n",
    "    def get_nes(pos_tags):\n",
    "        nes = ne_chunk(pos_tags, binary=False)\n",
    "        nes_map = []\n",
    "        for tree_element in nes:\n",
    "            if type(tree_element) == nltk.tree.Tree:\n",
    "                for element in tree_element:\n",
    "                        nes_map.append(tree_element.label())\n",
    "            else:\n",
    "                nes_map.append(None)\n",
    "        return nes_map\n",
    "                \n",
    "    def preprocess_sentence(sent):\n",
    "        tokens = word_tokenize(sent)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        clean_tokens = []\n",
    "        synsets = set([])\n",
    "        chars = ''.join([c for c in sent if c not in punctuation + ' '])\n",
    "        nes_map = get_nes(pos_tags)\n",
    "        for token, pos, ne in zip(tokens, pos_tags, nes_map):\n",
    "#             token = token.lower()\n",
    "#             if ne is not None:\n",
    "#                 token = ne\n",
    "#                 clean_tokens.append(token)\n",
    "#             lemma = lemmatize(token, pos)\n",
    "#             clean_tokens.append(lemma)\n",
    "            stem = stemmatize(token)\n",
    "            clean_tokens.append(stem)\n",
    "#             synset = get_synset(lemma, pos)\n",
    "#             if synset is not None:\n",
    "#                 synsets.add(synset)\n",
    "        return clean_tokens, synsets, chars\n",
    "\n",
    "    clean_tokens = []\n",
    "    synsets = []\n",
    "    chars = []\n",
    "    nes = []\n",
    "    for sent1, sent2 in X:\n",
    "        tok1, syn1, ch1 = preprocess_sentence(sent1)\n",
    "        tok2, syn2, ch2 = preprocess_sentence(sent2)\n",
    "        clean_tokens.append((tok1, tok2))\n",
    "        synsets.append((syn1, syn2))\n",
    "        chars.append((ch1, ch2))\n",
    "        \n",
    "    return clean_tokens, synsets, chars\n",
    "\n",
    "preprocessed_tokens_train, synsets_train, chars_train = preprocess(train_data)\n",
    "preprocessed_tokens_test, synsets_test, chars_test = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: Text representation and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.9 s, sys: 0 ns, total: 47.9 s\n",
      "Wall time: 47.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.metrics import jaccard_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "def overlap(A, B):\n",
    "    return len(A.intersection(B))/(min(len(A), len(B)))\n",
    "\n",
    "def set_kernel_distance(a, b):\n",
    "    return np.exp2(np.dot(a, b.T))\n",
    "\n",
    "def get_features(preprocessed_tokens, synsets, chars):\n",
    "    \n",
    "    def build_bow(sequences, method, chars=False, ngram_range=(1,1)):\n",
    "        assert method in ['bow', 'tf', 'tf_idf']\n",
    "        if not chars:\n",
    "            corpus = [' '.join(tokens1) + ' '.join(tokens2) for tokens1, tokens2 in sequences]\n",
    "            analyzer = 'word'\n",
    "        else:\n",
    "            corpus = [' '.join(list(chars1)) + ' '.join(list(chars2)) for chars1, chars2 in sequences]\n",
    "            analyzer = 'char'\n",
    "        if method == 'bow':\n",
    "            cv = CountVectorizer(binary=True, analyzer=analyzer, ngram_range=ngram_range)\n",
    "        elif method == 'tf':\n",
    "            cv = CountVectorizer(binary=False, analyzer=analyzer, ngram_range=ngram_range)\n",
    "        else:\n",
    "            cv = TfidfVectorizer()\n",
    "        cv.fit(corpus)\n",
    "        return cv\n",
    "    bow_tokens = build_bow(preprocessed_tokens, 'bow')\n",
    "    tf_tokens = build_bow(preprocessed_tokens, 'tf')\n",
    "    tf_idf_tokens = build_bow(preprocessed_tokens, 'tf_idf')\n",
    "    bow_chars = build_bow(chars, 'bow', chars=True)\n",
    "    tf_chars = build_bow(chars, 'tf', chars=True)\n",
    "    tf_idf_chars = build_bow(chars, 'tf_idf', chars=True)\n",
    "    bow_bigrams = build_bow(preprocessed_tokens, 'bow', (2, 2))\n",
    "    tf_bigrams = build_bow(preprocessed_tokens, 'tf', (2, 2))\n",
    "    bow_trigrams = build_bow(preprocessed_tokens, 'bow', (3, 3))\n",
    "    tf_trigrams = build_bow(preprocessed_tokens, 'tf', (3, 3))\n",
    "    def feature_encoder(tokens, synsets, chars,\n",
    "                        bow_tokens=bow_tokens, tf_tokens=tf_tokens, tf_idf_tokens=tf_idf_tokens,\n",
    "                        bow_chars=bow_chars, tf_chars=tf_chars, tf_idf_chars=tf_idf_chars,\n",
    "                        bow_bigrams=bow_bigrams,tf_bigrams=tf_bigrams,\n",
    "                        bow_trigrams=bow_trigrams, tf_trigrams=tf_trigrams):\n",
    "                toks1, toks2 = tokens\n",
    "                syns1, syns2 = synsets\n",
    "                chars1, chars2 = chars\n",
    "                \n",
    "                # encoding\n",
    "                encoded_bow_tokens = np.concatenate((bow_tokens.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     bow_tokens.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_tokens = np.concatenate((tf_tokens.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_tokens.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_idf_tokens = np.concatenate((tf_idf_tokens.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_idf_tokens.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_bow_chars = np.concatenate((bow_chars.transform([' '.join(chars1)]).toarray(),\n",
    "                                                     bow_chars.transform([' '.join(chars2)]).toarray())).flatten()\n",
    "                encoded_tf_chars = np.concatenate((tf_chars.transform([' '.join(chars1)]).toarray(),\n",
    "                                                     tf_chars.transform([' '.join(chars2)]).toarray())).flatten()\n",
    "                encoded_tf_idf_chars = np.concatenate((tf_idf_chars.transform([' '.join(chars1)]).toarray(),\n",
    "                                                     tf_idf_chars.transform([' '.join(chars2)]).toarray())).flatten()\n",
    "                encoded_bow_bigrams = np.concatenate((bow_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     bow_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_bigrams = np.concatenate((tf_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_bow_trigrams = np.concatenate((bow_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     bow_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_trigrams = np.concatenate((tf_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                \n",
    "                                                     \n",
    "                # distances\n",
    "                edit_dist = edit_distance(chars1, chars2)\n",
    "                set_dist_encoded_bow_tokens = set_kernel_distance(\n",
    "                    encoded_bow_tokens[:encoded_bow_tokens.shape[0]//2],\n",
    "                    encoded_bow_tokens[encoded_bow_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_tokens = cosine_similarity(\n",
    "                    encoded_tf_tokens[:encoded_tf_tokens.shape[0]//2],\n",
    "                    encoded_tf_tokens[encoded_tf_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_idf_tokens = cosine_similarity(\n",
    "                    encoded_tf_idf_tokens[:encoded_tf_idf_tokens.shape[0]//2],\n",
    "                    encoded_tf_idf_tokens[encoded_tf_idf_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_chars = cosine_similarity(\n",
    "                    encoded_tf_chars[:encoded_tf_chars.shape[0]//2],\n",
    "                    encoded_tf_chars[encoded_tf_chars.shape[0]//2:])\n",
    "                set_dist_encoded_bow_bigrams = set_kernel_distance(\n",
    "                    encoded_bow_bigrams[:encoded_bow_bigrams.shape[0]//2],\n",
    "                    encoded_bow_bigrams[encoded_bow_bigrams.shape[0]//2:])\n",
    "                cos_encoded_tf_bigrams = cosine_similarity(\n",
    "                    encoded_tf_bigrams[:encoded_tf_bigrams.shape[0]//2],\n",
    "                    encoded_tf_bigrams[encoded_tf_bigrams.shape[0]//2:])\n",
    "                set_dist_encoded_bow_trigrams = set_kernel_distance(\n",
    "                    encoded_bow_trigrams[:encoded_bow_trigrams.shape[0]//2],\n",
    "                    encoded_bow_trigrams[encoded_bow_trigrams.shape[0]//2:])\n",
    "                cos_encoded_tf_trigrams = cosine_similarity(\n",
    "                    encoded_tf_trigrams[:encoded_tf_trigrams.shape[0]//2],\n",
    "                    encoded_tf_trigrams[encoded_tf_trigrams.shape[0]//2:])\n",
    "                \n",
    "                                                      \n",
    "                toks1_set, toks2_set = set(toks1), set(toks2)\n",
    "                if len(toks1_set) > 0 and len(toks2_set) > 0:\n",
    "                    tok_jaccard = jaccard_distance(set(toks1), set(toks2))\n",
    "                    tok_overlap = overlap(set(toks1), set(toks2))\n",
    "                else:\n",
    "                    tok_jaccard = 1\n",
    "                    tok_overlap = 1\n",
    "                if len(chars1) > 0 and len(chars2) > 0:\n",
    "                    chars_jaccard = jaccard_distance(set(list(chars1)), set(list(chars2)))\n",
    "                    chars_overlap = overlap(set(list(chars1)), set(list(chars2)))\n",
    "                else:\n",
    "                    chars_jaccard = 1\n",
    "                    chars_overlap = 1\n",
    "                return OrderedDict(\n",
    "                           edit_dist=edit_dist, \n",
    "                           set_dist_encoded_bow_tokens=set_dist_encoded_bow_tokens,\n",
    "                           cos_encoded_tf_idf_tokens=cos_encoded_tf_idf_tokens,\n",
    "                           cos_encoded_tf_chars=cos_encoded_tf_chars,\n",
    "                           set_dist_encoded_bow_bigrams=set_dist_encoded_bow_bigrams,\n",
    "                           cos_encoded_tf_bigrams=cos_encoded_tf_bigrams,\n",
    "                           set_dist_encoded_bow_trigrams=set_dist_encoded_bow_trigrams,\n",
    "                           cos_encoded_tf_trigrams=cos_encoded_tf_trigrams,\n",
    "                           tok_jaccard=tok_jaccard,\n",
    "                           tok_overlap=tok_overlap,    \n",
    "                           chars_jaccard=chars_jaccard,\n",
    "                           chars_overlap=chars_overlap\n",
    "                            )                      \n",
    "                \n",
    "    features = []\n",
    "    for toks, syns, ch in zip(preprocessed_tokens, synsets, chars):\n",
    "        feat = feature_encoder(toks, syns, ch)\n",
    "        features.append(feat)\n",
    "    mat_features = pd.DataFrame(features).values\n",
    "    std_scaler = StandardScaler().fit(mat_features)\n",
    "    def scale(features_row, scaler=std_scaler):\n",
    "        scaled = scaler.transform(pd.DataFrame([features_row]).values)\n",
    "        scaled_features_row = OrderedDict(zip(features_row.keys(), scaled[0]))\n",
    "        return scaled_features_row\n",
    "    scaled_feat = []\n",
    "    for feat in features:\n",
    "        scaled_feat.append(scale(feat))\n",
    "    return scaled_feat, feature_encoder, scale\n",
    "\n",
    "features_train, feature_encoder, scaler = get_features(preprocessed_tokens_train, synsets_train, chars_train)\n",
    "features_test = []\n",
    "for toks, syns, ch in zip(preprocessed_tokens_test, synsets_test, chars_test):\n",
    "    feat = feature_encoder(toks, syns, ch)\n",
    "    features_test.append(feat)\n",
    "scaled_feat = []\n",
    "for feat in features_test:\n",
    "    scaled_feat.append(scaler(feat))\n",
    "features_test = []\n",
    "for feat in scaled_feat:\n",
    "    new_d = {}\n",
    "    for key in feat:\n",
    "        if not math.isnan(feat[key]):\n",
    "            new_d[key] = feat[key]\n",
    "        else:\n",
    "            new_d[key] = 1\n",
    "    features_test.append(new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and final evaluation\n",
    "\n",
    "We experimented with a number of regression algorithms from the SKLearn library. The most promising ones were usually the ones based on boosting, a kind of ensemble learning algorithms that are able to improve weak learners by using multiple models and giving more weight to the samples that caused errors.\n",
    "\n",
    "We do not include all the algorithms. For instance, we performed a grid-search for trying different hyperparameters of an MLP, even adding more than one hidden layer, but apart from being costly, it underperformed other alternatives.\n",
    "\n",
    "We optimized some of the hyperparameters of the learning algorithms and we leave the best that we found as default.\n",
    "\n",
    "Finally, we performed the mean of the outputs of the best three models we found (basic ensemble voting). Our model obtained a Pearson correlation of  0.752 in the test set, we would have been the 11th best participant in the original task. Notice that this result is without using any form of data augmentation or transfer learning, so we are satisfied with the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6973790487302391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "class SVMLinModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'SVMLinModel'\n",
    "        self.description = 'SVMLinModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = SVR(kernel='linear', C=2)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "SVMLinModel = SVMLinModel(pd.DataFrame(features_train).values, train_labels)\n",
    "print(SVMLinModel.cross_validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7237113149734203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "class SVMModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'SVMModel'\n",
    "        self.description = 'SVMModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = SVR(C=1.4, epsilon=0.0, gamma='scale')\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "sVMModel = SVMModel(pd.DataFrame(features_train).values, train_labels)\n",
    "print(sVMModel.cross_validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7127200695436545\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAEWCAYAAADywzSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class RandomForestModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'RandomForestModel'\n",
    "        self.description = 'RandomForestModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=1)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "randomForestModel = RandomForestModel(pd.DataFrame(features_train).values, train_labels)\n",
    "print(randomForestModel.cross_validate())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "importances = randomForestModel.regr.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "names = list(features_train[0].keys())\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194348543631532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "class GradientBoostingModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'GradientBoostingModel'\n",
    "        self.description = 'GradientBoostingModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = GradientBoostingRegressor(alpha=0.95,\n",
    "                                n_estimators=56, max_depth=4,\n",
    "                                learning_rate=.101, min_samples_leaf=9, max_features=11,\n",
    "                                min_samples_split=30, random_state=40)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "gradientBoostingModel = GradientBoostingModel(pd.DataFrame(features_train).values, train_labels)\n",
    "print(gradientBoostingModel.cross_validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.7527692441079573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class SVMModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'SVMModel'\n",
    "        self.description = 'SVMModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = SVR(C=1.4, epsilon=0.0, gamma='scale')\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GradientBoostingModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'GradientBoostingModel'\n",
    "        self.description = 'GradientBoostingModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = GradientBoostingRegressor(alpha=0.95,\n",
    "                                n_estimators=56, max_depth=4,\n",
    "                                learning_rate=.101, min_samples_leaf=9, max_features=11,\n",
    "                                min_samples_split=30, random_state=40)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomForestModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'RandomForestModel'\n",
    "        self.description = 'RandomForestModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=1)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "gradientBoostingModel = GradientBoostingModel(pd.DataFrame(features_train).values, train_labels)\n",
    "sVMModel = SVMModel(pd.DataFrame(features_train).values, train_labels)\n",
    "randomForestModel = RandomForestModel(pd.DataFrame(features_train).values, train_labels)\n",
    "\n",
    "\n",
    "randomForestModel.fit(pd.DataFrame(features_train).values, train_labels)\n",
    "sVMModel.fit(pd.DataFrame(features_train).values, train_labels)\n",
    "gradientBoostingModel.fit(pd.DataFrame(features_train).values, train_labels)\n",
    "\n",
    "\n",
    "pred1 = sVMModel.predict(pd.DataFrame(features_test).values)\n",
    "pred2 = gradientBoostingModel.predict(pd.DataFrame(features_test).values)\n",
    "pred3 = randomForestModel.predict(pd.DataFrame(features_test).values)\n",
    "\n",
    "\n",
    "pred = (pred1+pred2+pred3)\n",
    "print('Test:', gradientBoostingModel.evaluate(test_labels, pred)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We have built a semantic textual similarity system that is able to effectively detect paraphrases (and distinguish them from unrelated sentences), as shown by the fact that we would have ranked the 11th in the original task. Even if we were skeptical that a pure classical NLP approach combined with machine learning would be effective, we have convinced ourselves that it was possible.\n",
    "\n",
    "We have explored both lexical dimensions (most of the features) and syntatic dimension (for instance, noun phrases). The latter didn't work as expected. Combining the most promising features was key to success, regardless of the particular machine learning algorithm.\n",
    "\n",
    "We were inspired both by the practical sessions of this course and the ideas suggested by the original participants, as well as the references that we have cited at the end of thsi Notebook. For instance, we saw that using trigrams could be an interesting idea. From the winner, we took the idea that some sentences were almost literally the same one and could be interesting to heuristically detect these cases, although we implemented it with edit distance.\n",
    "\n",
    "Remarkably, the winners used a plain linear regression. Their feature engineering was that powerful that even the simplest model could grasp the complexity of the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity (Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre) \n",
    "\n",
    "Computational models for semantic textual similarity (González Aguirre, Aitor, 2017)\n",
    "\n",
    "Corpus-based and Knowledge-based Measures of Text Semantic Similarity (Ms. Anjali Ganesh Jivani).\n",
    "\n",
    "Majumder, Goutam & Pakray, Dr. Partha & Gelbukh, Alexander & Pinto, David. (2016). Semantic Textual Similarity Methods, Tools, and Applications: A Survey. Computacion y Sistemas. 20. 647-665. 10.13053/CyS-20-4-2506. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum: A note on the use of transfer learning approaches\n",
    "On parallel with the development of this project with classical NLP methods and classical machine learning, we investigated the use of deep learning for NLP. Specifically, we investigated the use of transfer learning from pre-trained models. Using Facebook's FastText word embeddings (which take characters into account), taking the average of all the word embeddings in a sentence and then applying cosine similarity with the one coming from the other sentence gave us a strong baseline (for only using one feature): 0.62 on cross-validation.\n",
    "\n",
    "We investigated and did some prelinary experiments with contextual embeddings, specifically vanilla BERT and RoBERTa. Their problem is that they are thought for being used in a token-level manner, but the embeddings can still be averaged, or we can add an additional layer and fine-tune the model.\n",
    "\n",
    "A recent model, Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: https://arxiv.org/abs/1908.10084, modifies BERT such that it is able to produce more effective sentence embeddings.\n",
    "\n",
    "We include the code for achieving the 0.62 correlation only with word embeddings. If we add this feature to our model, we will achieve a better punctuation for sure, but we considered that it did not make sense to apply transfer learning if we had already achieved good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip --directory-prefix=data\n",
    "%cd data\n",
    "!unzip wiki-news-300d-1M.vec.zip\n",
    "%cd ..\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "train_tokens = [word_tokenize(sent1) + word_tokenize(sent2) for sent1, sent2 in train_data]\n",
    "vocabulary = set([])\n",
    "for tokenized in train_tokens:\n",
    "    for token in tokenized:\n",
    "        vocabulary.add(token)\n",
    "pretrained_embeddings_path = os.path.join('data', 'wiki-news-300d-1M.vec')\n",
    "needed_tokens = set()\n",
    "embedding_table = {}\n",
    "dim = 0\n",
    "for line in open(pretrained_embeddings_path, 'r').readlines():\n",
    "    if dim == 0:\n",
    "        dim = int(line.split()[1])\n",
    "        continue\n",
    "    row = line.split()\n",
    "    token = row[0]\n",
    "    if token not in vocabulary:\n",
    "        continue\n",
    "    vector = np.array(list(map(float, row[1:])))\n",
    "    embedding_table[token] = vector\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class WordEmbeddingsModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, embedding_table, dim, method, *kwargs):\n",
    "        assert method in ['avg', 'sum', 'max']\n",
    "        self.embedding_table = embedding_table\n",
    "        self.dim = dim\n",
    "        self.method = method\n",
    "        super().__init__(*kwargs)\n",
    "    \n",
    "    def _get_sentence_embedding(self, sent):\n",
    "        tokenized = word_tokenize(sent)\n",
    "        def contains_punct(token):\n",
    "            for c in string.punctuation:\n",
    "                if c in token:\n",
    "                    return True\n",
    "            return False\n",
    "        tokenized = [token for token in tokenized if not contains_punct(token)] # empitjora\n",
    "        tokenized = [token.lower() for token in tokenized if token.lower() not in stop_words] # empitjora\n",
    "        embeddings = np.zeros((len(tokenized), self.dim))\n",
    "        for idx, token in enumerate(tokenized):\n",
    "            if token in embedding_table:\n",
    "                embeddings[idx] = self.embedding_table[token]\n",
    "            else:\n",
    "                embeddings[idx] = np.zeros(dim)       \n",
    "        if self.method == 'avg':\n",
    "            aggregated_embeddings = np.mean(embeddings, axis=0)\n",
    "        elif self.method == 'sum':\n",
    "            aggregated_embeddings = np.sum(embeddings, axis=0)\n",
    "        elif self.method == 'max':\n",
    "            aggregated_embeddings = np.max(embeddings, axis=0)\n",
    "        return aggregated_embeddings\n",
    "    \n",
    "    def _get_embeddings_and_cosine_similarity(self, sent1, sent2):\n",
    "        \n",
    "        emb1 = self._get_sentence_embedding(sent1)\n",
    "        emb2 = self._get_sentence_embedding(sent2)\n",
    "        cos_sim = scipy.spatial.distance.cosine(emb1, emb2)\n",
    "        emb1_emb2 = np.concatenate([emb1, emb2])\n",
    "        return cos_sim, emb1_emb2\n",
    "\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        pass\n",
    "\n",
    "class NegatedModel():\n",
    "        def fit(self, X, y):\n",
    "            return self\n",
    "        def predict(self, X):\n",
    "            pred = []\n",
    "            for row in X:\n",
    "                pred.append(-row[0])\n",
    "            return pred\n",
    "\n",
    "class WordEmbeddingsCosineSimilarityModel(WordEmbeddingsModel):\n",
    "    def __init__(self, *kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = NegatedModel()\n",
    "        self.name = 'WordEmbeddingsCosineSimilarityModel'\n",
    "        self.description = 'Pre-trained word Embeddings + stop words and punctuation filtering + cosine sim'\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return np.array([[self._get_embeddings_and_cosine_similarity(sent1, sent2)[0]] for sent1, sent2 in x])\n",
    "\n",
    "wordEmbeddingsCosineSimilarityModel = WordEmbeddingsCosineSimilarityModel(\n",
    "    embedding_table, dim, 'avg', train_data, train_labels)\n",
    "wordEmbeddingsCosineSimilarityModel.cross_validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
