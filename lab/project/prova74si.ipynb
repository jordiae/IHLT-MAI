{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!mkdir -p data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/train.tgz --directory-prefix=data\n",
    "!wget https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz --directory-prefix=data\n",
    "%cd data\n",
    "!tar zxvf train.tgz\n",
    "!tar zxvf test-gold.tgz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 149 ms, total: 345 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "train_files = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    with open(os.path.join('data', 'train', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        train_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'train', 'STS.gs.' + file + '.txt'), 'r') as f:\n",
    "        train_labels += [float(num) for num in f.readlines()]\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_files = ['MSRpar', 'MSRvid', 'SMTeuroparl', 'surprise.OnWN', 'surprise.SMTnews']\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in test_files:\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.input.' + file + '.txt'), 'r') as f:\n",
    "        test_data += [sent.split('\\t') for sent in f.readlines()]\n",
    "    with open(os.path.join('data', 'test-gold', 'STS.gs.'+ file + '.txt'), 'r') as f:\n",
    "        test_labels += [float(num) for num in f.readlines()]\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.2 s, sys: 286 ms, total: 52.5 s\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stopwords_set = set(stopwords.words('english')) \n",
    "\n",
    "def preprocess(X):\n",
    "    def is_number(s):\n",
    "        try:\n",
    "            x = float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def characters_not_punct(token):\n",
    "        for c in token:\n",
    "            if c in punctuation:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def lemmatize(token, pos):\n",
    "        if pos in {'N','V'}:\n",
    "            return wnl.lemmatize(token.lower(), pos.lower())\n",
    "        return token.lower()\n",
    "    \n",
    "    def stemmatize(token):\n",
    "            return PorterStemmer().stem(token)\n",
    "\n",
    "    def nltk_pos_to_wordnet_pos(nltk_pos):\n",
    "        mapping = {'NN': wn.NOUN, 'JJ': wn.ADJ, 'VB': wn.VERB, 'RB': wn.ADV}\n",
    "        if nltk_pos in mapping:\n",
    "            return mapping[nltk_pos]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_synset(lemma, pos):\n",
    "        wordnet_pos = nltk_pos_to_wordnet_pos(pos[1])\n",
    "        if wordnet_pos is not None:\n",
    "            word_synsets = wn.synsets(lemma, wordnet_pos)\n",
    "            if len(word_synsets) > 0:\n",
    "                most_freq_synset = word_synsets[0] # The most frequent synset is the first one\n",
    "                return most_freq_synset\n",
    "        return None\n",
    "    \n",
    "    def get_nes(pos_tags):\n",
    "        nes = ne_chunk(pos_tags, binary=False)\n",
    "        nes_map = []\n",
    "        for tree_element in nes:\n",
    "            if type(tree_element) == nltk.tree.Tree:\n",
    "                for element in tree_element:\n",
    "                        nes_map.append(tree_element.label())\n",
    "            else:\n",
    "                nes_map.append(None)\n",
    "        return nes_map\n",
    "                \n",
    "    def preprocess_sentence(sent):\n",
    "        tokens = word_tokenize(sent)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        clean_tokens = []\n",
    "        synsets = set([])\n",
    "        chars = ''.join([c for c in sent if c not in punctuation + ' '])\n",
    "        nes_map = get_nes(pos_tags)\n",
    "        for token, pos, ne in zip(tokens, pos_tags, nes_map):\n",
    "#             token = token.lower()\n",
    "#             if ne is not None:\n",
    "#                 token = ne\n",
    "#                 clean_tokens.append(token)\n",
    "#             lemma = lemmatize(token, pos)\n",
    "#             clean_tokens.append(lemma)\n",
    "            stem = stemmatize(token)\n",
    "            clean_tokens.append(stem)\n",
    "#             synset = get_synset(lemma, pos)\n",
    "#             if synset is not None:\n",
    "#                 synsets.add(synset)\n",
    "        return clean_tokens, synsets, chars\n",
    "\n",
    "    clean_tokens = []\n",
    "    synsets = []\n",
    "    chars = []\n",
    "    nes = []\n",
    "    for sent1, sent2 in X:\n",
    "        tok1, syn1, ch1 = preprocess_sentence(sent1)\n",
    "        tok2, syn2, ch2 = preprocess_sentence(sent2)\n",
    "        clean_tokens.append((tok1, tok2))\n",
    "        synsets.append((syn1, syn2))\n",
    "        chars.append((ch1, ch2))\n",
    "        \n",
    "    return clean_tokens, synsets, chars\n",
    "\n",
    "preprocessed_tokens_train, synsets_train, chars_train = preprocess(train_data)\n",
    "preprocessed_tokens_test, synsets_test, chars_test = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.metrics import jaccard_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "def overlap(A, B):\n",
    "    return len(A.intersection(B))/(min(len(A), len(B)))\n",
    "\n",
    "def set_kernel_distance(a, b):\n",
    "    return np.exp2(np.dot(a, b.T))\n",
    "\n",
    "def get_features(preprocessed_tokens, synsets, chars):\n",
    "    \n",
    "    def build_bow(sequences, method, chars=False, ngram_range=(1,1)):\n",
    "        assert method in ['bow', 'tf', 'tf_idf']\n",
    "        if not chars:\n",
    "            corpus = [' '.join(tokens1) + ' '.join(tokens2) for tokens1, tokens2 in sequences]\n",
    "            analyzer = 'word'\n",
    "        else:\n",
    "            corpus = [' '.join(list(chars1)) + ' '.join(list(chars2)) for chars1, chars2 in sequences]\n",
    "            analyzer = 'char'\n",
    "        if method == 'bow':\n",
    "            cv = CountVectorizer(binary=True, analyzer=analyzer, ngram_range=ngram_range)\n",
    "        elif method == 'tf':\n",
    "            cv = CountVectorizer(binary=False, analyzer=analyzer, ngram_range=ngram_range)\n",
    "        else:\n",
    "            cv = TfidfVectorizer()\n",
    "        cv.fit(corpus)\n",
    "        return cv\n",
    "    bow_tokens = build_bow(preprocessed_tokens, 'bow')\n",
    "    tf_tokens = build_bow(preprocessed_tokens, 'tf')\n",
    "    tf_idf_tokens = build_bow(preprocessed_tokens, 'tf_idf')\n",
    "    bow_chars = build_bow(chars, 'bow', chars=True)\n",
    "    tf_chars = build_bow(chars, 'tf', chars=True)\n",
    "    tf_idf_chars = build_bow(chars, 'tf_idf', chars=True)\n",
    "    bow_bigrams = build_bow(preprocessed_tokens, 'bow', (2, 2))\n",
    "    tf_bigrams = build_bow(preprocessed_tokens, 'tf', (2, 2))\n",
    "    tf_idf_bigrams = build_bow(preprocessed_tokens, 'tf_idf', (2, 2))\n",
    "    bow_trigrams = build_bow(preprocessed_tokens, 'bow', (3, 3))\n",
    "    tf_trigrams = build_bow(preprocessed_tokens, 'tf', (3, 3))\n",
    "    tf_idf_trigrams = build_bow(preprocessed_tokens, 'tf_idf', (3, 3))\n",
    "    def feature_encoder(tokens, synsets, chars,\n",
    "                        bow_tokens=bow_tokens, tf_tokens=tf_tokens, tf_idf_tokens=tf_idf_tokens,\n",
    "                        bow_chars=bow_chars, tf_chars=tf_chars, tf_idf_chars=tf_idf_chars):\n",
    "                toks1, toks2 = tokens\n",
    "                syns1, syns2 = synsets\n",
    "                chars1, chars2 = chars\n",
    "                \n",
    "                # encoding\n",
    "                encoded_bow_tokens = np.concatenate((bow_tokens.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     bow_tokens.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_tokens = np.concatenate((tf_tokens.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_tokens.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_idf_tokens = np.concatenate((tf_idf_tokens.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_idf_tokens.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_bow_chars = np.concatenate((bow_chars.transform([' '.join(chars1)]).toarray(),\n",
    "                                                     bow_chars.transform([' '.join(chars2)]).toarray())).flatten()\n",
    "                encoded_tf_chars = np.concatenate((tf_chars.transform([' '.join(chars1)]).toarray(),\n",
    "                                                     tf_chars.transform([' '.join(chars2)]).toarray())).flatten()\n",
    "                encoded_tf_idf_chars = np.concatenate((tf_idf_chars.transform([' '.join(chars1)]).toarray(),\n",
    "                                                     tf_idf_chars.transform([' '.join(chars2)]).toarray())).flatten()\n",
    "                encoded_bow_bigrams = np.concatenate((bow_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     bow_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_bigrams = np.concatenate((tf_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_idf_bigrams = np.concatenate((tf_idf_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_idf_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_bow_trigrams = np.concatenate((bow_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     bow_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_trigrams = np.concatenate((tf_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                encoded_tf_idf_trigrams = np.concatenate((tf_idf_chars.transform([' '.join(toks1)]).toarray(),\n",
    "                                                     tf_idf_chars.transform([' '.join(toks2)]).toarray())).flatten()\n",
    "                                                     \n",
    "                # distances\n",
    "                edit_dist = edit_distance(chars1, chars2)\n",
    "                set_dist_encoded_bow_tokens = set_kernel_distance(\n",
    "                    encoded_bow_tokens[:encoded_bow_tokens.shape[0]//2],\n",
    "                    encoded_bow_tokens[encoded_bow_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_tokens = cosine_similarity(\n",
    "                    encoded_tf_tokens[:encoded_tf_tokens.shape[0]//2],\n",
    "                    encoded_tf_tokens[encoded_tf_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_idf_tokens = cosine_similarity(\n",
    "                    encoded_tf_idf_tokens[:encoded_tf_idf_tokens.shape[0]//2],\n",
    "                    encoded_tf_idf_tokens[encoded_tf_idf_tokens.shape[0]//2:])\n",
    "                cos_encoded_tf_chars = cosine_similarity(\n",
    "                    encoded_tf_chars[:encoded_tf_chars.shape[0]//2],\n",
    "                    encoded_tf_chars[encoded_tf_chars.shape[0]//2:])\n",
    "                set_dist_encoded_bow_bigrams = set_kernel_distance(\n",
    "                    encoded_bow_bigrams[:encoded_bow_bigrams.shape[0]//2],\n",
    "                    encoded_bow_bigrams[encoded_bow_bigrams.shape[0]//2:])\n",
    "                cos_encoded_tf_bigrams = cosine_similarity(\n",
    "                    encoded_tf_bigrams[:encoded_tf_bigrams.shape[0]//2],\n",
    "                    encoded_tf_bigrams[encoded_tf_bigrams.shape[0]//2:])\n",
    "                set_dist_encoded_bow_trigrams = set_kernel_distance(\n",
    "                    encoded_bow_trigrams[:encoded_bow_trigrams.shape[0]//2],\n",
    "                    encoded_bow_trigrams[encoded_bow_trigrams.shape[0]//2:])\n",
    "                cos_encoded_tf_trigrams = cosine_similarity(\n",
    "                    encoded_tf_trigrams[:encoded_tf_trigrams.shape[0]//2],\n",
    "                    encoded_tf_trigrams[encoded_tf_trigrams.shape[0]//2:])\n",
    "                \n",
    "                                                      \n",
    "                toks1_set, toks2_set = set(toks1), set(toks2)\n",
    "                if len(toks1_set) > 0 and len(toks2_set) > 0:\n",
    "                    tok_jaccard = jaccard_distance(set(toks1), set(toks2))\n",
    "                    tok_overlap = overlap(set(toks1), set(toks2))\n",
    "                else:\n",
    "                    tok_jaccard = 1\n",
    "                    tok_overlap = 1\n",
    "#                 if len(syns1) > 0 and len(syns2) > 0:\n",
    "#                     syn_jaccard = jaccard_distance(syns1, syns2)\n",
    "#                     syn_overlap = overlap(syns1, syns2)\n",
    "#                 else:\n",
    "#                     syn_jaccard = 1\n",
    "#                     syn_overlap = 1\n",
    "                if len(chars1) > 0 and len(chars2) > 0:\n",
    "                    chars_jaccard = jaccard_distance(set(list(chars1)), set(list(chars2)))\n",
    "                    chars_overlap = overlap(set(list(chars1)), set(list(chars2)))\n",
    "                else:\n",
    "                    chars_jaccard = 1\n",
    "                    chars_overlap = 1\n",
    "                return OrderedDict(edit_dist=edit_dist, set_dist_encoded_bow_tokens=set_dist_encoded_bow_tokens,\n",
    "                           cos_encoded_tf_idf_tokens=cos_encoded_tf_idf_tokens,\n",
    "                           cos_encoded_tf_chars=cos_encoded_tf_chars,\n",
    "                           set_dist_encoded_bow_bigrams=set_dist_encoded_bow_bigrams,\n",
    "                           cos_encoded_tf_bigrams=cos_encoded_tf_bigrams,\n",
    "                           set_dist_encoded_bow_trigrams=set_dist_encoded_bow_trigrams,\n",
    "                           cos_encoded_tf_trigrams=cos_encoded_tf_trigrams,\n",
    "                           tok_jaccard=tok_jaccard,\n",
    "                           tok_overlap=tok_overlap,    \n",
    "#                            syn_jaccard=syn_jaccard,\n",
    "#                            syn_overlap=syn_overlap,\n",
    "                           chars_jaccard=chars_jaccard,\n",
    "                           chars_overlap=chars_overlap)                      \n",
    "                \n",
    "    features = []\n",
    "    for toks, syns, ch in zip(preprocessed_tokens, synsets, chars):\n",
    "        feat = feature_encoder(toks, syns, ch)\n",
    "        features.append(feat)\n",
    "    mat_features = pd.DataFrame(features).values\n",
    "    std_scaler = StandardScaler().fit(mat_features)\n",
    "    def scale(features_row, scaler=std_scaler):\n",
    "        scaled = scaler.transform(pd.DataFrame([features_row]).values)\n",
    "        scaled_features_row = OrderedDict(zip(features_row.keys(), scaled[0]))\n",
    "        return scaled_features_row\n",
    "    scaled_feat = []\n",
    "    for feat in features:\n",
    "        scaled_feat.append(scale(feat))\n",
    "    return scaled_feat, feature_encoder, scale\n",
    "\n",
    "features_train, feature_encoder, scaler = get_features(preprocessed_tokens_train, synsets_train, chars_train)\n",
    "features_test = []\n",
    "for toks, syns, ch in zip(preprocessed_tokens_test, synsets_test, chars_test):\n",
    "    feat = feature_encoder(toks, syns, ch)\n",
    "    features_test.append(feat)\n",
    "scaled_feat = []\n",
    "for feat in features_test:\n",
    "    scaled_feat.append(scaler(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "features_test = []\n",
    "for feat in scaled_feat:\n",
    "    new_d = {}\n",
    "    for key in feat:\n",
    "        if not math.isnan(feat[key]):\n",
    "            new_d[key] = feat[key]\n",
    "        else:\n",
    "            new_d[key] = 0\n",
    "    features_test.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,  x, y, regr=LinearRegression(),):\n",
    "        self.regr = regr\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.name = None\n",
    "        self.description = None\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        pickle.dump(self, open(self.name + '.model', 'wb').write())\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, name):\n",
    "        return pickle.load(open(self.name + '.model', 'rb').read())\n",
    "        \n",
    "    \n",
    "    def _extract_features(self, x):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x_features = self._extract_features(x)\n",
    "        self.y = y\n",
    "        self.regr.fit(self.x_features, self.y)\n",
    "    \n",
    "    \n",
    "    def predict(self, new_x):\n",
    "        new_x_features = self._extract_features(new_x)\n",
    "        return self.regr.predict(new_x_features)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, true_labels, predicted_labels):\n",
    "        pearson, p_value = stats.pearsonr(true_labels, predicted_labels)\n",
    "        return pearson, p_value\n",
    "    \n",
    "    \n",
    "    def cross_validate(self, n_folds=5, seed=1):\n",
    "        assert self.x_features is not None\n",
    "        kf = sklearn.model_selection.KFold(n_splits=n_folds, random_state=seed)\n",
    "        average_pearson = 0\n",
    "        for train_index, val_index in kf.split(self.x_features):\n",
    "            X_train, X_val = self.x_features[train_index], self.x_features[val_index]\n",
    "            y_train, y_val = self.y[train_index], self.y[val_index]\n",
    "            self.regr.fit(X_train, y_train)\n",
    "            predicted_labels = self.regr.predict(X_val)\n",
    "            pearson, _ = self.evaluate(y_val, predicted_labels)\n",
    "            average_pearson += abs(pearson)\n",
    "        return average_pearson/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class RandomForestModel(Model):\n",
    "    def __init__(self, *kwargs):\n",
    "        self.name = 'RandomForestModel'\n",
    "        self.description = 'RandomForestModel'    \n",
    "        super().__init__(*kwargs)\n",
    "        self.regr = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=1)\n",
    "        \n",
    "    def _extract_features(self, x):\n",
    "        return x\n",
    "\n",
    "randomForestModel = RandomForestModel(pd.DataFrame(features_train).values, train_labels)\n",
    "# randomForestModel.cross_validate()\n",
    "\n",
    "randomForestModel.fit(pd.DataFrame(features_train).values, train_labels)\n",
    "pred = randomForestModel.predict(pd.DataFrame(features_test).values)\n",
    "randomForestModel.evaluate(test_labels, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
